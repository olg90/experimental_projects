#%%  Load the data and plot a place field
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import nbinom
from scipy.optimize import minimize

# ========================= Load data from .mat files =========================
n = [] # number of place fields
p = [] # neuron maps

dnametype=0 # 0-5
#              0                1              2
dnames = ['BigroomData', 'CircleData', 'HalfroomData', 
          'HexagonData', 'RectangleData', 'SquareData']
#              3                4              5

location=0
if location==0:
    dir1 = r'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{}'.format(dnames[dnametype])
if location==1:
    dir1 = r'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\BigroomData'
os.chdir(dir1)
dir2 = os.listdir(dir1)

for foldername in dir2:
    dir3 = os.path.join(dir1, foldername)
    os.chdir(dir3)
    dirs3 = os.listdir()
    dirs4 = [x for x in dirs3 if x[-7] == 'm']
    for matname in dirs4:
        dir5 = os.path.join(dir3, matname)
        mat = loadmat(dir5)
        n.append(mat['numFields'][0][0])
        p.append(mat['pfields'])


plt.imshow(p[0])
plt.show()

pshapes = [(p[i].shape[0], p[i].shape[1]) for i in range(len(p))]
pshapes = list(set(pshapes))
print(pshapes)

extensions = [(68, 120), # 0
              (44, 60), # 1
              (64, 60), # 2
              (16,16),  # 3
              (32, 40), # 4
              (20,20)] # 5

# =============================================================================
# for x in p:
#     plt.imshow(x)
#     plt.show()
# 
# =============================================================================

shapeslist = [[(65, 116), (65, 117)], # BigroomData
              [(43, 53), (43, 50), (43, 57)], # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)], # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)], # RectangleData
              [(20, 20)]] # SquareData

extensions = [(68, 120), # BigroomData
              (44, 60), # CircleData
              (64, 60), # HalfroomData
              (16,16),  # HexagonData
              (32, 40), # RectangleData
              (20,20)] # SquareData

#%%  Plot all place maps

import numpy as np
import matplotlib.pyplot as plt

# ===================== Parameters =====================
neuron_maps = p[5]

# ===================== Compute best subplot layout =====================
n = len(neuron_maps)
cols = int(np.ceil(np.sqrt(n)))
rows = int(np.ceil(n / cols))

# ===================== Plot =====================
fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.8, rows * 1.8))
axes = np.ravel(axes)

for i, (ax, neuron_map) in enumerate(zip(axes, neuron_maps)):
    ax.imshow(neuron_map, cmap='viridis', aspect='auto')
    ax.set_title(f"{i}", fontsize=7, pad=1)
    ax.axis('off')

# Turn off unused axes
for ax in axes[len(neuron_maps):]:
    ax.axis('off')

# Tighten layout to minimize white space
plt.subplots_adjust(wspace=0.05, hspace=0.05, left=0.01, right=0.99, top=0.97, bottom=0.01)
plt.show()

#%%
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
#%%
p1=[(pi>0)*1 for pi in p]

n = 1  # 1,2,4
grid = (n, n)

plist = []
clist = []
for pi in p1:
    pshape = pi.shape
    x1 = pshape[0] // grid[0]
    x2 = pshape[1] // grid[1]
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

#%% Generate trimmed plots
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

p0=p_trimmed[0]        
pshape=p0.shape
grid=(16,4)

plt.figure(figsize=(10,4))
plt.imshow(p0)
plt.colorbar()
plt.show()

plt.figure(figsize=(10,4))
plt.imshow(p0)
plt.yticks([])
plt.xticks([])
plt.show()

fs=20#
plt.figure(figsize=(10,4))
plt.title('Firing rate map, grid size = ({},{})'.format(grid[0], grid[1]),fontsize=fs)
# =============================================================================
# plt.title('Firing rate map, Neuron # {}'.format(1),fontsize=fs)
# =============================================================================
im = plt.imshow(p0, cmap='viridis')
# Add colorbar
cbar = plt.colorbar(shrink=1)
cbar.ax.tick_params(labelsize=fs)
cbar.set_label('Firing rate', fontsize=fs, labelpad=20)
cbar.ax.yaxis.label.set_rotation(270)
plt.tick_params(axis='both', labelsize=fs)

# Draw red horizontal grid lines
for i in range(0, pshape[0], grid[0]):
    plt.hlines(i - 0.5, xmin=-0.5, xmax=pshape[1] - 0.5, colors='red', linewidth=2)

# Draw red vertical grid lines
for j in range(0, pshape[1], grid[1]):
    plt.vlines(j - 0.5, ymin=-0.5, ymax=pshape[0] - 0.5, colors='red', linewidth=2)

plt.tight_layout()
plt.show()


#%% Plot of grid over place field

import matplotlib.pyplot as plt
import numpy as np

# Your trimmed firing rate map and grid
p0 = p_trimmed[0]
pshape = p0.shape
grid = (8, 4)

# Plot the base firing rate map
fig, ax = plt.subplots()
im = ax.imshow(p0, cmap='viridis')

# Add colorbar
cbar = fig.colorbar(im, ax=ax, shrink=0.7)
cbar.set_label('Firing rate', fontsize=15, labelpad=20)
cbar.ax.yaxis.label.set_rotation(270)

# Draw horizontal and vertical red lines
for i in range(0, pshape[0], grid[0]):
    ax.axhline(i - 0.5, color='red', linewidth=1)

for j in range(0, pshape[1], grid[1]):
    ax.axvline(j - 0.5, color='red', linewidth=1)

# Optional: show tighter layout
plt.tight_layout()
plt.show()

# =============================================================================
# #%% Plots for collaborator meeting
# 
# th=10
# p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
# p_trimmed = [pi[:-1, :] for pi in p_trimmed]
# pth = [(pi>th)*pi for pi in p_trimmed]
# pthc = np.array([(pi>th)*1 for pi in p_trimmed]) # convert to 1s and 0s
# rates=np.unique(p_trimmed)
# 
# plt.imshow(pth[0])
# plt.xticks([])
# plt.yticks([])
# =============================================================================
#%%  beta binomali gaussian
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, betabinom, binom
from scipy.optimize import curve_fit, minimize



## Original
# --- Data preparation (based on list of arrays `p`) ---
# Step 1: Trim edges if necessary, shape is (64 x 116)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi > 0) * 1 for pi in p_trimmed]

# Step 2: Divide into grid and count active neurons
pshape = pth[0].shape
n = 1  # 1,2,4
grid = (n, n)
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

plist = []
clist = []
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Step 3: Flatten counts
csum = np.sum(clist, axis=0)
data = csum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20

# --- Gaussian Mixture Fit ---
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    w2 = 1 - w1
    return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)

hist_vals, _ = np.histogram(data, bins=binarr, density=True)
p0 = [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]
bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3],
          [1, np.inf, np.inf, np.inf, np.inf])

params_gmm, _ = curve_fit(
    gaussian_mixture, bin_centers, hist_vals, p0=p0, bounds=bounds, maxfev=10000
)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV² for each Gaussian component
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# --- Beta-binomial Fit ---
n_neurons = len(pth)

def neg_log_likelihood(params):
    a, b = params
    if a <= 0 or b <= 0:
        return np.inf
    return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))

res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B',
               bounds=[(1e-5, None), (1e-5, None)])
alpha_fit, beta_fit = res.x
x_discrete = np.arange(0, n_neurons + 1)
pmf_beta = betabinom.pmf(x_discrete, n=n_neurons, a=alpha_fit, b=beta_fit)

# Mean and CV² for beta-binomial
mean_bb = n_neurons * alpha_fit / (alpha_fit + beta_fit)
var_bb = (n_neurons * alpha_fit * beta_fit * (n_neurons + alpha_fit + beta_fit)) / \
         ((alpha_fit + beta_fit) ** 2 * (alpha_fit + beta_fit + 1))
cv2_bb = var_bb / mean_bb ** 2

# --- Binomial Fit (MLE) ---
p_hat = np.mean(data) / n_neurons
pmf_binom = binom.pmf(x_discrete, n=n_neurons, p=p_hat)
mean_binom = n_neurons * p_hat
var_binom = n_neurons * p_hat * (1 - p_hat)
cv2_binom = var_binom / mean_binom ** 2 if mean_binom != 0 else np.nan

# --- Plot All Fits ---
plt.figure()
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

plt.plot(bin_centers, y_gmm, 'r-', lw=5,
         label=f'Gaussian mixture\n(μ1={mu1:.2f}, CV²₁={cv2_1:.2f},\nμ2={mu2:.2f}, CV²₂={cv2_2:.2f})')

plt.plot(x_discrete, pmf_beta, 'g--', lw=5,
         label=f'Beta-binomial\n(mean={mean_bb:.2f}, CV²={cv2_bb:.2f})')

plt.plot(x_discrete, pmf_binom, 'm-.', lw=5,
         label=f'Binomial\n(n={n_neurons}, p={p_hat:.2f},\
             \nmean={mean_binom:.2f}, CV²={cv2_binom:.2f})')

plt.title(f'({grid[0]},{grid[1]})-grid', fontsize=fs+10)
plt.xlabel(f'# place fields in a ({grid[0]},{grid[1]}) grid', fontsize=fs+10)
plt.ylabel('Density of # neurons', fontsize=fs+10)
plt.tick_params(axis='both', labelsize=fs+10)
plt.legend(fontsize=fs+5)
plt.tight_layout()
plt.show()

#%% Binomial and negative binomial compare
# =============================================================================
# 
# p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
# p_trimmed = [pi[:-1, :] for pi in p_trimmed]
# pth = [(pi>0)*1 for pi in p_trimmed]
# rates=np.unique(p_trimmed)
# 
# ## The unique rates
# # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
# #       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
# 
# pshape = pth[0].shape
# 
# n=1 # 1,2,4
# grid=(n,n)
# x1 = pshape[0]//grid[0]
# x2 = pshape[1]//grid[1]
# 
# plist=[]
# clist=[]
# for pi in pth:
#     patches=np.zeros((x1,x2), dtype=object)
#     counts=np.zeros((x1,x2))
#     for i in range(x1):
#         for j in range(x2):
#             patch=pi[i*grid[0]:(i+1)*grid[0],j*grid[1]:+(j+1)*grid[1]]
#             count = 1*np.any(patch>0)
#             patches[i,j]=patch
#             counts[i,j]=count
#     plist.append(patches)
#     clist.append(counts)
# 
# csum=np.sum(clist, axis=0)
# data=csum.flatten()
# binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
# binplot = np.arange(np.min(data), np.max(data) + 1)
# fs=15
# 
# from scipy.stats import norm
# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.optimize import curve_fit
# 
# # mixture of two Gaussians
# def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
#     w2 = 1 - w1
#     return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)
# 
# # histogram
# hist_vals, bin_edges = np.histogram(data, bins=binarr, density=True)
# bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
# 
# # fit
# p0 = [0.5, np.mean(data)-1, 1, np.mean(data)+1, 1]
# params, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0, maxfev=10000)
# 
# # evaluate fit
# x_fit = np.linspace(np.min(data), np.max(data), 1000)
# y_fit = gaussian_mixture(x_fit, *params)
# 
# # plot
# plt.figure()
# plt.hist(data, bins=binarr, density=True,
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# plt.plot(x_fit, y_fit, 'r', label='Gaussian mixture fit')
# plt.title('({},{})-grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both', labelsize=fs)
# plt.legend()
# plt.show()
# 
# from scipy.stats import betabinom
# from scipy.optimize import minimize
# 
# # Step 1: Your `data` already contains counts per region (number of neurons active)
# data = csum.flatten()
# 
# # Step 2: Total number of neurons per grid patch = number of pthc arrays
# n_neurons = len(pth)
# 
# # Step 3: Fit alpha, beta using MLE
# def neg_log_likelihood(params):
#     a, b = params
#     if a <= 0 or b <= 0:
#         return np.inf
#     return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))
# 
# res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B', 
#                bounds=[(1e-5, None), (1e-5, None)])
# alpha_fit, beta_fit = res.x
# 
# print(f"Fitted alpha: {alpha_fit:.3f}, beta: {beta_fit:.3f}")
# 
# # Step 4: Plot histogram with fitted beta-binomial PMF
# binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
# binplot = np.arange(np.min(data), np.max(data) + 1)
# 
# fs = 15
# plt.figure()
# plt.hist(data, bins=binarr, density=True, 
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# 
# # Overlay beta-binomial fit
# x = np.arange(0, n_neurons + 1)
# pmf = betabinom.pmf(x, n=n_neurons, a=alpha_fit, b=beta_fit)
# plt.plot(x, pmf, 'r--', lw=2, label='Beta-binomial fit')
# 
# plt.title('({},{})-grid'.format(
#     grid[0], grid[1]), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both', labelsize=fs)
# plt.legend(fontsize=fs)
# plt.show()
# =============================================================================
        
#%%  Threshold (5 and 10) and divide into rectangles

th=0
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi>th)*pi for pi in p_trimmed]
pthc = np.array([(pi>th)*1 for pi in p_trimmed]) # convert to 1s and 0s
rates=np.unique(p_trimmed)

## The unique rates
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
#       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]

pshape = pth[0].shape

# =============================================================================
# pshape = pth[0].shape # (65, 116)
# def find_factors(num):
#     factors = []
#     for i in range(1, int(num**0.5) + 1):
#         if num % i == 0:
#             factors.append(i)
#             if num // i != i:
#                 factors.append(num // i)
#     factors.sort()
#     return factors
# 
# f1=find_factors(pshape[0])
# f2=find_factors(pshape[1])
# =============================================================================
# =============================================================================
# grid=(1,1)
# x1 = pshape[0]//grid[0]
# x2 = pshape[1]//grid[1]
# =============================================================================
# grid x shape = [1, 2, 4, 8, 16, 32, 64]
# grid y shape = [1, 2, 4, 29, 58, 116]

n=1 # 1,2,4
grid=(n,n)
x1 = pshape[0]//grid[0]
x2 = pshape[1]//grid[1]

plist=[]
clist=[]
for pi in pthc:
    patches=np.zeros((x1,x2), dtype=object)
    counts=np.zeros((x1,x2))
    for i in range(x1):
        for j in range(x2):
            patch=pi[i*grid[0]:(i+1)*grid[0],j*grid[1]:+(j+1)*grid[1]]
            count = 1*np.any(patch>0)
            patches[i,j]=patch
            counts[i,j]=count
    plist.append(patches)
    clist.append(counts)

csum=np.sum(clist, axis=0)
data=csum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)
fs=15
# =============================================================================
# pth3=[]
# for pi in plist:
#     pth2 = np.zeros(plist[0].shape)
#     pishape=pi.shape    
#     for i in range(pishape[0]):
#         for j in range(pishape[1]):
#             pth2[i,j]=1 if np.sum(pi[i,j])>0 else 0
#     pth3.append(pth2)
# 
# =============================================================================
# =============================================================================
# 
##%%
# fig, ax = plt.subplots()
# 
# im = ax.imshow(pth3[0], extent=[0, 116, 0, 64], cmap='gray', origin='lower')
# 
# ax.set_xticks([])
# ax.set_yticks([])
# 
# # Horizontal lines: 16 rows → every 64/16 = 4 units
# for i in range(17):  # 16 divisions = 17 lines
#     y = i * (64 / 16)
#     ax.axhline(y, color='red', linewidth=1)
# 
# # Vertical lines: 116 columns → every 1 unit
# for j in range(117):  # 116 divisions = 117 lines
#     x = j * (116 / 116)
#     ax.axvline(x, color='red', linewidth=1)
# 
# plt.show()
# 
# 
#     #%%
# =============================================================================
# =============================================================================
# plt.figure()
# plt.hist(data, bins=binarr, density=True, 
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# plt.title('({},{})-grid, counting all AP rates > {}'.format(
#     grid[0], grid[1], th), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), 
#            fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both',labelsize=fs)
# plt.show()
# =============================================================================
# =============================================================================
# plt.figure()
# plt.imshow(csum, cmap='hot')
# plt.title('Number of neurons firing in a ({},{})-grid'.format(grid[0], grid[1]))
# plt.colorbar()
# =============================================================================
from scipy.stats import norm
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

# mixture of two Gaussians
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    w2 = 1 - w1
    return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)

# histogram
hist_vals, bin_edges = np.histogram(data, bins=binarr, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# fit
p0 = [0.5, np.mean(data)-1, 1, np.mean(data)+1, 1]
params, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0)

# evaluate fit
x_fit = np.linspace(np.min(data), np.max(data), 1000)
y_fit = gaussian_mixture(x_fit, *params)

# plot
plt.figure()
plt.hist(data, bins=binarr, density=True,
         color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
plt.plot(x_fit, y_fit, 'r', label='Gaussian mixture fit')
plt.title('({},{})-grid, counting all AP rates > {}'.format(grid[0], grid[1], th), fontsize=fs)
plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
plt.ylabel('Density of # neurons', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend()
plt.show()

from scipy.stats import betabinom
from scipy.optimize import minimize

# Step 1: Your `data` already contains counts per region (number of neurons active)
data = csum.flatten()

# Step 2: Total number of neurons per grid patch = number of pthc arrays
n_neurons = len(pthc)

# Step 3: Fit alpha, beta using MLE
def neg_log_likelihood(params):
    a, b = params
    if a <= 0 or b <= 0:
        return np.inf
    return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))

res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B', bounds=[(1e-5, None), (1e-5, None)])
alpha_fit, beta_fit = res.x

print(f"Fitted alpha: {alpha_fit:.3f}, beta: {beta_fit:.3f}")

# Step 4: Plot histogram with fitted beta-binomial PMF
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

fs = 15
plt.figure()
plt.hist(data, bins=binarr, density=True, 
         color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')

# Overlay beta-binomial fit
x = np.arange(0, n_neurons + 1)
pmf = betabinom.pmf(x, n=n_neurons, a=alpha_fit, b=beta_fit)
plt.plot(x, pmf, 'r--', lw=2, label='Beta-binomial fit')

plt.title('({},{})-grid, counting all AP rates > {}'.format(
    grid[0], grid[1], th), fontsize=fs)
plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
plt.ylabel('Density of # neurons', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs)
plt.show()


#%% Percentage of filling of the space

th=0
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi>th)*pi for pi in p_trimmed]

per = [np.sum((pi>0)*1)/np.prod(pi.shape) for pi in p_trimmed]



from scipy.stats import expon
import numpy as np
import matplotlib.pyplot as plt

# Compute histogram
plt.hist(per, bins=40, density=True, label='data', edgecolor='k')

# Fit exponential distribution
loc, scale = expon.fit(per, floc=0)  # force location = 0
x = np.linspace(min(per), max(per), 1000)
pdf = expon.pdf(x, loc=loc, scale=scale)

# Plot exponential fit
plt.plot(x, pdf, 'r', label='exponential fit', linewidth=4, alpha=1)
plt.legend()
plt.xlabel('Percentage of the rectangular room the neuron fires in')
plt.ylabel('Count')
plt.show()

from scipy.stats import gamma
import numpy as np
import matplotlib.pyplot as plt

plt.figure()

# Histogram
plt.hist(per, bins=45, density=True, label='data', edgecolor='k')

# Fit gamma distribution
shape, loc, scale = gamma.fit(per, floc=0)  # force location = 0
x = np.linspace(min(per), max(per), 1000)
pdf = gamma.pdf(x, a=shape, loc=loc, scale=scale)

# Plot gamma fit
plt.plot(x, pdf, 'r', label='gamma fit', linewidth=3)
plt.legend()
plt.xlabel('Percentage of the rectangular room the neuron fires in')
plt.ylabel('Count')
plt.show()
#%% Percentage of the room filling space plotted together

# prepare data
th = 0
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi > th) * pi for pi in p_trimmed]

# compute percentage of active area per neuron
per = [np.sum((pi > 0) * 1) / np.prod(pi.shape) for pi in p_trimmed]

from scipy.stats import expon, gamma
import numpy as np
import matplotlib.pyplot as plt

# create histogram
plt.figure(figsize=(8, 6))
plt.hist(per, bins=45, density=True, label='data', edgecolor='k', alpha=0.6)

# define x-axis range for pdfs
x = np.linspace(min(per), max(per), 1000)

# fit exponential distribution
loc_exp, scale_exp = expon.fit(per, floc=0)
pdf_exp = expon.pdf(x, loc=loc_exp, scale=scale_exp)

# fit gamma distribution
shape_gamma, loc_gamma, scale_gamma = gamma.fit(per, floc=0)
pdf_gamma = gamma.pdf(x, a=shape_gamma, loc=loc_gamma, scale=scale_gamma)

# plot both pdfs
plt.plot(x, pdf_exp, 'r-', linewidth=3, label='exponential fit')
plt.plot(x, pdf_gamma, 'b--', linewidth=3, label='gamma fit')

# finalize plot
plt.xlabel('percentage of the rectangular room the neuron fires in')
plt.ylabel('density')
plt.legend()
plt.tight_layout()
plt.show()


#%%

# =================== Fit NB with integer r using grid search ===================

# =============================================================================
# possible_r = np.arange(1, 50)  # Try integer r from 1 to 49
# best_ll = -np.inf
# best_r = None
# best_p = None
#
# for r in possible_r:
#     mean = np.mean(n)
#     var = np.var(n)
#     if var > mean:  # NB only defined for overdispersed data
#         p_hat = r / (r + mean)
#         ll = np.sum(nbinom.logpmf(n, r, p_hat))
#         if ll > best_ll:
#             best_ll = ll
#             best_r = r
#             best_p = p_hat
# 
# rhat, phat = best_r, best_p
# 
# =============================================================================
# Original MLE estimation using continuous parameters
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(n,), bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

# ========================= Plot histogram + fitted distribution =========================
binarr = np.arange(np.min(n) - 0.5, np.max(n) + 1.5)
binplot = np.arange(np.min(n), np.max(n) + 1)

plt.figure(figsize=(8, 3))
plt.hist(n, bins=binarr, color='skyblue', edgecolor='black', alpha=0.7, density=True)
pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('Density of # of neurons', fontsize=12)
plt.xlabel('# of place fields for a neuron', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

# =============================================================================
# # Plot out all the place fields
# for i in range(len(p)):
#     plt.figure()
#     plt.imshow(p[i])
#     plt.colorbar()
# =============================================================================

pcount = [np.where(pi[:, :-1] != 0, 1, 0) if pi.shape[1] == 117 else np.where(pi != 0, 1, 0) for pi in p]

from collections import Counter

shape_counts = Counter([pi.shape for pi in pcount])
print(shape_counts) 

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom, norm

pcountsum = np.sum(pcount, axis=0)

plt.figure()
plt.imshow(pcountsum, cmap='hot')
plt.title('Number of neurons that fire at each pixel\nin the rectangular room')
plt.colorbar()

#%%

binarr = np.arange(np.min(pcountsum) - 0.5, np.max(pcountsum) + 1.5)
binplot = np.arange(np.min(pcountsum), np.max(pcountsum) + 1)
# Flatten data for fitting
data_flat = pcountsum.flatten()
# Fit Normal distribution
mu_norm, std_norm = norm.fit(data_flat)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skewnorm
from scipy.optimize import minimize

# Flatten and bin data
data = pcountsum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

# ======== Fit Skew-Normal ========
a_skew, loc_skew, scale_skew = skewnorm.fit(data)
x_vals = np.linspace(np.min(data), np.max(data), 1000)
pdf_skew = skewnorm.pdf(x_vals, a_skew, loc=loc_skew, scale=scale_skew)

# ======== Fit Gaussian Mixture (2 components) ========
def neg_log_likelihood(params):
    w, mu1, sigma1, mu2, sigma2 = params
    if not (0 < w < 1 and sigma1 > 0 and sigma2 > 0):
        return np.inf
    pdf_vals = w * norm.pdf(data, mu1, sigma1) + (1 - w) * norm.pdf(data, mu2, sigma2)
    return -np.sum(np.log(pdf_vals + 1e-12))

initial_guess = [0.5, np.percentile(data, 25), 1.0, np.percentile(data, 75), 1.0]
bounds = [(1e-3, 1 - 1e-3), (None, None), (1e-3, None), (None, None), (1e-3, None)]
result = minimize(neg_log_likelihood, initial_guess, bounds=bounds)
w, mu1, sigma1, mu2, sigma2 = result.x
pdf_mix = w * norm.pdf(x_vals, mu1, sigma1) + (1 - w) * norm.pdf(x_vals, mu2, sigma2)
pdf1 = w * norm.pdf(x_vals, mu1, sigma1)
pdf2 = (1 - w) * norm.pdf(x_vals, mu2, sigma2)

# ======== Plot All Together ========
plt.figure(figsize=(9, 4))
plt.hist(data, bins=binarr, density=True, color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# =============================================================================
# plt.plot(x_vals, pdf_skew, 'g-', linewidth=2, label=f'Skew Normal (a={a_skew:.2f})')
# =============================================================================
plt.plot(x_vals, pdf_mix, 'r-', linewidth=2, label='Gaussian Mixture')
# =============================================================================
# plt.plot(x_vals, pdf1, 'k--', label='Component 1')
# plt.plot(x_vals, pdf2, 'm--', label='Component 2')
# =============================================================================

plt.xlabel('# of neurons firing APs at a pixel')
plt.ylabel('Density of # neurons firing APs at a pixel')
plt.xticks(binplot)
plt.legend()
plt.tight_layout()
plt.show()

# Trim and collect matrices
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Convert to NumPy array stack only if all shapes are now equal
shapes = [pi.shape for pi in p_trimmed]
common_shape = shapes[0]
assert all(shape == common_shape for shape in shapes), "Not all matrices have the same shape after trimming."
# Sum them together
pratesum = np.sum(p_trimmed, axis=0)
plt.figure()
plt.title('Sum of all AP rates at each location')
plt.imshow(pratesum, cmap='hot')
plt.colorbar()

plt.figure()
plt.title('Mean rate at each location')
meanmap=pratesum/pcountsum
plt.imshow(meanmap, cmap='hot')
plt.colorbar()

#%%
# Count occurrences of most common non-zero rate in each pi
counts = [np.count_nonzero(pi == np.bincount(pi.flatten())[1:].argmax() + 1) for pi in p]

# Get the most common non-zero rate in each pi
rates = [np.bincount(pi.flatten())[1:].argmax() + 1 for pi in p]

print(counts)
print(rates)

# Histogram of counts
plt.hist(counts, bins=np.arange(min(counts), max(counts) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Counts')
plt.xlabel('Count')
plt.ylabel('Frequency')
plt.xticks(np.unique(counts))
plt.show()

# Histogram of rates
plt.hist(rates, bins=np.arange(min(rates), max(rates) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Rates')
plt.xlabel('Rate')
plt.ylabel('Frequency')
plt.xticks(np.unique(rates))
plt.show()

#%% Number of neurons with AP rate

# For each pi, find the unique nonzero rates
unique_rates_per_pi = [np.unique(pi[pi != 0]) for pi in p]

# Flatten the list and combine all unique rates
# A list of all the rates that appear in each neuron uniquely
all_unique_rates = np.concatenate(unique_rates_per_pi)

# Plot histogram
plt.figure(figsize=(6,4))
plt.hist(all_unique_rates, 
         bins=np.arange(all_unique_rates.min(), all_unique_rates.max() + 2) - 0.5, 
         edgecolor='black', alpha=0.7, density=True)
plt.xlabel('AP rate')
plt.ylabel('# of neurons with this AP rate in their place map')
plt.xticks(np.unique(all_unique_rates))

def neg_log_likelihood(params, data):
    r, pp = params
    if r <= 0 or not (0 < pp < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, pp))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(all_unique_rates,), 
                  bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('# of neurons', fontsize=12)
plt.xlabel('AP rates (Hz)', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

#%% Percentage of map covered

p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
pcovered = [np.sum(pi.flatten()>0)/(pi.shape[0]*pi.shape[1]) for pi in p_trimmed]
plt.hist(pcovered, bins=100)

#%% What percentage of all maps do their largest place field have the largest rates?
# For each pi, check if the largest place field has the largest rate
matches = []
for pi in p:
    if np.all(pi == 0):
        matches.append(False)
        continue
    max_rate = pi[pi != 0].max()
    rate_with_largest_field = np.bincount(pi.flatten())[1:].argmax() + 1
    matches.append(rate_with_largest_field == max_rate)

# Calculate percentage
percentage = np.mean(matches)
print(percentage, "largest place field, largest rate")


#%% Smallest size largest rate?
# Check if the highest rate appears the least number of times in each pi
matches = []
for pi in p:
    if np.all(pi == 0):
        continue
    flat = pi.flatten()
    flat = flat[flat != 0]
    counts = np.bincount(flat)
    rates = np.arange(len(counts))
    rates = rates[1:]
    counts = counts[1:]
    max_rate = rates[np.argmax(rates)]
    min_count_rate = rates[np.argmin(counts)]
    matches.append(max_rate == min_count_rate)

# Compute percentage
percent = np.mean(matches)
print(percent, 'smallest place field, largest rate')

#%%
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import nbinom, weibull_min

data = all_unique_rates
data = data[data > 0]

# Histogram (density=True for PDF scale)
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# --- Negative log likelihood for Negative Binomial ---
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

# Fit Negative Binomial params
initial_guess = [5, 0.5]
result = minimize(neg_log_likelihood, initial_guess, args=(data,), 
                  bounds=[(1e-3, None), (1e-5, 1-1e-5)])
r_hat, p_hat = result.x

# Negative Binomial PMF over data range
x_plot = np.arange(data.min(), data.max() + 1)
pmf_fit = nbinom.pmf(x_plot, r_hat, p_hat)

# --- Weibull Fit ---
# Weibull is continuous, so we use MLE from scipy (force loc=0)
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x_wb = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x_wb, c_wb, loc=loc_wb, scale=scale_wb)

# To compare Weibull PDF with histogram and NB PMF, scale PDF:
# Multiply by bin width ~ 1 for visual comparison on histogram scale
pdf_wb_scaled = pdf_wb

# --- Plot ---
plt.figure(figsize=(6, 3))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.6, label='Data (histogram)', color='skyblue')
plt.plot(x_plot, pmf_fit, 'ro-', label=f'Negative Binomial Fit (r={r_hat:.2f}, p={p_hat:.2f})')
plt.plot(x_wb, pdf_wb_scaled, 'g-', label=f'Weibull Fit (k={c_wb:.2f}, scale={scale_wb:.2f})')
plt.xlabel('AP rate of a place field (Hz)')
plt.ylabel('Density of # of neurons')
plt.legend()
plt.tight_layout()
plt.show()

#%%

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import weibull_min, invgamma, fisk  # fisk = log-logistic

# Your positive data
data = all_unique_rates
data = data[data > 0]

# Histogram for visualization
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Fit Weibull (shape c), force loc=0
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x, c_wb, loc=loc_wb, scale=scale_wb)

# Fit Inverse Gamma (shape a), force loc=0
a_ig, loc_ig, scale_ig = invgamma.fit(data, floc=0)
pdf_ig = invgamma.pdf(x, a_ig, loc=loc_ig, scale=scale_ig)

# Fit Log-Logistic (Fisk), force loc=0
c_ll, loc_ll, scale_ll = fisk.fit(data, floc=0)
pdf_ll = fisk.pdf(x, c_ll, loc=loc_ll, scale=scale_ll)

# Function to find inflection points (zero crossings of second derivative)
def find_inflection_points(pdf_vals, x_vals):
    dx = x_vals[1] - x_vals[0]
    second_deriv = np.gradient(np.gradient(pdf_vals, dx), dx)
    zero_crossings = np.where(np.diff(np.sign(second_deriv)))[0]
    return x_vals[zero_crossings]

# Find inflection points for each PDF
inflect_wb = find_inflection_points(pdf_wb, x)
inflect_ig = find_inflection_points(pdf_ig, x)
inflect_ll = find_inflection_points(pdf_ll, x)

# Plot
plt.figure(figsize=(12, 6))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.5, label='Data (histogram)', color='skyblue')

plt.plot(x, pdf_wb, 'r-', label=f'Weibull (k={c_wb:.3f})')
for ip in inflect_wb:
    plt.axvline(ip, color='r', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ig, 'g-', label=f'Inverse Gamma (a={a_ig:.3f})')
for ip in inflect_ig:
    plt.axvline(ip, color='g', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ll, 'm-', label=f'Log-Logistic (c={c_ll:.3f})')
for ip in inflect_ll:
    plt.axvline(ip, color='m', linestyle='--', alpha=0.5)

plt.xlabel('Rate')
plt.ylabel('Probability density')
plt.title('Fits and Inflection Points of Weibull, Inverse Gamma, Log-Logistic')
plt.legend()
plt.tight_layout()
plt.show()

# Print inflection points
print("Inflection points (x values) where PDF curvature changes sign:")
print(f"Weibull: {inflect_wb}")
print(f"Inverse Gamma: {inflect_ig}")
print(f"Log-Logistic: {inflect_ll}")

#%% Mixture Model Fits: Gaussian, Beta-binomial, Binomial Mixtures + AIC

import numpy as np  # Numerical computing library
import matplotlib.pyplot as plt  # Plotting library
from scipy.stats import norm, betabinom, binom  # Probability distributions
from scipy.optimize import curve_fit, minimize  # Optimization and fitting tools

# --- Data Preparation ---

# Trim each array in 'p' if it has 117 columns (remove last column)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Remove the last row from all matrices to match expected size
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

# Binarize: convert all values > 0 to 1 (active), 0 otherwise
pth = [(pi > 0).astype(int) for pi in p_trimmed]

# Extract the shape of the arrays (should all be same shape)
pshape = pth[0].shape

# Grid size for patching
n = 1
grid = (n, n)

# Compute number of patches that fit in each dimension
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

# Initialize storage for patches and counts
plist = []
clist = []

# Divide each array into patches and count active neurons in each patch
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            # Extract n x n patch
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            # Count if any neuron is active in this patch
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Sum counts across all neurons, flatten to 1D array
csum = np.sum(clist, axis=0)
data = csum.flatten()

# Binning setup for histogram
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20  # Font size for plots

# --- Gaussian Mixture Model ---

# Define PDF of 2-component Gaussian mixture
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    return w1 * norm.pdf(x, mu1, sigma1) + (1 - w1) * norm.pdf(x, mu2, sigma2)

# Histogram of data (used for fitting)
hist_vals, _ = np.histogram(data, bins=binarr, density=True)

# Initial guess and bounds for fitting
p0 = [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]
bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3], [1, np.inf, np.inf, np.inf, np.inf])

# Fit Gaussian mixture model to histogram
params_gmm, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0, bounds=bounds)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV² for both Gaussian components
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# --- Beta-binomial Mixture Model ---

n_neurons = len(pth)  # Number of neurons

# Define 2-component beta-binomial mixture PMF
def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

# Negative log-likelihood for beta-binomial mixture
def nll_bb_mix(params):
    w, a1, b1, a2, b2 = params
    pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
    if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to fit parameters    w  a1 b1 a2 b2
res_bb_mix = minimize(nll_bb_mix, [0.5, 1, 2, 5, 5], bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x

# Generate PMF from fit
x_discrete = np.arange(0, n_neurons + 1)
pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)

# Compute mean, variance, CV² for each component
mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# --- Binomial Mixture Model ---

# Define 2-component binomial mixture PMF
def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# Negative log-likelihood for binomial mixture
def nll_binom_mix(params):
    w, p1, p2 = params
    pmf = pmf_binom_mix(data, w, p1, p2)
    if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to find best parameters
res_binom_mix = minimize(nll_binom_mix, [0.05, 0.1, 0.2], bounds=[(1e-3, 1 - 1e-3), (1e-5, 1 - 1e-5), (1e-5, 1 - 1e-5)])
w_bm, p1_bm, p2_bm = res_binom_mix.x
pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)

# Compute means, variances, CV² for both binomial components
mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# --- AIC Calculation ---

# Compute log-likelihoods
logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
logL_bbmix = np.sum(np.log(pmf_bb_mix(data, w_bb, a1_bb, b1_bb, a2_bb, b2_bb) + 1e-12))
logL_binmix = np.sum(np.log(pmf_binom_mix(data, w_bm, p1_bm, p2_bm) + 1e-12))

# AIC = 2k - 2*logL where k = number of fitted parameters
aic_gmm = 2 * 5 - 2 * logL_gmm
aic_bbmix = 2 * 5 - 2 * logL_bbmix
aic_binmix = 2 * 3 - 2 * logL_binmix

# Print AIC table
print("\nMixture Model AIC Comparison:")
print(f"{'Model':30} {'AIC':>10}")
print("-" * 42)
print(f"{'Gaussian Mixture':30} {aic_gmm:10.2f}")
print(f"{'Beta-binomial Mixture':30} {aic_bbmix:10.2f}")
print(f"{'Binomial Mixture':30} {aic_binmix:10.2f}")

# --- Plotting ---

plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

# Plot Gaussian Mixture
plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
         label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

# Plot Beta-binomial Mixture
plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

# Plot Binomial Mixture
plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'Binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({grid[0]},{grid[1]})-grid: Mixture Model Fits', fontsize=fs+6)
plt.xlabel(f'# place fields per ({grid[0]},{grid[1]}) patch', fontsize=fs)
plt.ylabel('Count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0]-1, binarr[-1]+1])
plt.show()

#%% Mixture Model Fits: Gaussian, Beta-binomial, Binomial Mixtures + AIC

import numpy as np  # Numerical computing library
import matplotlib.pyplot as plt  # Plotting library
from scipy.stats import norm, betabinom, binom  # Probability distributions
from scipy.optimize import curve_fit, minimize  # Optimization and fitting tools

# Prepare data

# Trim each array in 'p' if it has 117 columns (remove last column)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Remove the last row from all matrices to match expected size
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

# Binarize: convert all values > 0 to 1 (active), 0 otherwise
pth = [(pi > 0).astype(int) for pi in p_trimmed]

# Extract the shape of the arrays (should all be same shape)
pshape = pth[0].shape

# Grid size for patching
n = 1
grid = (n, n)

# Compute number of patches that fit in each dimension
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

# Initialize storage for patches and counts
plist = []
clist = []

# Divide each array into patches and count active neurons in each patch
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            # Extract n x n patch
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            # Count if any neuron is active in this patch
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Sum counts across all neurons, flatten to 1D array
csum = np.sum(clist, axis=0)
data = csum.flatten()

# Binning setup for histogram
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20  # Font size for plots

# Gaussian mixture model

# Define PDF of 2-component Gaussian mixture
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    return w1 * norm.pdf(x, mu1, sigma1) + (1 - w1) * norm.pdf(x, mu2, sigma2)

# Histogram of data (used for fitting)
hist_vals, _ = np.histogram(data, bins=binarr, density=True)

# Initial guess and bounds for fitting
p0 = [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]
bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3], [1, np.inf, np.inf, np.inf, np.inf])

# Fit Gaussian mixture model to histogram
params_gmm, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0, bounds=bounds)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV2 for both Gaussian components
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# Beta binomial mixture model

n_neurons = len(pth)  # Number of neurons

# Define 2-component beta-binomial mixture PMF
def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

# Negative log-likelihood for beta-binomial mixture
def nll_bb_mix(params):
    w, a1, b1, a2, b2 = params
    pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
    if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to fit parameters    w  a1 b1 a2 b2
res_bb_mix = minimize(nll_bb_mix, [0.5, 1, 2, 5, 5], bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x

# Generate PMF from fit
x_discrete = np.arange(0, n_neurons + 1)
pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)

# Compute mean, variance, CV² for each component
mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# Binomial mixture model

# Define 2-component binomial mixture PMF
def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# Negative log-likelihood for binomial mixture
def nll_binom_mix(params):
    w, p1, p2 = params
    pmf = pmf_binom_mix(data, w, p1, p2)
    if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to find best parameters
res_binom_mix = minimize(nll_binom_mix, [0.05, 0.1, 0.2], bounds=[(1e-3, 1 - 1e-3), (1e-5, 1 - 1e-5), (1e-5, 1 - 1e-5)])
w_bm, p1_bm, p2_bm = res_binom_mix.x
pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)

# Compute means, variances, CV² for both binomial components
mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# Get AICs for each model

# Compute log-likelihoods
logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
logL_bbmix = np.sum(np.log(pmf_bb_mix(data, w_bb, a1_bb, b1_bb, a2_bb, b2_bb) + 1e-12))
logL_binmix = np.sum(np.log(pmf_binom_mix(data, w_bm, p1_bm, p2_bm) + 1e-12))

# AIC = 2k - 2*logL where k = number of fitted parameter and L is the log likelihood
aic_gmm = 2 * 5 - 2 * logL_gmm
aic_bbmix = 2 * 5 - 2 * logL_bbmix
aic_binmix = 2 * 3 - 2 * logL_binmix

# Plotting
plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

# Plot Gaussian Mixture
plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
         label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

# Plot Beta-binomial Mixture
plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

# Plot Binomial Mixture
plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'Binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({grid[0]},{grid[1]})-grid: Mixture Model Fits', fontsize=fs+6)
plt.xlabel(f'# place fields per ({grid[0]},{grid[1]}) patch', fontsize=fs)
plt.ylabel('Count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0]-1, binarr[-1]+1])
plt.show()

# =============================================================================
# #%% Manual parameters
# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.stats import norm, betabinom, binom
# 
# # parameters 
# params_gmm = [0.5, 2.0, 1.0, 5.0, 1.5] # w, mu1, sigma1, mu2, sigma2
# params_bb  = [0.1, 1.0, 3.0, 5, 5]     # w, a1, b1, a2, b2
# params_bin = [0.2, 0.1, 0.17]          # w, p1, p2
# 
# # prepare data
# n = 4
# pth = [((pi[:, :-1] if pi.shape[1] == 117 else pi)[:-1, :] > 0).astype(int) for pi in p]
# x1, x2 = pth[0].shape[0] // n, pth[0].shape[1] // n
# csum = np.sum([[[np.any(pi[i*n:(i+1)*n, j*n:(j+1)*n])
#                  for j in range(x2)] for i in range(x1)] for pi in pth], axis=0)
# data = csum.flatten()
# 
# # mixture model preparations
# n_neurons = len(pth)
# binarr = np.arange(data.min() - 0.5, data.max() + 1.5)
# bin_centers = (binarr[:-1] + binarr[1:]) / 2
# x_discrete = np.arange(0, n_neurons + 1)
# 
# def gaussian_mixture(x, w, mu1, s1, mu2, s2):
#     return w * norm.pdf(x, mu1, s1) + (1 - w) * norm.pdf(x, mu2, s2)
# 
# def pmf_bb_mix(x, w, a1, b1, a2, b2):
#     return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)
# 
# def pmf_binom_mix(x, w, p1, p2):
#     return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)
# 
# # gaussian mixture model
# w1, mu1, s1, mu2, s2 = params_gmm
# y_gmm = gaussian_mixture(bin_centers, *params_gmm)
# cv2_1 = (s1 / mu1)**2 if mu1 else np.nan
# cv2_2 = (s2 / mu2)**2 if mu2 else np.nan
# logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
# aic_gmm = 2 * 5 - 2 * logL_gmm
# 
# # beta mixture model
# w_bb, a1_bb, b1_bb, a2_bb, b2_bb = params_bb
# pmf_bbmix = pmf_bb_mix(x_discrete, *params_bb)
# logL_bbmix = np.sum(np.log(pmf_bb_mix(data, *params_bb) + 1e-12))
# aic_bbmix = 2 * 5 - 2 * logL_bbmix
# 
# mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
# var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
# cv2_1_bb = var1_bb / mean1_bb ** 2
# 
# mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
# var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
# cv2_2_bb = var2_bb / mean2_bb ** 2
# 
# # binomial mixture model
# w_bm, p1_bm, p2_bm = params_bin
# pmf_binmix = pmf_binom_mix(x_discrete, *params_bin)
# logL_binmix = np.sum(np.log(pmf_binom_mix(data, *params_bin) + 1e-12))
# aic_binmix = 2 * 3 - 2 * logL_binmix
# 
# mean1_bm = n_neurons * p1_bm
# var1_bm = n_neurons * p1_bm * (1 - p1_bm)
# cv2_1_bm = var1_bm / mean1_bm ** 2
# 
# mean2_bm = n_neurons * p2_bm
# var2_bm = n_neurons * p2_bm * (1 - p2_bm)
# cv2_2_bm = var2_bm / mean2_bm ** 2
# 
# # plot results
# fs = 20
# plt.figure(figsize=(10, 8))
# plt.hist(data, bins=binarr, density=False,
#          color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')
# 
# plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
#          label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
#                f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')
# 
# plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
#          label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
#                f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')
# 
# plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
#          label=f'Binomial mixture\nn={n_neurons},\n'
#                f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
#                f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')
# 
# plt.title(f'({n},{n})-grid: Mixture Model Fits', fontsize=fs + 6)
# plt.xlabel(f'# place fields per ({n},{n}) patch', fontsize=fs)
# plt.ylabel('Count', fontsize=fs)
# plt.tick_params(axis='both', labelsize=fs)
# plt.legend(fontsize=fs - 2)
# plt.tight_layout()
# plt.xlim([binarr[0] - 1, binarr[-1] + 1])
# plt.show()
# 
# =============================================================================

#%% Fit all three version 2

import numpy as np  # numerical computing library
import matplotlib.pyplot as plt  # plotting library
from scipy.stats import norm, betabinom, binom  # probability distributions
from scipy.optimize import minimize  # optimization for likelihood-based fits
from sklearn.mixture import GaussianMixture  # gmm fitting to raw data

# prepare data

# trim each array in 'p' if it has 117 columns (remove last column)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# remove the last row from all matrices to match expected size
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

# binarize: convert all values > 0 to 1 (active), 0 otherwise
pth = [(pi > 0).astype(int) for pi in p_trimmed]

# extract the shape of the arrays (should all be same shape)
pshape = pth[0].shape

# grid size for patching
n = 1
grid = (n, n)

# compute number of patches that fit in each dimension
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

# initialize storage for patches and counts
plist = []
clist = []

# divide each array into patches and count active neurons in each patch
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            # extract n x n patch
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            # count if any neuron is active in this patch
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# sum counts across all neurons, flatten to 1d array
csum = np.sum(clist, axis=0)
data = csum.flatten()

# bin setup for plotting only
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20  # font size for plots

# gaussian mixture model

# fit gmm directly to raw data
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(data.reshape(-1, 1))
logL_gmm = gmm.score(data.reshape(-1, 1)) * len(data)
w1, w2 = gmm.weights_
mu1, mu2 = gmm.means_.flatten()
sigma1, sigma2 = np.sqrt(gmm.covariances_.flatten())

# compute cv² for each gaussian component
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# beta-binomial mixture model

n_neurons = len(pth)  # number of neurons (assumed number of trials per patch)

# define 2-component beta-binomial mixture pmf
def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

# negative log-likelihood for beta-binomial mixture
def nll_bb_mix(params):
    w, a1, b1, a2, b2 = params
    pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
    if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# fit parameters by minimizing nll
res_bb_mix = minimize(nll_bb_mix, [0.5, 1, 2, 5, 5], bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x
logL_bbmix = -res_bb_mix.fun

# compute mean, variance, cv² for both beta-binomial components
mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# binomial mixture model

# define 2-component binomial mixture pmf
def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# negative log-likelihood for binomial mixture
def nll_binom_mix(params):
    w, p1, p2 = params
    pmf = pmf_binom_mix(data, w, p1, p2)
    if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# minimize nll to fit parameters
res_binom_mix = minimize(nll_binom_mix, [0.05, 0.1, 0.2], bounds=[(1e-3, 1 - 1e-3), (1e-5, 1 - 1e-5), (1e-5, 1 - 1e-5)])
w_bm, p1_bm, p2_bm = res_binom_mix.x
logL_binmix = -res_binom_mix.fun

# compute means, variances, cv² for both binomial components
mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# compute aic for all models
aic_gmm = 2 * 5 - 2 * logL_gmm
aic_bbmix = 2 * 5 - 2 * logL_bbmix
aic_binmix = 2 * 3 - 2 * logL_binmix

# prepare x axis for discrete models
x_discrete = np.arange(0, n_neurons + 1)
pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)
pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)

# plotting
plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='histogram')

# plot gmm
gmm_pdf = w1 * norm.pdf(bin_centers, mu1, sigma1) + w2 * norm.pdf(bin_centers, mu2, sigma2)
plt.plot(bin_centers, gmm_pdf * len(data), 'r-', lw=5,
         label=f'gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

# plot beta-binomial mixture
plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

# plot binomial mixture
plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({grid[0]},{grid[1]})-grid: mixture model fits', fontsize=fs+6)
plt.xlabel(f'# place fields per ({grid[0]},{grid[1]}) patch', fontsize=fs)
plt.ylabel('count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0]-1, binarr[-1]+1]) # limit the plot to within the lower/upper bins
plt.show()

#%% (Third meeting) The different room shapes, histograms (GOOD ONE)

def plotresults(data, binnarr, bin_centers, n_neurons, i, dnames, gn):
    # beta-binomial mixture model
    def pmf_bb_mix(x, w, a1, b1, a2, b2):
        return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)
    
    def nll_bb_mix(params):
        w, a1, b1, a2, b2 = params
        pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
        if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
            return -np.sum(np.log(pmf + 1e-12))
        return np.inf
    
    res_bb_mix = minimize(nll_bb_mix, [0.5, 1, 2, 5, 5], bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
    w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x
    logL_bbmix = -res_bb_mix.fun
    aic_bbmix = 2 * 5 - 2 * logL_bbmix  # 5 parameters

    mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
    var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
    cv2_1_bb = var1_bb / mean1_bb ** 2
    
    mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
    var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
    cv2_2_bb = var2_bb / mean2_bb ** 2

    # binomial mixture model
    def pmf_binom_mix(x, w, p1, p2):
        return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)
    
    def nll_binom_mix(params):
        w, p1, p2 = params
        pmf = pmf_binom_mix(data, w, p1, p2)
        if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
            return -np.sum(np.log(pmf + 1e-12))
        return np.inf
    
    res_binom_mix = minimize(nll_binom_mix, [0.05, 0.1, 0.2],
                             bounds=[(1e-3, 1 - 1e-3),
                                     (1e-5, 1 - 1e-5),
                                     (1e-5, 1 - 1e-5)])
    w_bm, p1_bm, p2_bm = res_binom_mix.x
    logL_binmix = -res_binom_mix.fun
    aic_binmix = 2 * 3 - 2 * logL_binmix  # 3 parameters
    
    mean1_bm = n_neurons * p1_bm
    var1_bm = n_neurons * p1_bm * (1 - p1_bm)
    cv2_1_bm = var1_bm / mean1_bm ** 2
    
    mean2_bm = n_neurons * p2_bm
    var2_bm = n_neurons * p2_bm * (1 - p2_bm)
    cv2_2_bm = var2_bm / mean2_bm ** 2

    # single-component binomial
    def pmf_binom_single(x, p):
        return binom.pmf(x, n_neurons, p)
    
    def nll_binom_single(p):
        pmf = pmf_binom_single(data, p[0])
        if 0 < p[0] < 1:
            return -np.sum(np.log(pmf + 1e-12))
        return np.inf
    
    res_binom_single = minimize(nll_binom_single, [0.1], bounds=[(1e-5, 1-1e-5)])
    p_single = res_binom_single.x[0]
    logL_bin_single = -res_binom_single.fun
    mean_bin_single = n_neurons * p_single
    var_bin_single = n_neurons * p_single * (1 - p_single)
    cv2_bin_single = var_bin_single / mean_bin_single ** 2
    aic_bin_single = 2 * 1 - 2 * logL_bin_single

    # prepare x axis
    x_discrete = np.arange(0, n_neurons + 1)
    pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)
    pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)
    pmf_bin_single = pmf_binom_single(x_discrete, p_single)

    # plotting
    fs=15
    plt.figure(figsize=(10, 8))
    plt.hist(data, bins=binnarr, density=False, color='skyblue', edgecolor='black', alpha=0.6, label='histogram')
    
    plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
             label=f'beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f},\n'
                   f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\n w={w_bb:.2f}, AIC={aic_bbmix:.2f}')

    plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
             label=f'binomial mixture\nn={n_neurons},\n'
                   f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f},\n'
                   f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nw={w_bm:.2f}, AIC={aic_binmix:.2f}')
    
    plt.plot(x_discrete, pmf_bin_single * len(data), 'b-.', lw=5,
             label=f'binomial\np={p_single:.2f},n={n_neurons},\nμ={mean_bin_single:.2f}, CV²={cv2_bin_single:.2f}, AIC={aic_bin_single:.2f}')

    plt.title('{}, ({},{})-grid'.format(dnames[i], gn, gn), fontsize=fs+6)
    plt.xlabel(f'# place fields per ({gn},{gn}) patch', fontsize=fs)
    plt.ylabel('count', fontsize=fs)
    plt.tick_params(axis='both', labelsize=fs)
    plt.legend(fontsize=fs - 2)
    plt.tight_layout()
    plt.xlim([binnarr[0]-1, binnarr[-1]+1])
    plt.show()



# =============================================================================
# import numpy as np
# import os
# import matplotlib.pyplot as plt
# from scipy.io import loadmat
# 
# import numpy as np  # numerical computing library
# import matplotlib.pyplot as plt  # plotting library
# from scipy.stats import norm, betabinom, binom  # probability distributions
# from scipy.optimize import minimize  # optimization for likelihood-based fits
# from sklearn.mixture import GaussianMixture  # gmm fitting to raw data
# 
# # =================== Setup ===================
# dnames = ['BigroomData', 'CircleData', 'HalfroomData',
#           'HexagonData', 'RectangleData', 'SquareData']
# 
# shapeslist = [[(65, 116), (65, 117)], # BigroomData
#               [(43, 53), (43, 50), (43, 57)], # CircleData
#               [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)], # HalfroomData
#               [(12, 12), (13, 13)],  # HexagonData
#               [(24, 36), (31, 40)], # RectangleData
#               [(20, 20)]] # SquareData
# 
# extensions = [(68, 120), # 0
#               (44, 60),  # 1
#               (64, 60),  # 2
#               (16, 16),  # 3
#               (32, 40),  # 4
#               (20, 20)]  # 5
# 
# location = 0  # Change if needed
# 
# # =================== Collect and pad all data ===================
# p_extended = [[] for _ in range(6)]  # One list per dnametype
# plist=[]
# nlist=[]
# 
# for dnametype in range(6):
#     p = []
#     n = []
# 
#     # Set data directory
#     if location == 0:
#         dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
#     elif location == 1:
#         dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
#     
#     dir2 = os.listdir(dir1)
# 
#     for foldername in dir2:
#         dir3 = os.path.join(dir1, foldername)
#         if not os.path.isdir(dir3): continue
#         dirs3 = os.listdir(dir3)
#         dirs4 = [x for x in dirs3 if x[-7] == 'm']
#         for matname in dirs4:
#             mat = loadmat(os.path.join(dir3, matname))
#             n.append(mat['numFields'][0][0])
#             p.append(mat['pfields'])
# 
#     # Pad each p matrix to the shape in extensions[dnametype]
#     target_shape = extensions[dnametype]
#     padded_list = []
#     for pi in p:
#         h, w = pi.shape
#         H, W = target_shape
#         pad_bottom = H - h
#         pad_right = W - w
#         padded = np.pad(pi, ((0, pad_bottom), (0, pad_right)), mode='constant')
#         padded_list.append(padded)
# 
#     p_extended[dnametype] = padded_list  # Store for this dnametype
#     nlist.append(n)
#     plist.append(p)
# 
# i=0
# plt.imshow(plist[i][0])  # Show first from CircleData
# plt.title('{}'.format(dnames[i]))
# plt.show()
# 
# plt.imshow(p_extended[i][0])  # Show first from CircleData
# plt.title('{} extended'.format(dnames[i]))
# plt.show()
# =============================================================================

# =============================================================================
# # Plot all the maps for some room data
# i=1
# for x in plist[i]:
#     plt.figure()
#     plt.imshow(x)
#     plt.show()
# =============================================================================

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import beta, norm, betabinom, binom, gamma, lognorm  # probability distributions
from scipy.optimize import minimize  # optimization for likelihood-based fits
from sklearn.mixture import GaussianMixture  # gmm fitting to raw data

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import Counter

# =================== Setup ===================
dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [[(65, 116), (65, 117)],  # BigroomData
              [(43, 53), (43, 50), (43, 57)],  # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)],  # RectangleData
              [(20, 20)]]  # SquareData

location = 0  # Change if needed

# =================== Collect all data ===================
# =============================================================================
# 
# # Maximum dimensions
# p_extended = [[] for _ in range(len(dnames))]  # One list per dataset
# plist = []
# nlist = []
# 
# for dnametype in range(len(dnames)):
#     p = []
#     n = []
# 
#     # Set data directory
#     if location == 0:
#         dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
#     elif location == 1:
#         dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
#     
#     dir2 = os.listdir(dir1)
# 
#     for foldername in dir2:
#         dir3 = os.path.join(dir1, foldername)
#         if not os.path.isdir(dir3):
#             continue
#         dirs3 = os.listdir(dir3)
#         dirs4 = [x for x in dirs3 if x[-7] == 'm']
#         for matname in dirs4:
#             mat = loadmat(os.path.join(dir3, matname))
#             n.append(mat['numFields'][0][0])
#             p.append(mat['pfields'])  # original, unmodified matrices
# 
#     # =================== Count original shapes with proportion ===================
#     original_shapes = [pmap.shape for pmap in p]
#     shape_counts = Counter(original_shapes)
#     total_maps = len(original_shapes)
#     
#     print(f"{dnames[dnametype]} ({total_maps})")
#     for shape, count in shape_counts.items():
#         proportion = count / total_maps
#         print(f"{shape}: {count} ({proportion:.2f})")
#     print()
# 
#     # =================== Compute max shape and pad (original code) ===================
#     max_x = max(x for x, y in shapeslist[dnametype])
#     max_y = max(y for x, y in shapeslist[dnametype])
#     target_shape = (max_x, max_y)
# 
#     padded_list = []
#     for pi in p:
#         h, w = pi.shape
#         H, W = target_shape
#         pad_bottom = H - h
#         pad_right = W - w
#         padded = np.pad(pi, ((0, pad_bottom), (0, pad_right)), mode='constant')
#         padded_list.append(padded)
# 
#     p_extended[dnametype] = padded_listO
#     nlist.append(n)
#     plist.append(p)
# =============================================================================

# Minimum dimensions

dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [[(65, 116), (65, 117)],  # BigroomData
              [(43, 53), (43, 50), (43, 57)],  # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)],  # RectangleData
              [(20, 20)]]  # SquareData

location = 0  # Change if needed

# =================== Collect all data ===================
p_extended = [[] for _ in range(len(dnames))]  # One list per dataset
plist = []
nlist = []

for dnametype in range(len(dnames)):
    p = []
    n = []

    # Set data directory
    if location == 0:
        dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
    elif location == 1:
        dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
    
    dir2 = os.listdir(dir1)

    for foldername in dir2:
        dir3 = os.path.join(dir1, foldername)
        if not os.path.isdir(dir3):
            continue
        dirs3 = os.listdir(dir3)
        dirs4 = [x for x in dirs3 if x[-7] == 'm']
        for matname in dirs4:
            mat = loadmat(os.path.join(dir3, matname))
            n.append(mat['numFields'][0][0])
            p.append(mat['pfields'])  # original, unmodified matrices

    # =================== Count original shapes with proportion ===================
    original_shapes = [pmap.shape for pmap in p]
    shape_counts = Counter(original_shapes)
    total_maps = len(original_shapes)
    
    print(f"{dnames[dnametype]} ({total_maps})")
    for shape, count in shape_counts.items():
        proportion = count / total_maps
        print(f"{shape}: {count} ({proportion:.2f})")
    print()

    # =================== Compute minimum shape and crop ===================
    min_x = min(x for x, y in shapeslist[dnametype])
    min_y = min(y for x, y in shapeslist[dnametype])
    target_shape = (min_x, min_y)

    cropped_list = []
    for pi in p:
        h, w = pi.shape
        H, W = target_shape
        cropped = pi[:H, :W]  # crop to minimum dimensions
        cropped_list.append(cropped)

    p_extended[dnametype] = cropped_list
    nlist.append(n)
    plist.append(p)


#%%  % of the room
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, gamma, norm, lognorm

# =================== Helper: Fit Distributions ===================
def best_fit_distribution(data):
    """
    Try several distributions and return the one with smallest AIC.
    """
    candidates = {
        "beta": beta,
        "gamma": gamma,
        "normal": norm,
        "lognorm": lognorm
    }

    best_name = None
    best_params = None
    best_aic = np.inf

    for name, dist in candidates.items():
        try:
            x = np.array(data)
            if name == "beta":
                x_scaled = x / 100.0  # scale 0-100 to 0-1
                params = dist.fit(x_scaled, floc=0, fscale=1)
                ll = np.sum(dist.logpdf(x_scaled, *params))
            elif name == "lognorm":
                params = dist.fit(x, floc=0)
                ll = np.sum(dist.logpdf(x, *params))
            else:
                params = dist.fit(x)
                ll = np.sum(dist.logpdf(x, *params))

            k = len(params)
            aic = 2*k - 2*ll
            if aic < best_aic:
                best_name = name
                best_params = params
                best_aic = aic
        except Exception:
            continue

    return best_name, best_params

# =================== Compute % of room coverage and fit ===================
roomtype_values = []

for dnametype, padded_maps in enumerate(p_extended):
    if not padded_maps:
        continue

    H, W = padded_maps[0].shape
    total_pixels = H * W

    # Compute % room coverage (binary)
    vals = [np.sum(m > 0) / total_pixels * 100 for m in padded_maps]
    roomtype_values.append(vals)

    # Fit best distribution
    dist_name, params = best_fit_distribution(vals)
    print(f"{dnames[dnametype]}: best fit = {dist_name}  params = {params}")

    # Calculate mean and CV^2 for the chosen distribution
    mean, cv2 = None, None
    if dist_name == "gamma":
        k, loc, theta = params
        mean = k * theta + loc
        cv2 = 1.0 / k
    elif dist_name == "beta":
        a, b, loc, scale = params
        mean = loc + scale * a / (a + b)
        var = scale**2 * a*b / ((a+b)**2 * (a+b+1))
        cv2 = var / mean**2
    elif dist_name == "lognorm":
        s, loc, scale = params
        mean = scale * np.exp(s**2 / 2) + loc
        var = (scale**2) * (np.exp(s**2) - 1) * np.exp(s**2) 
        cv2 = var / mean**2
    elif dist_name == "normal":
        mu, sigma = params
        mean = mu
        cv2 = (sigma / mu)**2 if mu != 0 else np.nan

    # Plot histogram of counts
    plt.figure(figsize=(6, 4))
    count, bins, _ = plt.hist(
        vals, bins=20,
        color='lightblue', edgecolor='k', alpha=0.7,
        label='Data (counts)'
    )

    # Scale PDF to histogram counts
    bin_width = bins[1] - bins[0]
    n = len(vals)
    x = np.linspace(min(vals), max(vals), 200)

    if dist_name == "beta":
        pdf = beta.pdf(x / 100.0, *params) / 100.0
    elif dist_name == "lognorm":
        pdf = lognorm.pdf(x, *params)
    elif dist_name == "gamma":
        pdf = gamma.pdf(x, *params)
    else:
        pdf = norm.pdf(x, *params)

    plt.plot(x, pdf * n * bin_width, 'r-', lw=2,
             label=f"{dist_name} fit (mean={mean:.2f}, CV²={cv2:.2f})")

    plt.ylim(0, max(count) * 1.05)
    plt.xlabel("% of room covered by place maps")
    plt.ylabel("Count")
    plt.title(f"{dnames[dnametype]} (N={len(vals)})")
    plt.legend()
    plt.tight_layout()
    plt.show()
    
#%% % of field for each individual place map
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, gamma, norm, lognorm
from scipy.ndimage import label

# ---------- Helper: find contiguous place fields ----------
def extract_field_percents(binary_map):
    H, W = binary_map.shape
    total_pixels = H * W
    structure = np.ones((3, 3), dtype=int)  # 8-connectivity
    labeled, n_fields = label(binary_map > 0, structure=structure)
    return [(labeled == k).sum() / total_pixels * 100 for k in range(1, n_fields + 1)]

# ---------- Fit distributions & choose best by AIC ----------
def best_fit_distribution(data):
    x = np.asarray(data)
    candidates = {"beta": beta, "gamma": gamma, "normal": norm, "lognorm": lognorm}
    best_name, best_params, best_aic = None, None, np.inf
    best_mean, best_cv2 = None, None

    for name, dist in candidates.items():
        try:
            if name == "beta":
                x_scaled = x / 100.0
                params = dist.fit(x_scaled, floc=0, fscale=1)
                ll = np.sum(dist.logpdf(x_scaled, *params))
                a, b, loc, scale = params
                mean = loc + scale * a / (a + b)
                var = scale**2 * a * b / ((a + b)**2 * (a + b + 1))
                cv2 = var / mean**2
            elif name == "gamma":
                params = dist.fit(x)
                ll = np.sum(dist.logpdf(x, *params))
                k, loc, theta = params
                mean = k * theta + loc
                cv2 = 1.0 / k
            elif name == "lognorm":
                params = dist.fit(x, floc=0)
                ll = np.sum(dist.logpdf(x, *params))
                s, loc, scale = params
                mean = scale * np.exp(s**2 / 2) + loc
                var = (scale**2) * (np.exp(s**2) - 1) * np.exp(s**2)
                cv2 = var / mean**2
            else:  # normal
                params = dist.fit(x)
                ll = np.sum(dist.logpdf(x, *params))
                mu, sigma = params
                mean = mu
                cv2 = (sigma / mu)**2 if mu != 0 else np.nan

            k = len(params)
            aic = 2 * k - 2 * ll
            if aic < best_aic:
                best_name, best_params, best_aic = name, params, aic
                best_mean, best_cv2 = mean, cv2
        except Exception:
            continue

    return best_name, best_params, best_mean, best_cv2

# ---------- Compute % of room per *field* and plot ----------
field_values = []

for dnametype, padded_maps in enumerate(p_extended):
    if not padded_maps:
        continue

    vals = []
    for m in padded_maps:
        vals.extend(extract_field_percents(m))
    if not vals:
        continue
    field_values.append(vals)

    # Fit and select best
    dist_name, params, mean, cv2 = best_fit_distribution(vals)
    print(f"\n{dnames[dnametype]} -- Best fit: {dist_name}  mean={mean:.2f}  CV²={cv2:.2f}")

    # Histogram
    plt.figure(figsize=(6, 4))
    counts, bins, _ = plt.hist(vals, bins=30,
                               color="lightblue", edgecolor="k", alpha=0.7,
                               label="Data (fields)")
    bin_width = bins[1] - bins[0]
    n = len(vals)
    x = np.linspace(min(vals), max(vals), 200)

    # Only the best PDF
    if dist_name == "beta":
        pdf = beta.pdf(x / 100.0, *params) / 100.0
    elif dist_name == "gamma":
        pdf = gamma.pdf(x, *params)
    elif dist_name == "lognorm":
        pdf = lognorm.pdf(x, *params)
    else:
        pdf = norm.pdf(x, *params)

    plt.plot(x, pdf * n * bin_width, 'r-', lw=2,
             label=f"{dist_name} fit (mean={mean:.2f}, CV²={cv2:.2f})")

    plt.ylim(0, max(counts) * 1.05)
    plt.xlabel("% of room covered by individual place fields")
    plt.ylabel("Count")
    plt.title(f"{dnames[dnametype]} (Fields N={len(vals)})")
    plt.legend(fontsize=8)
    plt.tight_layout()
    plt.show()
    
    
#%% Thresholded version

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, gamma, norm, lognorm
from scipy.ndimage import label

# ---------- Helper: find contiguous place fields ----------
# ---------- Helper: find contiguous place fields with threshold ----------
def extract_field_percents(binary_map, pixel_threshold=0):
    """
    Return list of % of total pixels for each contiguous patch of 1's
    that contains at least `pixel_threshold` pixels.
    """
    H, W = binary_map.shape
    total_pixels = H * W
    structure = np.ones((3, 3), dtype=int)  # 8-connectivity
    labeled, n_fields = label(binary_map > 0, structure=structure)

    sizes = []
    for k in range(1, n_fields + 1):
        field_size = (labeled == k).sum()
        if field_size > pixel_threshold:  # Apply threshold
            sizes.append(field_size / total_pixels * 100)
    return sizes

# ---------- Fit distributions & choose best by AIC ----------
def best_fit_distribution(data):
    x = np.asarray(data)
    candidates = {"beta": beta, "gamma": gamma, "normal": norm, "lognorm": lognorm}
    best_name, best_params, best_aic = None, None, np.inf
    best_mean, best_cv2 = None, None

    for name, dist in candidates.items():
        try:
            if name == "beta":
                x_scaled = x / 100.0
                params = dist.fit(x_scaled, floc=0, fscale=1)
                ll = np.sum(dist.logpdf(x_scaled, *params))
                a, b, loc, scale = params
                mean = loc + scale * a / (a + b)
                var = scale**2 * a * b / ((a + b)**2 * (a + b + 1))
                cv2 = var / mean**2
            elif name == "gamma":
                params = dist.fit(x)
                ll = np.sum(dist.logpdf(x, *params))
                k, loc, theta = params
                mean = k * theta + loc
                cv2 = 1.0 / k
            elif name == "lognorm":
                params = dist.fit(x, floc=0)
                ll = np.sum(dist.logpdf(x, *params))
                s, loc, scale = params
                mean = scale * np.exp(s**2 / 2) + loc
                var = (scale**2) * (np.exp(s**2) - 1) * np.exp(s**2)
                cv2 = var / mean**2
            else:  # normal
                params = dist.fit(x)
                ll = np.sum(dist.logpdf(x, *params))
                mu, sigma = params
                mean = mu
                cv2 = (sigma / mu)**2 if mu != 0 else np.nan

            k = len(params)
            aic = 2 * k - 2 * ll
            if aic < best_aic:
                best_name, best_params, best_aic = name, params, aic
                best_mean, best_cv2 = mean, cv2
        except Exception:
            continue

    return best_name, best_params, best_mean, best_cv2

min_pixels = 10   # minimum pixel count per place field to include

field_values = []
for dnametype, padded_maps in enumerate(p_extended):
    if not padded_maps:
        continue

    vals = []
    for m in padded_maps:
        vals.extend(extract_field_percents(m, pixel_threshold=min_pixels))
    if not vals:
        continue
    field_values.append(vals)

    # Fit and select best
    dist_name, params, mean, cv2 = best_fit_distribution(vals)
    print(f"\n{dnames[dnametype]} -- Best fit: {dist_name}  mean={mean:.2f}  CV²={cv2:.2f}")

    # Histogram
    plt.figure(figsize=(6, 4))
    counts, bins, _ = plt.hist(vals, bins=30,
                               color="lightblue", edgecolor="k", alpha=0.7,
                               label=f"Data (fields >{min_pixels} px)")
    bin_width = bins[1] - bins[0]
    n = len(vals)
    x = np.linspace(min(vals), max(vals), 200)

    # Only the best PDF
    if dist_name == "beta":
        pdf = beta.pdf(x / 100.0, *params) / 100.0
    elif dist_name == "gamma":
        pdf = gamma.pdf(x, *params)
    elif dist_name == "lognorm":
        pdf = lognorm.pdf(x, *params)
    else:
        pdf = norm.pdf(x, *params)

    plt.plot(x, pdf * n * bin_width, 'r-', lw=2,
             label=f"{dist_name} fit (mean={mean:.2f}, CV²={cv2:.2f})")
    plt.ylim(0, max(counts) * 1.05)
    plt.xlabel("% of room covered by individual place fields")
    plt.ylabel("Count")
    plt.title(f"{dnames[dnametype]} (Fields N={len(vals)})")
    plt.legend(fontsize=8)
    plt.tight_layout()
    plt.show()

#%%  Visualize a place field map
i = 0  # pick dataset index
imagenum=2
plt.imshow(plist[i][imagenum])  # Show original first matrix
plt.title(f'{dnames[i]} original')
plt.show()

plt.imshow(p_extended[i][imagenum])  # Show padded version
plt.title(f'{dnames[i]} padded')
plt.show()
#%% Plot all the images of an individual room
i = 0  # pick dataset index
for x in plist[i]:
    plt.figure()
    plt.imshow(x)
    plt.title(x.shape, i)
    plt.show()
#%% Plot all the histograms of # of place fields
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom
from scipy.optimize import minimize

# Choose index of sublist
i = 5  # The room index (0 bigroom)
data = np.array(nlist[i])  # Use exactly 'data = nlist[i]'

# === Negative Binomial Fit ===
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(data,), bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

# === Plot histogram and fitted PMF ===
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

plt.figure(figsize=(13,5))
plt.hist(data, bins=binarr, color='skyblue', edgecolor='black',
         alpha=0.7, density=False, label='Histogram')

pmf = nbinom.pmf(binplot, rhat, phat)
pmf_scaled = pmf * len(data)  # Scale to match histogram counts
plt.plot(binplot, pmf_scaled, 'ro-', label=f'NegBin (r={rhat:.2f}, p={phat:.2f})')
fs=20
plt.title('{}'.format(dnames[i]), fontsize=fs)
plt.ylabel('# of neurons', fontsize=fs)
plt.xlabel('# of place fields', fontsize=fs)
plt.xticks(binplot)
plt.tick_params(axis='both', labelsize=fs-5)
plt.plot([],[], '', label='mean={}, FF={}'.format(np.around(np.mean(data),3), np.around(np.var(data)/np.mean(data),3)))
plt.legend(fontsize=fs)
plt.show()

#%%  Plot histogram of # of place fields finding best distribution
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom, poisson
from scipy.optimize import minimize

# -------------------------------------------
# Helper functions
# -------------------------------------------
def negbin_nll(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

def aic(nll, k):
    return 2*k + 2*nll

# -------------------------------------------
# Loop through each dataset
# -------------------------------------------
for i, roomname in enumerate(dnames):
    data = np.array(nlist[i])
    if len(data) == 0:
        print(f"{roomname}: no data")
        continue

    # ---------- Negative Binomial ----------
    nb_guess = [2.0, 0.3]
    nb_result = minimize(negbin_nll, nb_guess, args=(data,),
                         bounds=[(1e-3,None),(1e-5,1-1e-5)])
    rhat, phat = nb_result.x
    nll_nb = negbin_nll([rhat, phat], data)
    aic_nb = aic(nll_nb, 2)
    mean_nb = rhat * (1 - phat) / phat
    var_nb  = rhat * (1 - phat) / (phat**2)
    ff_nb   = var_nb / mean_nb

    # ---------- Poisson ----------
    lam_hat = np.mean(data)
    nll_pois = -np.sum(poisson.logpmf(data, lam_hat))
    aic_pois = aic(nll_pois, 1)
    mean_pois = lam_hat
    var_pois  = lam_hat
    ff_pois   = var_pois / mean_pois  # should be 1

    # ---------- Plot ----------
    binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
    binplot = np.arange(np.min(data), np.max(data) + 1)

    plt.figure(figsize=(13,5))
    # histogram
    plt.hist(data, bins=binarr, color='skyblue', edgecolor='black',
             alpha=0.7, density=False, label='Histogram')

    # Negative Binomial PMF
    pmf_nb = nbinom.pmf(binplot, rhat, phat)
    plt.plot(binplot, pmf_nb*len(data), 'ro-', 
             label=(f'NegBin( r={rhat:.2f}, p={phat:.2f}, '
                    f'mean={mean_nb:.2f}, FF={ff_nb:.2f}, AIC={aic_nb:.1f})'))

    # Poisson PMF
    pmf_pois = poisson.pmf(binplot, lam_hat)
    plt.plot(binplot, pmf_pois*len(data), 'go-',
             label=(f'Poisson(λ={lam_hat:.2f}, '
                    f'mean={mean_pois:.2f}, FF={ff_pois:.2f}, AIC={aic_pois:.1f})'))

    # Figure formatting
    fs = 18
    plt.title(roomname, fontsize=fs)
    plt.ylabel('# of neurons', fontsize=fs)
    plt.xlabel('# of place fields', fontsize=fs)
    plt.xticks(binplot)
    plt.tick_params(axis='both', labelsize=fs-4)
    plt.legend(fontsize=fs-4)
    plt.tight_layout()
    plt.show()

    print(f"\n{roomname}")
    print("-------------------------")
    print(f"NegBin:  r={rhat:.3f}, p={phat:.3f}, mean={mean_nb:.3f}, FF={ff_nb:.3f}, AIC={aic_nb:.2f}")
    print(f"Poisson: λ={lam_hat:.3f}, mean={mean_pois:.3f}, FF={ff_pois:.3f}, AIC={aic_pois:.2f}")



#%% Get the counts and the patches
p1 = [[(p_extended[i][j]>0)*1 for j in range(len(p_extended[i]))] for i in range(len(p_extended))]
gn = 1  # 1,2,4, grid number
grid = (gn, gn)
plist=[]
clist=[]
for z, pi in enumerate(p1): # for each list of rooms in each room list
    pshape = pi[0].shape
    x1 = pshape[0] // grid[0]
    x2 = pshape[1] // grid[1]
    plisti = []
    clisti = []
    for q, pii in enumerate(pi):
        patches = np.zeros((x1, x2), dtype=object)
        counts = np.zeros((x1, x2))
        for i in range(x1):
            for j in range(x2):
                print(i, x1, j, x1, q, z)
                patch = pii[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
                count = 1 * np.any(patch > 0)
                patches[i, j] = patch
                counts[i, j] = count
        plisti.append(patches)
        clisti.append(counts)
    plist.append(plisti)
    clist.append(clisti)
    
csumlist=[np.sum(c, axis = 0) for c in clist]
datalist=[c.flatten() for c in csumlist]

# Remove zeros from flattened data
datalist = [c.flatten()[c.flatten() != 0] for c in csumlist] 
binnlist=[np.arange(np.min(data) - 0.5, np.max(data) + 1.5) for data in datalist]
bin_centerslist = [(binarr[:-1] + binarr[1:]) / 2 for binarr in binnlist]

#%% Plot the different room data histograms

i=5 # 0 bigroom, 1
data=datalist[i]
binarr=binnlist[i]
bin_centers=bin_centerslist[i]
n_neurons = len(p_extended[i])

# =============================================================================
# plt.hist(data, bins=binarr, density=False,
#          color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')
# plt.xlabel('# of neurons firing APs in an ({},{})-grid'.format(n,n))
# plt.ylabel('Counts')
# plt.title('{}, ({},{})-grid'.format(dnames[i], n, n))
# plt.show()
# =============================================================================

plotresults(data, binarr, bin_centers, n_neurons, i, dnames, gn)

#%%

# psums = [np.sum(pi, axis=0) for pi in p1]
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

fig, ax = plt.subplots()
i=5
im = ax.imshow(psums[i], cmap='nipy_spectral')
ax.set_title(dnames[i])

# Attach a new axis to the right of the plot
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.05)

# Make colorbar in the attached axis
cbar = fig.colorbar(im, cax=cax)

plt.show()

#%% Get complexity scores

from scipy.ndimage import label
import numpy as np
from skimage.measure import perimeter
from scipy.ndimage import binary_fill_holes
from skimage.morphology import convex_hull_image
def extract_shapes_from_plist(plist):
    
    all_shapes = []  # shape[pi][pii] = list of 2D arrays (one per shape)

    for pi in plist:
        pi_shapes = []
        for pii in pi:
            unique_vals = np.unique(pii)
            shapes_for_pii = []

            for val in unique_vals:
                if val == 0:
                    continue  # skip background if zero

                binary_mask = (pii == val)
                labeled_array, num_features = label(binary_mask, structure=np.ones((3, 3)))

                for i in range(1, num_features + 1):
                    shape_mask = (labeled_array == i)
                    shapes_for_pii.append(shape_mask.astype(np.uint8))

            pi_shapes.append(shapes_for_pii)
        all_shapes.append(pi_shapes)

    return all_shapes

def get_shape_score_single_shape(binary_image):
    area = np.sum(binary_image)
    perim = perimeter(binary_image)
    convex = convex_hull_image(binary_image)
    convex_area = np.sum(convex)
    filled = binary_fill_holes(binary_image)
    holes = np.sum(filled) - area

    compactness = 4 * np.pi * area / (perim ** 2) if perim > 0 else 0
    convexity_defect = 1 - (area / convex_area) if convex_area > 0 else 0

    holes_score = holes * 2.0
    convexity_score = convexity_defect * 10.0
    compactness_score = compactness_score = max(0, (1.0 - compactness)) * 10.0

    perimeter_score = perim

    score = holes_score + convexity_score + compactness_score + perimeter_score
    return score

pshapes = extract_shapes_from_plist(plist)
ptest = pshapes[0][0][0]
pscores=[[[get_shape_score_single_shape(x) for x in pineurons] for pineurons in pshapelist] for pshapelist in pshapes]

#%% Look at the histogram of scores

psflat=[]
for pi in pscores:
    uu=[]
    for x in pi:
        for y in x:
            uu.append(y)
    psflat.append(uu)
    
#%%

i=0
fs=15
plt.hist(psflat[i], bins=200)
plt.title('{}'.format(dnames[i]), fontsize=fs)
plt.xlabel('complexity score', fontsize=fs)
plt.ylabel('Counts', fontsize=fs)
plt.show()

#%%

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Function to find best-fitting distribution
def fit_best_distribution(data):
    distributions = [
        stats.norm,
        stats.lognorm,
        stats.expon,
        stats.gamma,
        stats.beta
    ]

    best_fit = None
    best_params = None
    best_negloglik = np.inf

    for dist in distributions:
        try:
            params = dist.fit(data)
            loglik = np.sum(dist.logpdf(data, *params))
            negloglik = -loglik

            if negloglik < best_negloglik:
                best_negloglik = negloglik
                best_fit = dist
                best_params = params
        except Exception:
            continue

    return best_fit, best_params

# === Example Usage ===
i = 0  # Change as needed
data = np.array(psflat[i])

# Fit the best distribution
best_dist, best_params = fit_best_distribution(data)

# === Plot histogram with counts ===
counts, bins, _ = plt.hist(data, bins=30, density=False,
                           alpha=0.6, color='skyblue', 
                           edgecolor='black', label='data')

# Midpoints for plotting
x = np.linspace(min(data), max(data), 1000)
pdf = best_dist.pdf(x, *best_params)

# Scale PDF to match histogram
bin_width = bins[1] - bins[0]
pdf_scaled = pdf * len(data) * bin_width

# Compute mean and CV^2 from fitted distribution
mean = best_dist.mean(*best_params)
std = best_dist.std(*best_params)
cv2 = (std / mean) ** 2 if mean != 0 else np.inf

# Plot scaled PDF
label_text = f'{best_dist.name} fit\nmean = {mean:.2f}, CV² = {cv2:.2f}'
plt.plot(x, pdf_scaled, 'r-', lw=2, label=label_text)

# Final plot styling
plt.title(f"{dnames[i]} - Best Fit: {best_dist.name}", fontsize=fs)
plt.xlabel("Complexity Score", fontsize=fs)
plt.ylabel("Counts", fontsize=fs)
plt.legend(fontsize=fs-5)
plt.tight_layout()
plt.ylim([0, np.max(counts)+25])
# plt.ylim([0, 50])
plt.show()

#%%  Plot the complexity scores over the original image
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
import numpy as np

import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
import numpy as np
from skimage import measure  # for contours

def plot_scores_on_original(int_array):
    unique_vals = np.unique(int_array)
    unique_vals = unique_vals[unique_vals != 0]  # skip background zero

    fig, ax = plt.subplots()
    ax.imshow(int_array)  # Show original image
    
    for val in unique_vals:
        binary_mask = (int_array == val)
        labeled_array, num_features = label(binary_mask, structure=np.ones((3, 3)))

        for i in range(1, num_features + 1):
            shape_mask = (labeled_array == i).astype(np.uint8)
            score = get_shape_score_single_shape(shape_mask)
            cy, cx = center_of_mass(shape_mask)
            ax.text(cx, cy, f"{score:.2f}", color='red', fontsize=10,
                    ha='center', va='center', weight='bold')
    
    ax.axis('off')
    plt.title('BigroomData complexity scores')
    plt.show()



plot_scores_on_original(p[0])

#%% Setup for centroids (Meeting 5)
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import beta, norm, betabinom, binom, gamma, lognorm  # probability distributions
from scipy.optimize import minimize  # optimization for likelihood-based fits
from sklearn.mixture import GaussianMixture  # gmm fitting to raw data

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import Counter

# =================== Setup ===================
dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [[(65, 116), (65, 117)],  # BigroomData
              [(43, 53), (43, 50), (43, 57)],  # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)],  # RectangleData
              [(20, 20)]]  # SquareData

location = 0  # Change if needed

# =================== Collect all data ===================
p_extended = [[] for _ in range(len(dnames))]  # One list per dataset
plist = []
nlist = []

for dnametype in range(len(dnames)):
    p = []
    n = []

    # Set data directory
    if location == 0:
        dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
    elif location == 1:
        dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
    
    dir2 = os.listdir(dir1)

    for foldername in dir2:
        dir3 = os.path.join(dir1, foldername)
        if not os.path.isdir(dir3):
            continue
        dirs3 = os.listdir(dir3)
        dirs4 = [x for x in dirs3 if x[-7] == 'm']
        for matname in dirs4:
            mat = loadmat(os.path.join(dir3, matname))
            n.append(mat['numFields'][0][0])
            p.append(mat['pfields'])  # original, unmodified matrices

    # =================== Count original shapes with proportion ===================
    original_shapes = [pmap.shape for pmap in p]
    shape_counts = Counter(original_shapes)
    total_maps = len(original_shapes)
    
    print(f"{dnames[dnametype]} ({total_maps})")
    for shape, count in shape_counts.items():
        proportion = count / total_maps
        print(f"{shape}: {count} ({proportion:.2f})")
    print()

    # =================== Compute minimum shape and crop ===================
    min_x = min(x for x, y in shapeslist[dnametype])
    min_y = min(y for x, y in shapeslist[dnametype])
    target_shape = (min_x, min_y)

    cropped_list = []
    for pi in p:
        h, w = pi.shape
        H, W = target_shape
        cropped = pi[:H, :W]  # crop to minimum dimensions
        cropped_list.append(cropped)

    p_extended[dnametype] = cropped_list
    nlist.append(n)
    plist.append(p)
    
    
p1=[]
for room in plist:
    plistroom=[]
    for neuron in room:
        plistroom.append(1*(neuron>0))
    p1.append(plistroom)

#%%  Get the centroids

from scipy.ndimage import label, center_of_mass
import matplotlib.pyplot as plt
import numpy as np

# Define 8-connectivity structure
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# =================== Compute centroids ===================
# centroids[i][j] contains a list of centroids for each contiguous patch of 1's
centroids = []

for room in p1:
    room_centroids = []
    for neuron_map in room:
        # Label contiguous patches with 8-connectivity
        labeled_array, num_features = label(neuron_map, structure=structure)
        # Compute centroid of each labeled patch
        neuron_centroids = center_of_mass(neuron_map, labeled_array, range(1, num_features + 1))
        room_centroids.append(neuron_centroids)
    centroids.append(room_centroids)
    
#%% distnace to nearest wall
    
import numpy as np
from scipy.ndimage import label, center_of_mass

dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

# Get the Bigroom place maps
bigroom_maps = plist[0]  # list of neurons

# 8-connectivity
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# List to store distances per neuron
distances_to_edge = []

for neuron_map in bigroom_maps:
    # Convert nonzero to 1
    binary_map = 1*(neuron_map>0)
    
    # Label contiguous patches
    labeled_array, num_features = label(binary_map, structure=structure)
    
    # Compute centroids
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    # Get map dimensions
    h, w = binary_map.shape
    
    # Compute distance to nearest edge for each centroid
    neuron_distances = []
    for c in centroids_list:
        row, col = c
        distance = min(row, h-1-row, col, w-1-col)  # distance to top, bottom, left, right edges
        neuron_distances.append(distance)
    
    distances_to_edge.append(neuron_distances)

# distances_to_edge[i][j] = distance of j-th centroid in i-th neuron to nearest edge

#%% CDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# Get Bigroom place maps
bigroom_maps = plist[0]

# 8-connectivity structure
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# Collect all distances
all_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    for c in centroids_list:
        row, col = c
        distance = min(row, h-1-row, col, w-1-col)
        all_distances.append(distance)

# Convert to numpy array
all_distances = np.array(all_distances)

# Sort distances
sorted_distances = np.sort(all_distances)

# Compute CDF: number of events up to distance d / total number of events
cdf = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)

# Plot
plt.figure(figsize=(6,4))
plt.step(sorted_distances, cdf, where='post')
plt.xlabel('Distance to nearest edge')
plt.ylabel('CDF')
plt.title('CDF of all centroids of placemaps in BigroomData')
plt.show()

#%% Beta CDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy import stats

# ===================== Get Bigroom place maps =====================
bigroom_maps = plist[0]

# 8-connectivity structure
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# ===================== Collect distances =====================
all_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    max_dist = min(h, w) / 2  # maximum distance to normalize
    for c in centroids_list:
        row, col = c
        distance = min(row, h-1-row, col, w-1-col)
        all_distances.append(distance / max_dist)  # normalize to [0,1]

all_distances = np.array(all_distances)

# ===================== Fit Beta distribution =====================
epsilon = 1e-6
all_distances_clipped = np.clip(all_distances, epsilon, 1-epsilon)

a, b, loc, scale = stats.beta.fit(all_distances_clipped, floc=0, fscale=1)

beta_mean = stats.beta.mean(a, b, loc=loc, scale=scale)
beta_var = stats.beta.var(a, b, loc=loc, scale=scale)
beta_cv2 = beta_var / beta_mean**2

# ===================== Compute empirical CDF =====================
sorted_distances = np.sort(all_distances)
empirical_cdf = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)

# Beta CDF
x = np.linspace(0,1,300)
beta_cdf = stats.beta.cdf(x, a, b, loc=loc, scale=scale)

# ===================== Plot =====================
plt.figure(figsize=(6,4))
plt.step(sorted_distances, empirical_cdf, where='post', label='Empirical CDF')
plt.plot(x, beta_cdf, 'r-', lw=2,
         label=f'Beta fit\nmean={beta_mean:.2f}, CV²={beta_cv2:.2f}')

plt.xlabel('Normalized distance to nearest edge')
plt.ylabel('CDF')
plt.title('CDF of normalized centroid distances with Beta fit')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% Beta PDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy import stats

# ===================== Get Bigroom place maps =====================
bigroom_maps = plist[0]

# 8-connectivity structure
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# ===================== Collect distances =====================
all_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    max_dist = min(h, w) / 2  # maximum distance to normalize
    for c in centroids_list:
        row, col = c
        distance = min(row, h-1-row, col, w-1-col)
        all_distances.append(distance / max_dist)  # normalize to [0,1]

all_distances = np.array(all_distances)

# ===================== Fit Beta distribution =====================
epsilon = 1e-6
all_distances_clipped = np.clip(all_distances, epsilon, 1-epsilon)

a, b, loc, scale = stats.beta.fit(all_distances_clipped, floc=0, fscale=1)

beta_mean = stats.beta.mean(a, b, loc=loc, scale=scale)
beta_var = stats.beta.var(a, b, loc=loc, scale=scale)
beta_cv2 = beta_var / beta_mean**2

# ===================== Plot Beta PDF =====================
x = np.linspace(0,1,300)
beta_pdf = stats.beta.pdf(x, a, b, loc=loc, scale=scale)

plt.figure(figsize=(6,4))
plt.hist(all_distances, bins=30, density=True, color='lightgray', edgecolor='black', alpha=0.6, label='Empirical histogram')
plt.plot(x, beta_pdf, 'r-', lw=2,
         label=f'Beta fit\nmean={beta_mean:.2f}, CV²={beta_cv2:.2f}')

plt.xlabel('Normalized distance to nearest edge')
plt.ylabel('Density')
plt.title('PDF of normalized centroid distances with Beta fit')
plt.legend()
plt.grid(alpha=0.3)
plt.show()


#%% Histogram PDF (nearest wall)
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy import stats
from scipy.special import gamma as gamma_func  # continuous gamma function

# ===================== Get Bigroom place maps =====================
bigroom_maps = plist[0]

# 8-connectivity structure
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# ===================== Collect distances =====================
all_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features + 1))
    
    h, w = binary_map.shape
    for c in centroids_list:
        row, col = c
        distance = min(row, h - 1 - row, col, w - 1 - col)
        all_distances.append(distance)

all_distances = np.array(all_distances)

# ===================== Fit Gamma distribution =====================
gamma_params = stats.gamma.fit(all_distances)
gamma_dist = stats.gamma(*gamma_params)
gamma_mean = gamma_dist.mean()
gamma_std = gamma_dist.std()
gamma_cv2 = (gamma_std / gamma_mean)**2
gamma_loglik = np.sum(gamma_dist.logpdf(all_distances))
gamma_aic = 2*len(gamma_params) - 2*gamma_loglik

# ===================== Fit continuous Negative Binomial (Gamma-Poisson) =====================
mean_dist = np.mean(all_distances)
var_dist = np.var(all_distances)
r_cont = mean_dist**2 / (var_dist - mean_dist)
p_cont = r_cont / (r_cont + mean_dist)

# Continuous Negative Binomial PDF
def cont_nbinom_pdf(x, r, p):
    return (gamma_func(x+r) / (gamma_func(x+1) * gamma_func(r))) * (p**r) * ((1-p)**x)

# Mean, CV², approximate log-likelihood & AIC
nb_mean = r_cont*(1-p_cont)/p_cont
nb_std = np.sqrt(r_cont*(1-p_cont)/p_cont**2)
nb_cv2 = (nb_std/nb_mean)**2
nb_loglik = np.sum(np.log(cont_nbinom_pdf(all_distances, r_cont, p_cont)))
nb_aic = 2*2 - 2*nb_loglik  # 2 parameters: r, p

# ===================== Plot histogram and fits =====================
plt.figure(figsize=(6,4))

bin_count = 40  # can change number of bins
counts, bins, _ = plt.hist(all_distances, bins=bin_count, density=False, 
                           alpha=0.6, color='skyblue', edgecolor='black', label='Data')

bin_width = bins[1] - bins[0]
x_vals = np.linspace(min(all_distances), max(all_distances), 300)

# Scale PDFs to match histogram counts
pdf_gamma_scaled = gamma_dist.pdf(x_vals) * len(all_distances) * bin_width
pdf_nb_scaled = cont_nbinom_pdf(x_vals, r_cont, p_cont) * len(all_distances) * bin_width

plt.plot(x_vals, pdf_gamma_scaled, 'r-', lw=2,
         label=f'Gamma fit\nmean={gamma_mean:.2f}, CV²={gamma_cv2:.2f}, AIC={gamma_aic:.1f}')

plt.plot(x_vals, pdf_nb_scaled, 'g-', lw=2,
         label=f'Gamma-Poisson fit\nmean={nb_mean:.2f}, CV²={nb_cv2:.2f}, AIC={nb_aic:.1f}')

plt.xlabel('Distance to nearest edge (pixels)')
plt.ylabel('Counts')
plt.title('Histogram of place field centroid distances (Bigroom)')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% All CDFs together of distances to the nearest wall 

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# ===================== Parameters =====================
bigroom_maps = plist[0]  # list of neuron maps
structure = np.ones((3, 3))  # 8-connectivity

# ===================== Plot setup =====================
plt.figure(figsize=(8,5))

# ===================== Loop over neurons =====================
for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    distances = []
    for row, col in centroids_list:
        # distance to nearest wall
        dist_to_wall = min(row, h-1-row, col, w-1-col)
        distances.append(dist_to_wall)
    
    distances = np.array(distances)
    if len(distances) == 0:
        continue  # skip neurons with no centroids
    
    # empirical CDF
    sorted_dist = np.sort(distances)
    cdf = np.arange(1, len(sorted_dist)+1) / len(sorted_dist)
    
    # plot CDF
    plt.step(sorted_dist, cdf, where='post', alpha=0.6)

# ===================== Final touches =====================
plt.xlabel('Distance to nearest wall (pixels)')
plt.ylabel('CDF')
plt.title('CDF of distances to nearest wall for each neuron')
plt.grid(alpha=0.3)
plt.legend(fontsize=6, ncol=2)  # adjust legend
plt.tight_layout()
plt.show()


#%% All CDF interdistances

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity

plt.figure(figsize=(8,5))

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    distances = []
    for row, col in centroids_list:
        # distance to nearest wall
        dist_to_wall = min(row, h-1-row, col, w-1-col)
        distances.append(dist_to_wall)
    
    distances = np.array(distances)
    if len(distances) < 2:
        continue  # need at least 2 centroids to compute inter-distances
    
    # sort distances
    distances_sorted = np.sort(distances)
    
    # compute inter-distance intervals
    inter_distances = np.diff(distances_sorted)
    
    # empirical CDF of inter-distances
    sorted_inter = np.sort(inter_distances)
    cdf = np.arange(1, len(sorted_inter)+1) / len(sorted_inter)
    
    # plot CDF
    plt.step(sorted_inter, cdf, where='post', alpha=0.6)

plt.xlabel('Inter-distance between consecutive centroids (pixels)')
plt.ylabel('CDF')
plt.title('CDF of inter-centroid distances for each neuron')
plt.grid(alpha=0.3)
plt.legend(fontsize=6, ncol=2)
plt.tight_layout()
plt.show()


#%% CDF of interdistances for each neuron and best fit
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, weibull_min, lognorm, norm
from scipy.optimize import minimize

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 8  # only include neurons with more than this many place fields

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def negloglik_dist(params, data, dist_name):
    """Negative log-likelihood for different distributions."""
    try:
        if dist_name == 'expon':
            scale = params[0]
            pdf_vals = expon.pdf(data, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            pdf_vals = gamma.pdf(data, a=a, scale=scale)
        elif dist_name == 'weibull':
            c, scale = params
            pdf_vals = weibull_min.pdf(data, c=c, scale=scale)
        elif dist_name == 'lognorm':
            s, scale = params
            pdf_vals = lognorm.pdf(data, s=s, scale=scale)
        elif dist_name == 'norm':
            mu, sigma = params
            pdf_vals = norm.pdf(data, loc=mu, scale=sigma)
        else:
            return np.inf
        return -np.sum(np.log(pdf_vals + 1e-12))
    except Exception:
        return np.inf

def fit_distribution(data, dist_name):
    """Fit a single distribution to data and return AIC and params."""
    if dist_name == 'expon':
        x0 = [np.mean(data)]
        bounds = [(1e-6, None)]
        k = 1
    elif dist_name == 'gamma':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'weibull':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'lognorm':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'norm':
        x0 = [np.mean(data), np.std(data)]
        bounds = [(None, None), (1e-6, None)]
        k = 2
    else:
        raise ValueError("Unknown distribution")
    
    res = minimize(negloglik_dist, x0=x0, args=(data, dist_name), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, k)
    return res.x, aic

# ===================== Main loop: plot all neurons =====================
best_counts = {'expon':0}
all_dist_aics = []

plt.figure(figsize=(8,5))

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue  # skip neurons with too few place fields
    
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    distances = []
    for row, col in centroids_list:
        dist_to_wall = min(row, h-1-row, col, w-1-col)
        distances.append(dist_to_wall)
    
    distances = np.array(distances)
    if len(distances) < 2:
        continue
    
    # sorted distances and inter-distance intervals
    distances_sorted = np.sort(distances)
    inter_distances = np.diff(distances_sorted)
    
    # fit all distributions
    dist_names = ['expon','gamma','weibull','lognorm','norm']
    aics = {}
    for dist_name in dist_names:
        _, aic = fit_distribution(inter_distances, dist_name)
        aics[dist_name] = aic
    
    # count if exponential is best
    best_dist = min(aics, key=aics.get)
    if best_dist == 'expon':
        best_counts['expon'] += 1
    
    all_dist_aics.append(aics)
    
    # plot empirical CDF
    sorted_inter = np.sort(inter_distances)
    cdf = np.arange(1, len(sorted_inter)+1)/len(sorted_inter)
    plt.step(sorted_inter, cdf, where='post', alpha=0.5)

plt.xlabel('Inter-distance between consecutive centroids (pixels)')
plt.ylabel('CDF')
plt.title(f'CDF of inter-centroid distances (neurons with >{min_fields} place fields)')
plt.grid(alpha=0.3)
plt.show()

# ===================== Summary =====================
num_neurons = len(all_dist_aics)
print(f"Number of neurons considered: {num_neurons}")
print(f"Number of neurons where Exponential is best fit: {best_counts['expon']}")
print(f"Fraction best fit by Exponential: {best_counts['expon']/num_neurons:.2f}")

# ===================== New figure: plot best exponential fit =====================
best_aic = np.inf
best_inter_distances = None
best_scale = None

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue

    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    h, w = binary_map.shape
    distances = [min(row, h-1-row, col, w-1-col) for row, col in centroids_list]
    distances = np.array(distances)
    if len(distances) < 2:
        continue

    inter_distances = np.diff(np.sort(distances))

    # Fit exponential
    scale, aic_expon = fit_distribution(inter_distances, 'expon')

    # Keep neuron with lowest exponential AIC
    if aic_expon < best_aic:
        best_aic = aic_expon
        best_inter_distances = inter_distances
        best_scale = scale[0]  # scale parameter of the exponential

# Plot empirical CDF and best-fit exponential CDF
if best_inter_distances is not None:
    sorted_inter = np.sort(best_inter_distances)
    cdf_emp = np.arange(1, len(sorted_inter)+1)/len(sorted_inter)

    x_vals = np.linspace(0, sorted_inter.max(), 200)
    cdf_expon = 1 - np.exp(-x_vals / best_scale)

    plt.figure(figsize=(7,4))
    plt.step(sorted_inter, cdf_emp, where='post', label='Empirical CDF')
    plt.plot(x_vals, cdf_expon, 'r-', lw=2, label=f'Exponential fit\nscale={best_scale:.2f}, AIC={best_aic:.1f}')
    plt.xlabel('Inter-distance between consecutive centroids (pixels)')
    plt.ylabel('CDF')
    plt.title('Neuron with Best Exponential Fit')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()
    
#%% Distance to the first centroid from the wall for each neuron CDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, lomax
from scipy.optimize import minimize

# ===================== Parameters =====================
dind=0
bigroom_maps = plist[dind]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 0  # minimum place fields to include neuron

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def negloglik_dist(params, data, dist_name):
    try:
        if dist_name == 'expon':
            scale = params[0]
            if scale <= 0: return np.inf
            pdf_vals = expon.pdf(data, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            if a <= 0 or scale <= 0: return np.inf
            pdf_vals = gamma.pdf(data, a=a, scale=scale)
        elif dist_name == 'lomax':
            c, scale = params
            if c <= 0 or scale <= 0: return np.inf
            pdf_vals = lomax.pdf(data, c=c, scale=scale)
        else:
            return np.inf
        return -np.sum(np.log(pdf_vals + 1e-12))
    except:
        return np.inf

def fit_distribution(data, dist_name):
    if dist_name == 'expon':
        x0 = [np.mean(data)]
        bounds = [(1e-6, None)]
        k = 1
    elif dist_name in ['gamma', 'lomax']:
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    else:
        raise ValueError("Unknown distribution")
    
    res = minimize(negloglik_dist, x0=x0, args=(data, dist_name), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, k)
    return res.x, aic

def mean_cv2(dist_name, params):
    if dist_name == 'expon':
        mean = params[0]
        cv2 = 1.0
    elif dist_name == 'gamma':
        a, scale = params
        mean = a * scale
        cv2 = 1 / a
    elif dist_name == 'lomax':
        c, scale = params
        if c <= 1:
            mean = np.inf
        else:
            mean = scale / (c - 1)
        if c <= 2:
            cv2 = np.inf
        else:
            cv2 = 1 / (c - 2)
    return mean, cv2

# ===================== Collect minimum distances =====================
min_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1*(neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    h, w = binary_map.shape
    distances = [min(row, h-1-row, col, w-1-col) for row, col in centroids_list]
    if len(distances) < 1:
        continue
    min_distances.append(np.min(distances))

min_distances = np.array(min_distances)
if len(min_distances) == 0:
    raise ValueError("No neurons passed the min_fields criterion")

# ===================== Empirical CDF =====================
sorted_data = np.sort(min_distances)
cdf_emp = np.arange(1, len(sorted_data)+1) / len(sorted_data)

# ===================== Fit distributions =====================
dist_names = [
    # 'expon', 
    # 'gamma', 
    'lomax']
fit_results = {}
x_vals = np.linspace(0, sorted_data.max()*1.1, 500)

for dist_name in dist_names:
    params, aic = fit_distribution(min_distances, dist_name)
    mean, cv2 = mean_cv2(dist_name, params)
    fit_results[dist_name] = {'params': params, 'aic': aic, 'mean': mean, 'cv2': cv2}

# ===================== Plot =====================
plt.figure(figsize=(8,5))
plt.step(sorted_data, cdf_emp, where='post', color='black', label='Empirical CDF')

for dist_name, info in fit_results.items():
    params = info['params']
    aic = info['aic']
    mean = info['mean']
    cv2 = info['cv2']
    if dist_name == 'expon':
        cdf_fit = expon.cdf(x_vals, scale=params[0])
    elif dist_name == 'gamma':
        cdf_fit = gamma.cdf(x_vals, a=params[0], scale=params[1])
    elif dist_name == 'lomax':
        cdf_fit = lomax.cdf(x_vals, c=params[0], scale=params[1])
    plt.plot(x_vals, cdf_fit, lw=2, label=f'{dist_name}\nmean={mean:.2f}, CV²={cv2:.4f}')

plt.xlabel('Distance between the wall and the closest centroid')
plt.ylabel('Cumulative distribution function')
plt.title('{}'.format(dnames[dind]), fontsize=20)
plt.legend(fontsize=8)
plt.grid(alpha=0.3)
plt.show()


#%% Changed the distance metric to "from the center"

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, lomax
from scipy.optimize import minimize

# ===================== Parameters =====================
dind = 0
bigroom_maps = plist[dind]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 2  # minimum place fields to include neuron

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def negloglik_dist(params, data, dist_name):
    try:
        if dist_name == 'expon':
            scale = params[0]
            if scale <= 0: return np.inf
            pdf_vals = expon.pdf(data, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            if a <= 0 or scale <= 0: return np.inf
            pdf_vals = gamma.pdf(data, a=a, scale=scale)
        elif dist_name == 'lomax':
            c, scale = params
            if c <= 0 or scale <= 0: return np.inf
            pdf_vals = lomax.pdf(data, c=c, scale=scale)
        else:
            return np.inf
        return -np.sum(np.log(pdf_vals + 1e-12))
    except:
        return np.inf

def fit_distribution(data, dist_name):
    if dist_name == 'expon':
        x0 = [np.mean(data)]
        bounds = [(1e-6, None)]
        k = 1
    elif dist_name in ['gamma', 'lomax']:
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    else:
        raise ValueError("Unknown distribution")
    
    res = minimize(negloglik_dist, x0=x0, args=(data, dist_name), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, k)
    return res.x, aic

def mean_cv2(dist_name, params):
    if dist_name == 'expon':
        mean = params[0]
        cv2 = 1.0
    elif dist_name == 'gamma':
        a, scale = params
        mean = a * scale
        cv2 = 1 / a
    elif dist_name == 'lomax':
        c, scale = params
        if c <= 1:
            mean = np.inf
        else:
            mean = scale / (c - 1)
        if c <= 2:
            cv2 = np.inf
        else:
            cv2 = 1 / (c - 2)
    return mean, cv2

# ===================== Collect distances to center =====================
center_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1*(neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features < min_fields:
        continue

    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    h, w = binary_map.shape
    center = np.array([h/2, w/2])

    # Distance from each centroid to the room center
    distances_to_center = [np.linalg.norm(np.array([r, c]) - center) for r, c in centroids_list]
    if len(distances_to_center) < 1:
        continue

    # Get the closest centroid to the room center
    closest_distance = np.min(distances_to_center)
    center_distances.append(closest_distance)

center_distances = np.array(center_distances)
if len(center_distances) == 0:
    raise ValueError("No neurons passed the min_fields criterion")

# ===================== Empirical CDF =====================
sorted_data = np.sort(center_distances)
cdf_emp = np.arange(1, len(sorted_data)+1) / len(sorted_data)

# ===================== Fit distributions =====================
dist_names = ['lomax']
fit_results = {}
x_vals = np.linspace(0, sorted_data.max()*1.1, 500)

for dist_name in dist_names:
    params, aic = fit_distribution(center_distances, dist_name)
    mean, cv2 = mean_cv2(dist_name, params)
    fit_results[dist_name] = {'params': params, 'aic': aic, 'mean': mean, 'cv2': cv2}

# ===================== Plot =====================
plt.figure(figsize=(8,5))
plt.step(sorted_data, cdf_emp, where='post', color='black', label='Empirical CDF')

for dist_name, info in fit_results.items():
    params = info['params']
    aic = info['aic']
    mean = info['mean']
    cv2 = info['cv2']
    if dist_name == 'expon':
        cdf_fit = expon.cdf(x_vals, scale=params[0])
    elif dist_name == 'gamma':
        cdf_fit = gamma.cdf(x_vals, a=params[0], scale=params[1])
    elif dist_name == 'lomax':
        cdf_fit = lomax.cdf(x_vals, c=params[0], scale=params[1])
    plt.plot(x_vals, cdf_fit, lw=2, label=f'{dist_name}\nmean={mean:.2f}, CV²={cv2:.4f}')

plt.xlabel('Distance of the centroid closest to the room center')
plt.ylabel('Cumulative distribution function')
plt.title('{}'.format(dnames[dind]), fontsize=20)
plt.legend(fontsize=8)
plt.grid(alpha=0.3)
plt.show()



#%%  Plot all the centroids together for inspection

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# ===================== Parameters =====================
bigroom_maps = plist[3]       # list of neuron maps
structure = np.ones((3, 3))   # 8-connectivity
min_fields = 0                # minimum place fields to include neuron

# ===================== Helper function =====================
def closest_to_wall(centroids, h, w):
    """
    Returns the centroid closest to any wall and its distance.
    """
    distances = [min(row, h-1-row, col, w-1-col) for row, col in centroids]
    min_idx = np.argmin(distances)
    return centroids[min_idx], distances[min_idx]

# ===================== Loop over neurons =====================
for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1*(neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    
    if num_features <= min_fields:
        print(f"Neuron {neuron_idx}: too few place fields ({num_features})")
        continue
    
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    h, w = binary_map.shape
    
    # Find the centroid closest to a wall
    centroid, dist = closest_to_wall(centroids_list, h, w)
    
    # Plot neuron map
    plt.figure(figsize=(4,4))
    plt.imshow(neuron_map, cmap='viridis')
    
    # Plot the closest centroid
    plt.plot(centroid[1], centroid[0], 'ro', markersize=8)
    plt.text(centroid[1]+1, centroid[0]+1, f"{dist:.1f}", color='white', fontsize=10)
    
    plt.title(f"Neuron {neuron_idx} - Closest centroid distance: {dist:.1f}")
    plt.axis('off')
    plt.show()


    
#%% Plot all together

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon
from scipy.optimize import minimize
import math

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 1  # only include neurons with more than this many place fields

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def negloglik_expon(params, data):
    scale = params[0]
    if scale <= 0:
        return np.inf
    pdf_vals = expon.pdf(data, scale=scale)
    return -np.sum(np.log(pdf_vals + 1e-12))

def fit_exponential(data):
    x0 = [np.mean(data)]
    bounds = [(1e-6, None)]
    res = minimize(negloglik_expon, x0=x0, args=(data,), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, 1)
    return res.x[0], aic

# ===================== Collect neuron fits =====================
results = []

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue
    
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features + 1))
    h, w = binary_map.shape
    distances = [min(row, h - 1 - row, col, w - 1 - col) for row, col in centroids_list]
    distances = np.array(distances)
    if len(distances) < 2:
        continue

    inter_distances = np.diff(np.sort(distances))
    if np.any(inter_distances <= 0):
        continue

    scale, aic = fit_exponential(inter_distances)

    results.append({
        "neuron_idx": neuron_idx,
        "inter_distances": inter_distances,
        "scale": scale,
        "aic": aic
    })

n_results = len(results)
print(f"Plotted {n_results} neurons with >{min_fields} place fields")

# ===================== Choose subplot grid automatically =====================
if n_results == 0:
    raise ValueError("No neurons met the min_fields criterion.")

ncols = math.ceil(np.sqrt(n_results))
nrows = math.ceil(n_results / ncols)

# ===================== Plot grid of CDFs =====================
fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))
axes = np.array(axes).flatten()

for ax_idx, (ax, result) in enumerate(zip(axes, results)):
    data = result["inter_distances"]
    scale = result["scale"]

    sorted_data = np.sort(data)
    cdf_emp = np.arange(1, len(sorted_data) + 1) / len(sorted_data)

    x_vals = np.linspace(0, sorted_data.max(), 200)
    cdf_exp = 1 - np.exp(-x_vals / scale)

    ax.step(sorted_data, cdf_emp, where="post", color="blue", alpha=0.7)
    ax.plot(x_vals, cdf_exp, "r-", lw=1.2)
    ax.set_title(f"Neuron {result['neuron_idx']}\nscale={scale:.2f}", fontsize=6)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.grid(alpha=0.2)

# Hide unused subplots
for ax in axes[n_results:]:
    ax.axis("off")
    
fig.suptitle('{} neurons with >{} placefields'.format(n_results, min_fields),
             fontsize=40)

plt.tight_layout()
plt.show()

#%% Minimum distances CDF
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, weibull_min, lognorm, norm, lomax
from scipy.optimize import minimize

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 6  # only include neurons with more than this many place fields

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2 * k - 2 * loglik

def negloglik_dist(params, data, dist_name):
    """Negative log-likelihood for different distributions."""
    try:
        if dist_name == 'expon':
            scale = params[0]
            pdf_vals = expon.pdf(data, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            pdf_vals = gamma.pdf(data, a=a, scale=scale)
        elif dist_name == 'weibull':
            c, scale = params
            pdf_vals = weibull_min.pdf(data, c=c, scale=scale)
        elif dist_name == 'lognorm':
            s, scale = params
            pdf_vals = lognorm.pdf(data, s=s, scale=scale)
        elif dist_name == 'norm':
            mu, sigma = params
            pdf_vals = norm.pdf(data, loc=mu, scale=sigma)
        elif dist_name == 'lomax':
            c, scale = params
            pdf_vals = lomax.pdf(data, c=c, scale=scale)
        else:
            return np.inf
        return -np.sum(np.log(pdf_vals + 1e-12))
    except Exception:
        return np.inf

def fit_distribution(data, dist_name):
    """Fit a single distribution to data and return AIC and parameters."""
    if dist_name == 'expon':
        x0 = [np.mean(data)]
        bounds = [(1e-6, None)]
        k = 1
    elif dist_name == 'gamma':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'weibull':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'lognorm':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'norm':
        x0 = [np.mean(data), np.std(data)]
        bounds = [(None, None), (1e-6, None)]
        k = 2
    elif dist_name == 'lomax':
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    else:
        raise ValueError("Unknown distribution")

    res = minimize(negloglik_dist, x0=x0, args=(data, dist_name), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, k)
    return res.x, aic

# ===================== Main analysis =====================
best_aic = np.inf
best_data = None
best_fits = None

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)

    if num_features <= min_fields:
        continue

    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features + 1))
    h, w = binary_map.shape
    distances = np.array([min(row, h - 1 - row, col, w - 1 - col) for row, col in centroids_list])

    if len(distances) == 0:
        continue

    # Take the smallest distance per neuron
    min_dist = np.min(distances)

    # For fair comparison, wrap in an array
    data = np.array([min_dist])

    # Skip if trivial
    if np.std(data) == 0:
        continue

    # Fit all distributions
    dist_names = ['expon', 'gamma', 'weibull', 'lognorm', 'norm', 'lomax']
    fits = {}
    for dist_name in dist_names:
        params, aic = fit_distribution(data, dist_name)
        fits[dist_name] = (params, aic)

    # Pick neuron with lowest exponential AIC (just as a consistent selection rule)
    if fits['expon'][1] < best_aic:
        best_aic = fits['expon'][1]
        best_data = data
        best_fits = fits

# ===================== Plot for the neuron with best exponential fit =====================
if best_data is not None:
    sorted_data = np.sort(best_data)
    cdf_emp = np.arange(1, len(sorted_data) + 1) / len(sorted_data)

    x_vals = np.linspace(0, sorted_data.max() * 1.2 + 1e-6, 200)

    plt.figure(figsize=(8, 5))
    plt.step(sorted_data, cdf_emp, where='post', lw=2, color='black', label='Empirical CDF')

    for dist_name, (params, aic) in best_fits.items():
        if dist_name == 'expon':
            scale = params[0]
            cdf_fit = expon.cdf(x_vals, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            cdf_fit = gamma.cdf(x_vals, a=a, scale=scale)
        elif dist_name == 'weibull':
            c, scale = params
            cdf_fit = weibull_min.cdf(x_vals, c=c, scale=scale)
        elif dist_name == 'lognorm':
            s, scale = params
            cdf_fit = lognorm.cdf(x_vals, s=s, scale=scale)
        elif dist_name == 'norm':
            mu, sigma = params
            cdf_fit = norm.cdf(x_vals, loc=mu, scale=sigma)
        elif dist_name == 'lomax':
            c, scale = params
            cdf_fit = lomax.cdf(x_vals, c=c, scale=scale)
        else:
            continue

        plt.plot(x_vals, cdf_fit, lw=1.8, label=f"{dist_name} (AIC={aic:.1f})")

    plt.xlabel("Smallest centroid-wall distance (pixels)")
    plt.ylabel("CDF")
    plt.title("CDF of Smallest Distance (Best Neuron, Lomax Added)")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()


#%% Gamma exponential CDF (lomax)

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, weibull_min, lognorm, norm, lomax
from scipy.optimize import minimize

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 15  # only include neurons with more than this many place fields

# ===================== Helper functions =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def negloglik_dist(params, data, dist_name):
    """Negative log-likelihood for different distributions."""
    try:
        if dist_name == 'expon':
            scale = params[0]
            pdf_vals = expon.pdf(data, scale=scale)
        elif dist_name == 'gamma':
            a, scale = params
            pdf_vals = gamma.pdf(data, a=a, scale=scale)
        elif dist_name == 'weibull':
            c, scale = params
            pdf_vals = weibull_min.pdf(data, c=c, scale=scale)
        elif dist_name == 'lognorm':
            s, scale = params
            pdf_vals = lognorm.pdf(data, s=s, scale=scale)
        elif dist_name == 'norm':
            mu, sigma = params
            pdf_vals = norm.pdf(data, loc=mu, scale=sigma)
        elif dist_name == 'lomax':
            c, scale = params
            pdf_vals = lomax.pdf(data, c=c, scale=scale)
        else:
            return np.inf
        return -np.sum(np.log(pdf_vals + 1e-12))
    except Exception:
        return np.inf

def fit_distribution(data, dist_name):
    """Fit a single distribution to data and return AIC and params."""
    if dist_name == 'expon':
        x0 = [np.mean(data)]
        bounds = [(1e-6, None)]
        k = 1
    elif dist_name in ['gamma', 'weibull', 'lognorm', 'lomax']:
        x0 = [1, np.mean(data)]
        bounds = [(1e-6, None), (1e-6, None)]
        k = 2
    elif dist_name == 'norm':
        x0 = [np.mean(data), np.std(data)]
        bounds = [(None, None), (1e-6, None)]
        k = 2
    else:
        raise ValueError("Unknown distribution")
    
    res = minimize(negloglik_dist, x0=x0, args=(data, dist_name), bounds=bounds)
    loglik = -res.fun
    aic = compute_aic(loglik, len(x0))
    return res.x, aic

# ===================== Main loop: plot all neurons =====================
best_counts = {'lomax': 0}
all_dist_aics = []

plt.figure(figsize=(8,5))

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue  # skip neurons with too few place fields
    
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    distances = []
    for row, col in centroids_list:
        dist_to_wall = min(row, h-1-row, col, w-1-col)
        distances.append(dist_to_wall)
    
    distances = np.array(distances)
    if len(distances) < 2:
        continue
    
    # sorted distances and inter-distance intervals
    distances_sorted = np.sort(distances)
    inter_distances = np.diff(distances_sorted)
    
    # fit distributions
    dist_names = ['expon','gamma','weibull','lognorm','norm','lomax']
    aics = {}
    for dist_name in dist_names:
        _, aic = fit_distribution(inter_distances, dist_name)
        aics[dist_name] = aic
    
    # count if Lomax is best
    best_dist = min(aics, key=aics.get)
    if best_dist == 'lomax':
        best_counts['lomax'] += 1
    
    all_dist_aics.append(aics)
    
    # plot empirical CDF
    sorted_inter = np.sort(inter_distances)
    cdf = np.arange(1, len(sorted_inter)+1)/len(sorted_inter)
    plt.step(sorted_inter, cdf, where='post', alpha=0.5)

plt.xlabel('Inter-distance between consecutive centroids (pixels)')
plt.ylabel('CDF')
plt.title(f'CDF of inter-centroid distances (neurons with >{min_fields} place fields)')
plt.grid(alpha=0.3)
plt.show()

# ===================== Summary =====================
num_neurons = len(all_dist_aics)
print(f"Number of neurons considered: {num_neurons}")
print(f"Number of neurons where Lomax is best fit: {best_counts['lomax']}")
print(f"Fraction best fit by Lomax: {best_counts['lomax']/num_neurons:.2f}")

# ===================== Plot best Lomax fit =====================
best_aic = np.inf
best_inter_distances = None
best_params = None

for neuron_idx, neuron_map in enumerate(bigroom_maps):
    binary_map = 1*(neuron_map>0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        continue

    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    h, w = binary_map.shape
    distances = [min(row, h-1-row, col, w-1-col) for row, col in centroids_list]
    distances = np.array(distances)
    if len(distances) < 2:
        continue

    inter_distances = np.diff(np.sort(distances))

    # Fit Lomax
    params, aic_lomax = fit_distribution(inter_distances, 'lomax')

    # Keep neuron with lowest Lomax AIC
    if aic_lomax < best_aic:
        best_aic = aic_lomax
        best_inter_distances = inter_distances
        best_params = params  # shape and scale

# Plot empirical CDF and best-fit Lomax CDF
if best_inter_distances is not None:
    sorted_inter = np.sort(best_inter_distances)
    cdf_emp = np.arange(1, len(sorted_inter)+1)/len(sorted_inter)

    x_vals = np.linspace(0, sorted_inter.max(), 200)
    c, scale = best_params
    cdf_lomax = lomax.cdf(x_vals, c=c, scale=scale)

    plt.figure(figsize=(7,4))
    plt.step(sorted_inter, cdf_emp, where='post', label='Empirical CDF')
    plt.plot(x_vals, cdf_lomax, 'r-', lw=2, label=f'Lomax fit\nshape={c:.2f}, scale={scale:.2f}, AIC={best_aic:.1f}')
    plt.xlabel('Inter-distance between consecutive centroids (pixels)')
    plt.ylabel('CDF')
    plt.title('Neuron with Best Lomax (Gamma–Exponential) Fit')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()


#%% CDF of the distance to the first centroid from the nearest wall (neuron)

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy.stats import expon, gamma, weibull_min, lognorm, norm
from scipy.special import gamma as gamma_func

# ===================== Parameters =====================
bigroom_maps = plist[0]
structure = np.ones((3, 3))  # 8-connectivity
min_fields = 1  # only include neurons with more than this many place fields

# ===================== Helper functions =====================
def get_smallest_distances(neuron_map):
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    if num_features <= min_fields:
        return None
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
    
    h, w = binary_map.shape
    distances = []
    for row, col in centroids_list:
        dist_to_wall = min(row, h-1-row, col, w-1-col)
        distances.append(dist_to_wall)
    return np.min(distances) if distances else None

def fit_distribution_stats(data, dist_name):
    """Fit distribution and return mean, CV², and CDF values for plotting."""
    if dist_name == 'expon':
        scale = np.mean(data)
        mean = scale
        var = scale**2
        cdf_vals = expon.cdf(data, scale=scale)
    elif dist_name == 'gamma':
        mean_data = np.mean(data)
        var_data = np.var(data)
        theta = var_data / mean_data
        k = mean_data / theta
        mean = k*theta
        var = k*theta**2
        cdf_vals = gamma.cdf(data, a=k, scale=theta)
    elif dist_name == 'weibull':
        # rough method-of-moments estimate
        mean_data = np.mean(data)
        var_data = np.var(data)
        c = 1.2
        scale = mean_data / gamma_func(1 + 1/c)
        mean = scale * gamma_func(1 + 1/c)
        var = scale**2 * (gamma_func(1 + 2/c) - (gamma_func(1 + 1/c))**2)
        cdf_vals = weibull_min.cdf(data, c=c, scale=scale)
    elif dist_name == 'norm':
        mu = np.mean(data)
        sigma = np.std(data)
        mean = mu
        var = sigma**2
        cdf_vals = norm.cdf(data, loc=mu, scale=sigma)
    elif dist_name == 'lognorm':
        mean_data = np.mean(data)
        var_data = np.var(data)
        sigma = np.sqrt(np.log(1 + var_data/mean_data**2))
        scale = mean_data / np.exp(sigma**2/2)
        mean = np.exp(sigma**2/2) * scale
        var = (np.exp(sigma**2) - 1) * (scale**2) * np.exp(sigma**2)
        cdf_vals = lognorm.cdf(data, s=sigma, scale=scale)
    else:
        raise ValueError("Unknown distribution")
    
    cv2 = var / mean**2
    return mean, cv2, cdf_vals

# ===================== Collect smallest distances =====================
smallest_distances = []
for neuron_map in bigroom_maps:
    sd = get_smallest_distances(neuron_map)
    if sd is not None:
        smallest_distances.append(sd)

smallest_distances = np.array(smallest_distances)
sorted_data = np.sort(smallest_distances)
cdf_empirical = np.arange(1, len(sorted_data)+1)/len(sorted_data)

# ===================== Fit distributions =====================
dist_names = ['expon','gamma','weibull','norm','lognorm']
fit_results = {}

for dist in dist_names:
    mean, cv2, cdf_vals = fit_distribution_stats(sorted_data, dist)
    fit_results[dist] = {'mean': mean, 'cv2': cv2, 'cdf': cdf_vals}

# ===================== Determine best distribution (lowest CV² as proxy) =====================
best_dist = min(fit_results, key=lambda d: abs(fit_results[d]['cv2'] - 1))  # example metric
best_mean = fit_results[best_dist]['mean']
best_cv2 = fit_results[best_dist]['cv2']
best_cdf = fit_results[best_dist]['cdf']

# ===================== Plot =====================
plt.figure(figsize=(7,5))
plt.step(sorted_data, cdf_empirical, where='post', label='Empirical CDF')
plt.plot(sorted_data, best_cdf, 'r-', lw=2, 
         label=f'{best_dist} fit\nmean={best_mean:.2f}, CV²={best_cv2:.2f}')
plt.xlabel('Smallest distance of each centroid (pixels)')
plt.ylabel('CDF')
plt.title('CDF of smallest distances per neuron with best fit')
plt.legend()
plt.grid(alpha=0.3)
plt.show()



#%% CDF histogram

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass
from scipy import stats
from scipy.special import gamma as gamma_func

# ===================== Get Bigroom place maps =====================
bigroom_maps = plist[0]

# 8-connectivity
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# Collect all distances
all_distances = []

for neuron_map in bigroom_maps:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features + 1))
    
    h, w = binary_map.shape
    for c in centroids_list:
        row, col = c
        distance = min(row, h - 1 - row, col, w - 1 - col)
        all_distances.append(distance)

all_distances = np.array(all_distances)

# ===================== Compute empirical CDF =====================
sorted_distances = np.sort(all_distances)
empirical_cdf = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)

# ===================== Fit Gamma =====================
gamma_params = stats.gamma.fit(all_distances)
gamma_dist = stats.gamma(*gamma_params)
gamma_cdf = gamma_dist.cdf(sorted_distances)

# ===================== Fit continuous Negative Binomial (Gamma-Poisson) =====================
mean_dist = np.mean(all_distances)
var_dist = np.var(all_distances)
r_cont = mean_dist**2 / (var_dist - mean_dist)
p_cont = r_cont / (r_cont + mean_dist)

def cont_nbinom_cdf(x, r, p):
    # Approximate continuous CDF by summing PDF
    x_vals = np.linspace(0, x.max(), 5000)
    pdf_vals = (gamma_func(x_vals + r) / (gamma_func(x_vals + 1) * gamma_func(r))) * (p**r) * ((1-p)**x_vals)
    cdf_vals = np.cumsum(pdf_vals)
    cdf_vals /= cdf_vals[-1]  # normalize to 1
    # interpolate at given x
    return np.interp(x, x_vals, cdf_vals)

nb_cdf = cont_nbinom_cdf(sorted_distances, r_cont, p_cont)

# ===================== Plot CDFs =====================
plt.figure(figsize=(6,4))
plt.step(sorted_distances, empirical_cdf, where='post', label='Empirical CDF')
plt.plot(sorted_distances, gamma_cdf, 'r-', lw=2,
         label=f'Gamma fit\nmean={gamma_dist.mean():.2f}, CV²={(gamma_dist.std()/gamma_dist.mean())**2:.2f}')
plt.plot(sorted_distances, nb_cdf, 'g-', lw=2,
         label=f'Gamma-Poisson fit\nmean={r_cont*(1-p_cont)/p_cont:.2f}, CV²={(np.sqrt(r_cont*(1-p_cont)/p_cont**2)/(r_cont*(1-p_cont)/p_cont))**2:.2f}')

plt.xlabel('Distance to nearest edge (pixels)')
plt.ylabel('CDF')
plt.title('CDF of centroid distances (Bigroom)')
plt.legend()
plt.grid(alpha=0.3)
plt.show()



#%%  plot a random point and its distance

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# Select the neuron map
neuron_map = plist[0][0]

# Convert nonzero to 1
binary_map = 1*(neuron_map>0)

# Label contiguous patches (8-connectivity)
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])
labeled_array, num_features = label(binary_map, structure=structure)

# Compute centroids of each patch
centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))

# Plot neuron map
plt.imshow(binary_map, cmap='gray')
plt.title('Distance of each centroid to the nearest wall')

# Get room dimensions
h, w = binary_map.shape

# Overlay all centroids and annotate distances
for c in centroids_list:
    row, col = c
    # Distance to nearest wall
    distance_to_wall = min(row, h-1-row, col, w-1-col)
    plt.plot(col, row, 'ro')  # centroid
    plt.text(col + 1, row + 1, f'{distance_to_wall:.1f}', color='red', fontsize=10)

plt.show()


#%% Fit distributions

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.ndimage import label, center_of_mass

# Assuming 'all_distances' is already computed from your code

# List of distributions to try
distributions = [stats.norm, stats.expon, stats.gamma, stats.beta, stats.lognorm]

# Sort distances for plotting empirical CDF
sorted_distances = np.sort(all_distances)
empirical_cdf = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)

plt.figure(figsize=(6,4))
plt.step(sorted_distances, empirical_cdf, where='post', label='Empirical CDF')

best_sse = np.inf
best_dist_name = None
best_params = None

for dist in distributions:
    try:
        # Fit distribution to data
        params = dist.fit(all_distances)
        
        # Get CDF values
        fitted_cdf = dist.cdf(sorted_distances, *params)
        
        # Compute sum of squared errors to empirical CDF
        sse = np.sum((fitted_cdf - empirical_cdf)**2)
        
        if sse < best_sse:
            best_sse = sse
            best_dist_name = dist.name
            best_params = params
        
        # Plot fitted CDF
        plt.plot(sorted_distances, fitted_cdf, label=f'{dist.name}')
        
    except Exception as e:
        print(f"Skipping {dist.name}: {e}")

plt.xlabel('Distance to nearest edge')
plt.ylabel('CDF')
plt.title('Empirical CDF with fitted distributions')
plt.legend()
plt.show()

print(f"Best fit: {best_dist_name} with parameters {best_params}")

#%% plot best CDF
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.ndimage import label, center_of_mass

# Assuming 'all_distances' is already computed
sorted_distances = np.sort(all_distances)
empirical_cdf = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)

# Candidate distributions
distributions = [stats.norm, stats.expon, stats.gamma, stats.beta, stats.lognorm]

best_sse = np.inf
best_dist = None
best_params = None
best_cdf = None
best_mean = None
best_cv2 = None

for dist in distributions:
    try:
        params = dist.fit(all_distances)
        fitted_cdf = dist.cdf(sorted_distances, *params)
        sse = np.sum((fitted_cdf - empirical_cdf)**2)
        if sse < best_sse:
            best_sse = sse
            best_dist = dist
            best_params = params
            best_cdf = fitted_cdf
            
            # Compute mean and CV^2 for the fitted distribution
            if dist == stats.norm:
                mu, sigma = params
                best_mean = mu
                best_cv2 = (sigma/mu)**2 if mu != 0 else np.nan
            elif dist == stats.expon:
                loc, scale = params
                best_mean = loc + scale
                best_cv2 = (scale/best_mean)**2 if best_mean != 0 else np.nan
            elif dist == stats.gamma:
                a, loc, scale = params
                best_mean = a*scale + loc
                best_cv2 = (np.sqrt(a)*scale/best_mean)**2 if best_mean != 0 else np.nan
            elif dist == stats.lognorm:
                s, loc, scale = params
                best_mean = np.exp(np.log(scale) + s**2 / 2)
                best_cv2 = (np.exp(s**2) - 1)
            elif dist == stats.beta:
                a, b, loc, scale = params
                best_mean = a / (a+b) * scale + loc
                best_cv2 = (a*b)/((a+b)**2*(a+b+1))
    except:
        continue

# Plot
plt.figure(figsize=(6,4))
plt.step(sorted_distances, empirical_cdf, where='post', label='Empirical CDF')
plt.plot(sorted_distances, best_cdf, color='red',
         label=f'{best_dist.name} (mean={best_mean:.2f}, CV²={best_cv2:.2f})')
plt.xlabel('Distance to nearest edge')
plt.ylabel('CDF')
plt.title('CDF of ordered centroid distances with best fit')
plt.legend()
plt.show()

# =============================================================================
# #%% Inter centroid distance not good
# 
# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.ndimage import label, center_of_mass
# 
# # Get Bigroom place maps
# bigroom_maps = plist[0]
# 
# # 8-connectivity structure
# structure = np.array([[1,1,1],
#                       [1,1,1],
#                       [1,1,1]])
# 
# # Collect all distances
# all_distances = []
# 
# for neuron_map in bigroom_maps:
#     binary_map = 1*(neuron_map>0)
#     labeled_array, num_features = label(binary_map, structure=structure)
#     centroids_list = center_of_mass(binary_map, labeled_array, range(1, num_features+1))
#     
#     h, w = binary_map.shape
#     for c in centroids_list:
#         row, col = c
#         distance = min(row, h-1-row, col, w-1-col)
#         all_distances.append(distance)
# 
# # Convert to numpy array
# all_distances = np.array(all_distances)
# 
# # Sort distances
# sorted_distances = np.sort(all_distances)
# 
# # Compute inter-centroid distances (differences between consecutive sorted distances)
# inter_distances = np.diff(sorted_distances)
# 
# # Compute CDF of inter-centroid distances
# sorted_inter = np.sort(inter_distances)
# cdf_inter = np.arange(1, len(sorted_inter)+1) / len(sorted_inter)
# 
# # Plot
# plt.figure(figsize=(6,4))
# plt.step(sorted_inter, cdf_inter, where='post')
# plt.xlabel('Distance between consecutive centroids')
# plt.ylabel('CDF')
# plt.title('CDF of inter-centroid distances in BigroomData')
# plt.show()
# =============================================================================

#%% Plot the mean line


import numpy as np
import matplotlib.pyplot as plt

# Use the mean from your CDF fit (best_mean)
mean_distance = best_mean  # from your previous CDF-fitting code

# Get the neuron map
neuron_map = plist[0][0]
binary_map = 1 * (neuron_map > 0)
h, w = binary_map.shape

# Create coordinate grid
yy, xx = np.indices((h, w))

# Compute distance to the nearest wall for each point
dist_to_wall = np.minimum.reduce([yy, h - 1 - yy, xx, w - 1 - xx])

# Find points approximately mean_distance away (±1 pixel tolerance)
mask = np.abs(dist_to_wall - mean_distance) < 1.0

# Plot
plt.figure(figsize=(6, 6))
plt.imshow(binary_map, cmap='gray')
plt.title(f'All Points ≈ {mean_distance:.2f} pixels way from a wall')

# Overlay points at mean distance
plt.plot(np.where(mask)[1], np.where(mask)[0], 'ro', markersize=2)

plt.show()

#%%  Plot PDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

# Given values (from your fit)
mean = best_mean       # μ
cv2 = best_cv2         # CV²

# --- Convert mean and CV² to Beta parameters ---
# Relationship:
#   μ = α / (α + β)
#   CV² = (αβ) / [(α + β)² (α + β + 1)]
# Solve for α, β
alpha = ((1 - mean) / cv2 - 1 / mean) * mean ** 2
beta_param = alpha * (1 / mean - 1)

# Create x range
x = np.linspace(0, 1, 200)

# Compute PDF
pdf = beta.pdf(x, alpha, beta_param)

# Plot PDF
plt.figure(figsize=(6, 4))
plt.plot(x, pdf, 'r-', lw=2, label=f'Beta PDF\nmean={mean:.3f}, CV²={cv2:.3f}\nα={alpha:.2f}, β={beta_param:.2f}')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Beta Distribution (from mean and CV²)')
plt.legend()
plt.grid(alpha=0.3)
plt.show()


#%% Sizes histogram

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy import stats

# =================== Get BigroomData ===================
bigroom_maps = plist[0]  # BigroomData is index 0

# Use 8-connectivity
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# =================== Collect sizes of all place fields ===================
all_sizes = []

for neuron_map in bigroom_maps:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)

all_sizes = np.array(all_sizes)

# =================== Apply threshold ===================
size_threshold = 0   # change threshold if desired
filtered_sizes = all_sizes[all_sizes > size_threshold]

print(f"Total fields before threshold: {len(all_sizes)}")
print(f"After threshold ({size_threshold}): {len(filtered_sizes)}")

# =================== Fit discrete distributions ===================
fit_results = {}

# Poisson
lambda_hat = np.mean(filtered_sizes)
poisson_loglik = np.sum(stats.poisson.logpmf(filtered_sizes, mu=lambda_hat))
fit_results["Poisson"] = (stats.poisson, (lambda_hat,), poisson_loglik)

# Negative Binomial (r, p)
mean_ = np.mean(filtered_sizes)
var_ = np.var(filtered_sizes)
if var_ > mean_:  # NB only valid when variance > mean
    r_hat = mean_**2 / (var_ - mean_)
    p_hat = r_hat / (r_hat + mean_)
    nb_loglik = np.sum(stats.nbinom.logpmf(filtered_sizes, n=r_hat, p=p_hat))
    fit_results["NegBinom"] = (stats.nbinom, (r_hat, p_hat), nb_loglik)

# Geometric
p_geom = 1 / (1 + np.mean(filtered_sizes))
geom_loglik = np.sum(stats.geom.logpmf(filtered_sizes, p=p_geom))
fit_results["Geometric"] = (stats.geom, (p_geom,), geom_loglik)

# =================== Pick best fit by log-likelihood ===================
best_name, (best_dist, best_params, best_ll) = max(fit_results.items(), key=lambda x: x[1][2])

print(f"\nBest discrete fit: {best_name}")
print(f"Parameters: {best_params}")
print(f"Log-likelihood: {best_ll:.2f}")

# =================== Plot histogram + best-fit PMF ===================
plt.figure(figsize=(7,5))

# Histogram (counts, not normalized)
counts, bins, _ = plt.hist(filtered_sizes,
                           bins=np.arange(min(filtered_sizes), max(filtered_sizes)+1),
                           color='skyblue', edgecolor='black', alpha=0.7,
                           density=False, label='Data (counts)')

# Scale PMF to counts
x = np.arange(min(filtered_sizes), max(filtered_sizes)+1)
pmf = best_dist.pmf(x, *best_params)
pmf_scaled = pmf * len(filtered_sizes)

# Format parameters for legend
param_text = ", ".join([f"{pname}={val:.2f}" for pname, val in zip(best_dist.shapes.split(',') if best_dist.shapes else ['λ'], best_params)]) \
             if hasattr(best_dist, 'shapes') else ", ".join([f"{val:.2f}" for val in best_params])

plt.plot(x, pmf_scaled, 'r-', lw=2,
         label=f"{best_name} fit\nparams: {param_text}")

plt.xlabel('Place Field Size (pixels)')
plt.ylabel('Count')
plt.title(f'Histogram + Best Discrete Fit (BigroomData)\nSize > {size_threshold} pixels')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% The CDF of sizes

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy import stats

# =================== Get BigroomData ===================
bigroom_maps = plist[0]  # BigroomData is index 0

# Use 8-connectivity
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# =================== Collect sizes of all place fields ===================
all_sizes = []

for neuron_map in bigroom_maps:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)

all_sizes = np.array(all_sizes)

# =================== Apply threshold ===================
size_threshold = 0   # change threshold if desired
filtered_sizes = all_sizes[all_sizes > size_threshold]

print(f"Total fields before threshold: {len(all_sizes)}")
print(f"After threshold ({size_threshold}): {len(filtered_sizes)}")

# =================== Fit discrete distributions ===================
fit_results = {}

# Poisson
lambda_hat = np.mean(filtered_sizes)
poisson_loglik = np.sum(stats.poisson.logpmf(filtered_sizes, mu=lambda_hat))
fit_results["Poisson"] = (stats.poisson, (lambda_hat,), poisson_loglik)

# Negative Binomial (r, p)
mean_ = np.mean(filtered_sizes)
var_ = np.var(filtered_sizes)
if var_ > mean_:  # NB only valid when variance > mean
    r_hat = mean_**2 / (var_ - mean_)
    p_hat = r_hat / (r_hat + mean_)
    nb_loglik = np.sum(stats.nbinom.logpmf(filtered_sizes, n=r_hat, p=p_hat))
    fit_results["NegBinom"] = (stats.nbinom, (r_hat, p_hat), nb_loglik)

# Geometric
p_geom = 1 / (1 + np.mean(filtered_sizes))
geom_loglik = np.sum(stats.geom.logpmf(filtered_sizes, p=p_geom))
fit_results["Geometric"] = (stats.geom, (p_geom,), geom_loglik)

# =================== Pick best fit by log-likelihood ===================
best_name, (best_dist, best_params, best_ll) = max(fit_results.items(), key=lambda x: x[1][2])

print(f"\nBest discrete fit: {best_name}")
print(f"Parameters: {best_params}")
print(f"Log-likelihood: {best_ll:.2f}")

# =================== Plot CDF (empirical + model) ===================
plt.figure(figsize=(7,5))

# Sort data for empirical CDF
sorted_sizes = np.sort(filtered_sizes)
empirical_cdf = np.arange(1, len(sorted_sizes)+1) / len(sorted_sizes)

plt.step(sorted_sizes, empirical_cdf, where='post', label='Empirical CDF', color='blue')

# Theoretical CDF
x = np.arange(min(filtered_sizes), max(filtered_sizes)+1)
theoretical_cdf = best_dist.cdf(x, *best_params)
plt.plot(x, theoretical_cdf, 'r-', lw=2,
         label=f"{best_name} fit\nparams: {', '.join([f'{p:.2f}' for p in best_params])}")

plt.xlabel('Place Field Size (pixels)')
plt.ylabel('CDF')
plt.title(f'Empirical vs Best-Fit CDF (BigroomData)\nSize > {size_threshold} pixels')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% Plot all CDFS for sizes

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy import stats

# =================== Get BigroomData ===================
bigroom_maps = plist[0]  # BigroomData is index 0

# Use 8-connectivity
structure = np.array([[1,1,1],
                      [1,1,1],
                      [1,1,1]])

# =================== Collect sizes of all place fields ===================
all_sizes = []

for neuron_map in bigroom_maps:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)

all_sizes = np.array(all_sizes)

# =================== Apply threshold ===================
size_threshold = 0   # change threshold if desired
filtered_sizes = all_sizes[all_sizes > size_threshold]

print(f"Total fields before threshold: {len(all_sizes)}")
print(f"After threshold ({size_threshold}): {len(filtered_sizes)}")

# =================== Fit discrete distributions ===================
fit_results = {}

# Poisson
lambda_hat = np.mean(filtered_sizes)
poisson_loglik = np.sum(stats.poisson.logpmf(filtered_sizes, mu=lambda_hat))
fit_results["Poisson"] = (stats.poisson, (lambda_hat,), poisson_loglik)

# Negative Binomial (r, p)
mean_ = np.mean(filtered_sizes)
var_ = np.var(filtered_sizes)
if var_ > mean_:  # NB only valid when variance > mean
    r_hat = mean_**2 / (var_ - mean_)
    p_hat = r_hat / (r_hat + mean_)
    nb_loglik = np.sum(stats.nbinom.logpmf(filtered_sizes, n=r_hat, p=p_hat))
    fit_results["NegBinom"] = (stats.nbinom, (r_hat, p_hat), nb_loglik)

# Geometric
p_geom = 1 / (1 + np.mean(filtered_sizes))
geom_loglik = np.sum(stats.geom.logpmf(filtered_sizes, p=p_geom))
fit_results["Geometric"] = (stats.geom, (p_geom,), geom_loglik)

# =================== Pick best fit by log-likelihood ===================
best_name, (best_dist, best_params, best_ll) = max(fit_results.items(), key=lambda x: x[1][2])
print(f"\nBest discrete fit: {best_name}")
print(f"Parameters: {best_params}")
print(f"Log-likelihood: {best_ll:.2f}")

# =================== Plot Empirical CDF + All Fits ===================
plt.figure(figsize=(8,5))

# Empirical CDF
sorted_sizes = np.sort(filtered_sizes)
empirical_cdf = np.arange(1, len(sorted_sizes)+1) / len(sorted_sizes)
plt.step(sorted_sizes, empirical_cdf, where='post', label='Empirical CDF', color='black')

# Theoretical CDFs for each fit
colors = {'Poisson':'red', 'NegBinom':'green', 'Geometric':'orange'}

for name, (dist, params, _) in fit_results.items():
    x = np.arange(min(filtered_sizes), max(filtered_sizes)+1)
    cdf_vals = dist.cdf(x, *params)
    param_text = ", ".join([f"{p:.2f}" for p in params])
    plt.plot(x, cdf_vals, lw=2, color=colors.get(name, 'blue'),
             label=f"{name} CDF\nparams: {param_text}")

plt.xlabel('Place Field Size (pixels)')
plt.ylabel('CDF')
plt.title(f'Empirical vs All Discrete Fits (BigroomData)\nSize > {size_threshold} pixels')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% Generate maps with ovals

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy.stats import rankdata

# ===================== Helper: sample from empirical CDF =====================
def sample_from_cdf(values, n_samples):
    """
    Sample n_samples from the empirical CDF of 'values'.
    """
    sorted_vals = np.sort(values)
    probs = np.arange(1, len(values)+1) / len(values)
    uniform_samples = np.random.rand(n_samples)
    # Map uniform [0,1] to the empirical CDF
    sampled_indices = np.searchsorted(probs, uniform_samples, side='right')
    return sorted_vals[sampled_indices]

# ===================== Inputs from Bigroom data =====================
# Number of fields per neuron
num_fields_per_neuron = np.array(nlist[0])

# Sizes of place fields
all_sizes = []
structure = np.array([[1,1,1],[1,1,1],[1,1,1]])
for neuron_map in plist[0]:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)
all_sizes = np.array(all_sizes)

# ===================== Synthetic map generation =====================
map_shape = (65, 116)  # example Bigroom size, adjust as needed
num_neurons_to_simulate = 5

synthetic_maps = []

for i in range(num_neurons_to_simulate):
    # Sample number of place fields for this neuron
    n_fields = int(sample_from_cdf(num_fields_per_neuron, 1)[0])

    # Sample sizes for each field
    field_sizes = sample_from_cdf(all_sizes, n_fields)

    # Initialize empty matrix
    neuron_map = np.zeros(map_shape)

    for size in field_sizes:
        # Sample random centroid coordinate
        row = np.random.randint(0, map_shape[0])
        col = np.random.randint(0, map_shape[1])

        # Create an oval (ellipse) at that point
        # We'll approximate oval as filled ellipse in matrix
        rr, cc = np.meshgrid(np.arange(map_shape[0]), np.arange(map_shape[1]), indexing='ij')
        # scale the axes: area ~ size => area = pi*a*b => a = sqrt(size/pi*aspect_ratio), b = sqrt(size/pi/aspect_ratio)
        aspect_ratio = 1.5  # ellipse ratio, width/height
        b = np.sqrt(size / (np.pi * aspect_ratio))
        a = b * aspect_ratio

        # Ellipse formula ((x-h)/a)^2 + ((y-k)/b)^2 <= 1
        ellipse_mask = ((rr - row)/a)**2 + ((cc - col)/b)**2 <= 1
        neuron_map[ellipse_mask] = 1

    synthetic_maps.append(neuron_map)

# ===================== Plot synthetic maps =====================
fig, axes = plt.subplots(1, num_neurons_to_simulate, figsize=(15, 3))
if num_neurons_to_simulate == 1:
    axes = [axes]

for ax, neuron_map in zip(axes, synthetic_maps):
    ax.imshow(neuron_map, cmap='hot', interpolation='nearest')
    ax.set_title('Synthetic Neuron Map')
    ax.axis('off')

plt.show()

#%% Generate maps with blobby shapes

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label

# ===================== Helper: sample from empirical CDF =====================
def sample_from_cdf(values, n_samples):
    sorted_vals = np.sort(values)
    probs = np.arange(1, len(values)+1) / len(values)
    uniform_samples = np.random.rand(n_samples)
    sampled_indices = np.searchsorted(probs, uniform_samples, side='right')
    return sorted_vals[sampled_indices]

# ===================== Inputs from Bigroom data =====================
num_fields_per_neuron = np.array(nlist[0])

# Sizes of place fields
all_sizes = []
structure = np.array([[1,1,1],[1,1,1],[1,1,1]])
for neuron_map in plist[0]:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)
all_sizes = np.array(all_sizes)

# ===================== Synthetic map generation =====================
map_shape = plist[0][0].shape  # use actual neuron map shape
num_neurons_to_simulate = 1

synthetic_maps = []

for i in range(num_neurons_to_simulate):
    n_fields = int(sample_from_cdf(num_fields_per_neuron, 1)[0])
    field_sizes = sample_from_cdf(all_sizes, n_fields)
    neuron_map = np.zeros(map_shape)

    rr, cc = np.meshgrid(np.arange(map_shape[0]), np.arange(map_shape[1]), indexing='ij')

    for size in field_sizes:
        # Random centroid
        row = np.random.randint(0, map_shape[0])
        col = np.random.randint(0, map_shape[1])

        # Irregular patch: sum of a few small ellipses
        patch = np.zeros_like(neuron_map)
        num_blobs = np.random.randint(2, 5)  # 2–4 blobs per patch
        for _ in range(num_blobs):
            # Small random offset from centroid
            r_offset = row + np.random.randint(-3, 4)
            c_offset = col + np.random.randint(-3, 4)
            r_offset = np.clip(r_offset, 0, map_shape[0]-1)
            c_offset = np.clip(c_offset, 0, map_shape[1]-1)

            # Random ellipse axes
            aspect_ratio = np.random.uniform(0.5, 2.0)
            b = np.sqrt(size / (num_blobs * np.pi * aspect_ratio))
            a = b * aspect_ratio

            ellipse_mask = ((rr - r_offset)/a)**2 + ((cc - c_offset)/b)**2 <= 1
            patch[ellipse_mask] = 1

        neuron_map = np.maximum(neuron_map, patch)  # combine patches

    synthetic_maps.append(neuron_map)

# ===================== Plot synthetic maps =====================
fig, axes = plt.subplots(1, num_neurons_to_simulate, figsize=(15, 3))
if num_neurons_to_simulate == 1:
    axes = [axes]

for ax, neuron_map in zip(axes, synthetic_maps):
    ax.imshow(neuron_map, cmap='hot', interpolation='nearest')
    ax.set_title('Synthetic Neuron Map')
    ax.axis('off')

plt.show()


#%% Generate maps with irregular patches using NegBinom for field count

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy import stats

# ===================== Fit Negative Binomial to number of place fields =====================
num_fields_per_neuron = np.array(nlist[0])  # Bigroom data
mean_n = np.mean(num_fields_per_neuron)
var_n = np.var(num_fields_per_neuron)

# Only fit NB if variance > mean
if var_n > mean_n:
    r_nb = mean_n**2 / (var_n - mean_n)
    p_nb = r_nb / (r_nb + mean_n)
else:
    r_nb = mean_n
    p_nb = 0.5  # fallback, geometric-like

print(f"Fitted Negative Binomial: r={r_nb:.2f}, p={p_nb:.2f}")

# Sizes of place fields
all_sizes = []
structure = np.array([[1,1,1],[1,1,1],[1,1,1]])
for neuron_map in plist[0]:
    binary_map = 1 * (neuron_map > 0)
    labeled_array, num_features = label(binary_map, structure=structure)
    for label_id in range(1, num_features + 1):
        size = np.sum(labeled_array == label_id)
        all_sizes.append(size)
all_sizes = np.array(all_sizes)

# ===================== Helper: sample from empirical CDF =====================
def sample_from_cdf(values, n_samples):
    sorted_vals = np.sort(values)
    probs = np.arange(1, len(values)+1) / len(values)
    uniform_samples = np.random.rand(n_samples)
    sampled_indices = np.searchsorted(probs, uniform_samples, side='right')
    return sorted_vals[sampled_indices]

# ===================== Synthetic map generation =====================
map_shape = plist[0][0].shape  # use actual neuron map shape
num_neurons_to_simulate = 1

synthetic_maps = []

for i in range(num_neurons_to_simulate):
    # Sample number of fields from Negative Binomial
    n_fields = stats.nbinom.rvs(r_nb, p_nb)
    n_fields = max(1, n_fields)  # ensure at least 1 field

    # Sample sizes for each field
    field_sizes = sample_from_cdf(all_sizes, n_fields)
    neuron_map = np.zeros(map_shape)

    rr, cc = np.meshgrid(np.arange(map_shape[0]), np.arange(map_shape[1]), indexing='ij')

    for size in field_sizes:
        # Random centroid
        row = np.random.randint(0, map_shape[0])
        col = np.random.randint(0, map_shape[1])

        # Irregular patch: sum of a few small ellipses
        patch = np.zeros_like(neuron_map)
        num_blobs = np.random.randint(2, 5)  # 2–4 blobs per patch
        for _ in range(num_blobs):
            r_offset = row + np.random.randint(-3, 4)
            c_offset = col + np.random.randint(-3, 4)
            r_offset = np.clip(r_offset, 0, map_shape[0]-1)
            c_offset = np.clip(c_offset, 0, map_shape[1]-1)

            aspect_ratio = np.random.uniform(0.5, 2.0)
            b = np.sqrt(size / (num_blobs * np.pi * aspect_ratio))
            a = b * aspect_ratio

            ellipse_mask = ((rr - r_offset)/a)**2 + ((cc - c_offset)/b)**2 <= 1
            patch[ellipse_mask] = 1

        neuron_map = np.maximum(neuron_map, patch)
        
    synthetic_maps.append(neuron_map)

# ===================== Plot synthetic maps =====================
fig, axes = plt.subplots(1, num_neurons_to_simulate, figsize=(15, 3))
if num_neurons_to_simulate == 1:
    axes = [axes]

for ax, neuron_map in zip(axes, synthetic_maps):
    ax.imshow(neuron_map, cmap='hot', interpolation='nearest')
    ax.set_title('Synthetic Neuron Map')
    ax.axis('off')

plt.show()


#%% Aggregate synthetic neurons and histogram per-pixel occupancy

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label
from scipy import stats

# -------------------- Use previous synthetic map generator --------------------
# Assume you already have:
#   - r_nb, p_nb : fitted Negative Binomial for number of fields
#   - all_sizes  : empirical sizes of place fields
#   - map_shape  : shape of the neuron map
#   - sample_from_cdf() function

num_synthetic_neurons = 200  # number of neurons to aggregate
aggregated_map = np.zeros(map_shape, dtype=int)

for i in range(num_synthetic_neurons):
    # Sample number of fields for this neuron
    n_fields = stats.nbinom.rvs(r_nb, p_nb)
    n_fields = max(1, n_fields)

    # Sample field sizes
    field_sizes = sample_from_cdf(all_sizes, n_fields)

    neuron_map = np.zeros(map_shape, dtype=int)
    rr, cc = np.meshgrid(np.arange(map_shape[0]), np.arange(map_shape[1]), indexing='ij')

    for size in field_sizes:
        # Random centroid
        row = np.random.randint(0, map_shape[0])
        col = np.random.randint(0, map_shape[1])

        # Irregular patch
        patch = np.zeros_like(neuron_map)
        num_blobs = np.random.randint(2, 5)
        for _ in range(num_blobs):
            r_offset = row + np.random.randint(-3, 4)
            c_offset = col + np.random.randint(-3, 4)
            r_offset = np.clip(r_offset, 0, map_shape[0]-1)
            c_offset = np.clip(c_offset, 0, map_shape[1]-1)

            aspect_ratio = np.random.uniform(0.5, 2.0)
            b = np.sqrt(size / (num_blobs * np.pi * aspect_ratio))
            a = b * aspect_ratio

            ellipse_mask = ((rr - r_offset)/a)**2 + ((cc - c_offset)/b)**2 <= 1
            patch[ellipse_mask] = 1

        neuron_map = np.maximum(neuron_map, patch)

    aggregated_map += neuron_map  # accumulate counts per pixel

# -------------------- Histogram of counts per pixel --------------------
pixel_counts = aggregated_map.flatten()
plt.figure(figsize=(7,5))
counts, bins, _ = plt.hist(pixel_counts, bins=np.arange(pixel_counts.min(), pixel_counts.max()+2),
                           color='skyblue', edgecolor='black', density=False)
plt.xlabel('Number of place fields per pixel')
plt.ylabel('Count of pixels')
plt.title('Histogram of pixel occupancy (aggregated 200 synthetic neurons)')
plt.grid(alpha=0.3)
plt.show()

# -------------------- Fit discrete distributions --------------------
fit_results = {}

# Poisson fit
lambda_hat = np.mean(pixel_counts)
poisson_loglik = np.sum(stats.poisson.logpmf(pixel_counts, mu=lambda_hat))
fit_results["Poisson"] = (stats.poisson, (lambda_hat,), poisson_loglik)

# Negative Binomial fit
mean_pix = np.mean(pixel_counts)
var_pix = np.var(pixel_counts)
if var_pix > mean_pix:
    r_hat = mean_pix**2 / (var_pix - mean_pix)
    p_hat = r_hat / (r_hat + mean_pix)
    nb_loglik = np.sum(stats.nbinom.logpmf(pixel_counts, n=r_hat, p=p_hat))
    fit_results["NegBinom"] = (stats.nbinom, (r_hat, p_hat), nb_loglik)

# Geometric fit
p_geom = 1 / (1 + mean_pix)
geom_loglik = np.sum(stats.geom.logpmf(pixel_counts, p=p_geom))
fit_results["Geometric"] = (stats.geom, (p_geom,), geom_loglik)

# -------------------- Best fit --------------------
best_name, (best_dist, best_params, best_ll) = max(fit_results.items(), key=lambda x: x[1][2])
print(f"Best fit: {best_name}, params: {best_params}, log-likelihood={best_ll:.2f}")

# -------------------- Plot histogram + best-fit PMF --------------------
plt.figure(figsize=(7,5))
counts, bins, _ = plt.hist(pixel_counts, bins=np.arange(pixel_counts.min(), pixel_counts.max()+2),
                           color='skyblue', edgecolor='black', alpha=0.7, density=False, label='Data')

x = np.arange(pixel_counts.min(), pixel_counts.max()+1)
pmf = best_dist.pmf(x, *best_params)
pmf_scaled = pmf * len(pixel_counts)

# Legend formatting
param_text = ", ".join([f"{val:.2f}" for val in best_params])
plt.plot(x, pmf_scaled, 'r-', lw=2, label=f"{best_name} fit\nparams: {param_text}")

plt.xlabel('Number of place fields per pixel')
plt.ylabel('Count of pixels')
plt.title('Pixel occupancy histogram + best discrete fit')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#%% Plot an equal distance map

import numpy as np
import matplotlib.pyplot as plt

def plot_distance_grid(room_shape, start_dist=3, step=3, max_dist_ratio=0.5, show_plot=True):
    """
    Generate a matrix marking points at certain distances from nearest wall.

    Parameters:
    - room_shape: tuple (H, W) of the room
    - start_dist: first distance from wall
    - step: distance increment
    - max_dist_ratio: max distance as fraction of smaller room dimension
    - show_plot: whether to display the plot

    Returns:
    - output: numpy array with 1s at points, 0s elsewhere
    """
    H, W = room_shape
    output = np.zeros((H, W))
    max_dist = int(min(H, W) * max_dist_ratio)

    for D in range(start_dist, max_dist + 1, step):
        for i in range(H):
            for j in range(W):
                dist_to_wall = min(i, H-1-i, j, W-1-j)
                if dist_to_wall == D:
                    output[i,j] = 1

    if show_plot:
        plt.figure(figsize=(6,6))
        plt.imshow(output, cmap='gray', origin='upper')
        plt.title(f"Points at distances {start_dist},{start_dist+step},{start_dist+step+step} from nearest wall")
        plt.axis('off')
        plt.show()

    return output

# ================= Example Usage =================
room_shape = plist[0][0].shape  # shape of first neuron
grid = plot_distance_grid(room_shape, start_dist=5, step=10)

#%%

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.optimize import minimize

# ---------------------------------------------------------------------
# Assume pixel_counts already computed from aggregated_map
# ---------------------------------------------------------------------

# Example: simulate some pixel_counts if not already defined
# pixel_counts = np.random.negative_binomial(2, 0.4, size=50000)

# -------------------- Histogram --------------------
plt.figure(figsize=(7,5))
counts, bins, _ = plt.hist(pixel_counts, bins=np.arange(pixel_counts.min(), pixel_counts.max()+2),
                           color='skyblue', edgecolor='black', density=False)
plt.xlabel('Number of place fields per pixel')
plt.ylabel('Count of pixels')
plt.title('Histogram of pixel occupancy')
plt.grid(alpha=0.3)
plt.show()

# -------------------- Fit Binomial --------------------
# Estimate n (max observed count is a good starting point)
n_hat = np.max(pixel_counts)
mean_pix = np.mean(pixel_counts)
p_hat_binom = mean_pix / n_hat
binom_loglik = np.sum(stats.binom.logpmf(pixel_counts, n=n_hat, p=p_hat_binom))

# -------------------- Fit Negative Binomial --------------------
var_pix = np.var(pixel_counts)
if var_pix > mean_pix:
    r_hat = mean_pix**2 / (var_pix - mean_pix)
    p_hat_nb = r_hat / (r_hat + mean_pix)
    nb_loglik = np.sum(stats.nbinom.logpmf(pixel_counts, n=r_hat, p=p_hat_nb))
else:
    nb_loglik = -np.inf
    r_hat, p_hat_nb = np.nan, np.nan

# -------------------- Fit Mixture of two Binomials --------------------
def negloglik_mixed(params, data):
    w, n1, p1, n2, p2 = params
    if not (0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1):
        return np.inf
    pmf1 = stats.binom.pmf(data, int(n1), p1)
    pmf2 = stats.binom.pmf(data, int(n2), p2)
    mix = w * pmf1 + (1 - w) * pmf2
    # Avoid log(0)
    mix[mix <= 1e-12] = 1e-12
    return -np.sum(np.log(mix))

# Initial guess
init_guess = [0.5, np.max(pixel_counts), p_hat_binom, np.max(pixel_counts)//2, p_hat_binom/2]
bounds = [(1e-3, 1-1e-3), (1, 200), (1e-3, 1-1e-3), (1, 200), (1e-3, 1-1e-3)]

res = minimize(negloglik_mixed, init_guess, args=(pixel_counts,), bounds=bounds)
if res.success:
    w_hat, n1_hat, p1_hat, n2_hat, p2_hat = res.x
    mixed_loglik = -res.fun
else:
    w_hat, n1_hat, p1_hat, n2_hat, p2_hat = [np.nan]*5
    mixed_loglik = -np.inf

# -------------------- Compare fits --------------------
fit_results = {
    "Binomial": (stats.binom, (n_hat, p_hat_binom), binom_loglik),
    "NegBinom": (stats.nbinom, (r_hat, p_hat_nb), nb_loglik),
    "MixedBinom": (None, (w_hat, n1_hat, p1_hat, n2_hat, p2_hat), mixed_loglik)
}

best_name, (best_dist, best_params, best_ll) = max(fit_results.items(), key=lambda x: x[1][2])
print("\n=== Fit Results ===")
for name, (dist, params, ll) in fit_results.items():
    print(f"{name:12s}: log-likelihood = {ll:.2f}, params = {params}")
print(f"\nBest fit: {best_name}")

# -------------------- Plot all fits --------------------
plt.figure(figsize=(8,5))
counts, bins, _ = plt.hist(pixel_counts, bins=np.arange(pixel_counts.min(), pixel_counts.max()+2),
                           color='skyblue', edgecolor='black', alpha=0.6, density=False, label='Data')
x = np.arange(pixel_counts.min(), pixel_counts.max()+1)
Npix = len(pixel_counts)

# Binomial
pmf_binom = stats.binom.pmf(x, n_hat, p_hat_binom)
plt.plot(x, pmf_binom * Npix, 'r-', lw=2, label=f'Binomial (n={n_hat}, p={p_hat_binom:.3f})')

# Negative Binomial
if np.isfinite(nb_loglik):
    pmf_nb = stats.nbinom.pmf(x, r_hat, p_hat_nb)
    plt.plot(x, pmf_nb * Npix, 'g-', lw=2, label=f'NegBinom (r={r_hat:.2f}, p={p_hat_nb:.3f})')

# Mixed Binomial
if np.isfinite(mixed_loglik):
    pmf_mix = (w_hat * stats.binom.pmf(x, int(n1_hat), p1_hat) +
               (1 - w_hat) * stats.binom.pmf(x, int(n2_hat), p2_hat))
    plt.plot(x, pmf_mix * Npix, 'm-', lw=2,
             label=(f'MixBinom (w={w_hat:.2f}, n1={n1_hat:.1f}, p1={p1_hat:.3f}, '
                    f'n2={n2_hat:.1f}, p2={p2_hat:.3f})'))

plt.xlabel('Number of place fields per pixel')
plt.ylabel('Count of pixels')
plt.title('Pixel occupancy histogram + Binomial family fits')
plt.legend(fontsize=8)
plt.grid(alpha=0.3)
plt.show()



    
#%% Center setup

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import beta, norm, betabinom, binom, gamma, lognorm  # probability distributions
from scipy.optimize import minimize  # optimization for likelihood-based fits
from sklearn.mixture import GaussianMixture  # gmm fitting to raw data

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import Counter

# =================== Setup ===================
dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [[(65, 116), (65, 117)],  # BigroomData
              [(43, 53), (43, 50), (43, 57)],  # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)],  # RectangleData
              [(20, 20)]]  # SquareData

location = 0  # Change if needed

# =================== Collect all data ===================
p_extended = [[] for _ in range(len(dnames))]  # One list per dataset
plist = []
nlist = []

for dnametype in range(len(dnames)):
    p = []
    n = []

    # Set data directory
    if location == 0:
        dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
    elif location == 1:
        dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
    
    dir2 = os.listdir(dir1)

    for foldername in dir2:
        dir3 = os.path.join(dir1, foldername)
        if not os.path.isdir(dir3):
            continue
        dirs3 = os.listdir(dir3)
        dirs4 = [x for x in dirs3 if x[-7] == 'm']
        for matname in dirs4:
            mat = loadmat(os.path.join(dir3, matname))
            n.append(mat['numFields'][0][0])
            p.append(mat['pfields'])  # original, unmodified matrices

    # =================== Count original shapes with proportion ===================
    original_shapes = [pmap.shape for pmap in p]
    shape_counts = Counter(original_shapes)
    total_maps = len(original_shapes)
    
    print(f"{dnames[dnametype]} ({total_maps})")
    for shape, count in shape_counts.items():
        proportion = count / total_maps
        print(f"{shape}: {count} ({proportion:.2f})")
    print()

    # =================== Compute minimum shape and crop ===================
    min_x = min(x for x, y in shapeslist[dnametype])
    min_y = min(y for x, y in shapeslist[dnametype])
    target_shape = (min_x, min_y)

    cropped_list = []
    for pi in p:
        h, w = pi.shape
        H, W = target_shape
        cropped = pi[:H, :W]  # crop to minimum dimensions
        cropped_list.append(cropped)

    p_extended[dnametype] = cropped_list
    nlist.append(n)
    plist.append(p)
    
    
p1=[]
for room in plist:
    plistroom=[]
    for neuron in room:
        plistroom.append(1*(neuron>0))
    p1.append(plistroom)
    
#%% Histogram PDF (center) with AIC for Gamma / Weibull / Normal mixtures
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import gamma, norm, weibull_min
from scipy.special import gamma as gamma_func

data = np.array(all_distances)

# ===================== Helper =====================
def compute_aic(loglik, k):
    return 2*k - 2*loglik

def mixture_moments(pdf_func, params):
    w = params[-1]
    if pdf_func == norm.pdf:
        mu1, sigma1, mu2, sigma2, _ = params
        mean1, mean2 = mu1, mu2
        var1, var2 = sigma1**2, sigma2**2
    elif pdf_func == gamma.pdf:
        k1, theta1, k2, theta2, _ = params
        mean1 = k1*theta1
        mean2 = k2*theta2
        var1 = k1*theta1**2
        var2 = k2*theta2**2
    elif pdf_func == weibull_min.pdf:
        c1, scale1, c2, scale2, _ = params
        mean1 = scale1 * gamma_func(1 + 1/c1)
        mean2 = scale2 * gamma_func(1 + 1/c2)
        var1 = scale1**2 * (gamma_func(1 + 2/c1) - (gamma_func(1 + 1/c1))**2)
        var2 = scale2**2 * (gamma_func(1 + 2/c2) - (gamma_func(1 + 1/c2))**2)
    CV2_1 = var1 / mean1**2
    CV2_2 = var2 / mean2**2
    return mean1, CV2_1, mean2, CV2_2

def fit_mixture_pdf(pdf_func, x0, bounds, data):
    def negloglik(params, data):
        *params_dist, w = params
        if not (0 < w < 1) or any(p <= 0 for p in params_dist):
            return np.inf
        if pdf_func == norm.pdf:
            mu1, sigma1, mu2, sigma2 = params_dist
            pdf_vals = w*pdf_func(data, loc=mu1, scale=sigma1) + (1-w)*pdf_func(data, loc=mu2, scale=sigma2)
        elif pdf_func == gamma.pdf:
            k1, theta1, k2, theta2 = params_dist
            pdf_vals = w*pdf_func(data, a=k1, scale=theta1) + (1-w)*pdf_func(data, a=k2, scale=theta2)
        elif pdf_func == weibull_min.pdf:
            c1, scale1, c2, scale2 = params_dist
            pdf_vals = w*pdf_func(data, c=c1, scale=scale1) + (1-w)*pdf_func(data, c=c2, scale=scale2)
        return -np.sum(np.log(pdf_vals + 1e-12))
    
    res = minimize(negloglik, x0=x0, args=(data,), bounds=bounds)
    params_opt = res.x
    loglik = -res.fun
    aic = compute_aic(loglik, len(params_opt))
    
    # compute mixture PDF for plotting
    x_vals = np.linspace(0, data.max(), 500)
    if pdf_func == norm.pdf:
        mu1, sigma1, mu2, sigma2, w = params_opt
        pdf_vals_plot = w*pdf_func(x_vals, loc=mu1, scale=sigma1) + (1-w)*pdf_func(x_vals, loc=mu2, scale=sigma2)
    elif pdf_func == gamma.pdf:
        k1, theta1, k2, theta2, w = params_opt
        pdf_vals_plot = w*pdf_func(x_vals, a=k1, scale=theta1) + (1-w)*pdf_func(x_vals, a=k2, scale=theta2)
    elif pdf_func == weibull_min.pdf:
        c1, scale1, c2, scale2, w = params_opt
        pdf_vals_plot = w*pdf_func(x_vals, c=c1, scale=scale1) + (1-w)*pdf_func(x_vals, c=c2, scale=scale2)
    
    mean1, cv2_1, mean2, cv2_2 = mixture_moments(pdf_func, params_opt)
    
    return params_opt, aic, mean1, cv2_1, mean2, cv2_2, x_vals, pdf_vals_plot

# ===================== Fit mixtures =====================
gamma_params, aic_gamma, mean1_gamma, cv2_1_gamma, mean2_gamma, cv2_2_gamma, x_gamma, pdf_gamma = fit_mixture_pdf(
    gamma.pdf, x0=[1,1,3,1,0.5], bounds=[(1e-3,None)]*4 + [(1e-3,1-1e-3)], data=data)

weibull_params, aic_weibull, mean1_weibull, cv2_1_weibull, mean2_weibull, cv2_2_weibull, x_weibull, pdf_weibull = fit_mixture_pdf(
    weibull_min.pdf, x0=[1,1,2,1,0.5], bounds=[(1e-3,None)]*4 + [(1e-3,1-1e-3)], data=data)

norm_params, aic_norm, mean1_norm, cv2_1_norm, mean2_norm, cv2_2_norm, x_norm, pdf_norm = fit_mixture_pdf(
    norm.pdf, x0=[data.mean(), data.std(), data.mean()*1.5, data.std(), 0.5],
    bounds=[(None,None),(1e-3,None),(None,None),(1e-3,None),(1e-3,1-1e-3)], data=data)

# ===================== Plot with w and AIC on next row =====================
plt.figure(figsize=(8,5))
plt.hist(data, bins=30, density=True, alpha=0.5, color='lightgray', edgecolor='black', label='Data')

plt.plot(x_gamma, pdf_gamma, 'b-', lw=2,
         label=(f'Gamma mixture\nmean1={mean1_gamma:.2f}, CV1²={cv2_1_gamma:.3f}\n'
                f'mean2={mean2_gamma:.2f}, CV2²={cv2_2_gamma:.3f}\n'
                f'w={gamma_params[-1]:.2f}, AIC={aic_gamma:.1f}'))

plt.plot(x_weibull, pdf_weibull, 'g-', lw=2,
         label=(f'Weibull mixture\nmean1={mean1_weibull:.2f}, CV1²={cv2_1_weibull:.3f}\n'
                f'mean2={mean2_weibull:.2f}, CV2²={cv2_2_weibull:.3f}\n'
                f'w={weibull_params[-1]:.2f}, AIC={aic_weibull:.1f}'))

plt.plot(x_norm, pdf_norm, 'c-', lw=2,
         label=(f'Normal mixture\nmean1={mean1_norm:.2f}, CV1²={cv2_1_norm:.3f}\n'
                f'mean2={mean2_norm:.2f}, CV2²={cv2_2_norm:.3f}\n'
                f'w={norm_params[-1]:.2f}, AIC={aic_norm:.1f}'))

plt.xlabel('Distance between placefield centroids and center')
plt.ylabel('Density')
plt.legend(fontsize=8)
plt.grid(alpha=0.3)
plt.show()

#%% Plot the center and the centroids and distances to the center

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label, center_of_mass

# ===================== Example: use first map =====================
place_map = p1[0][0]  # first neuron map
binary_map = 1 * (place_map > 0)

# 8-connectivity
structure = np.ones((3, 3))
labeled_array, num_features = label(binary_map, structure=structure)

# ===================== Find centroids =====================
centroids = center_of_mass(binary_map, labeled_array, range(1, num_features + 1))

# ===================== Compute distances to center =====================
h, w = binary_map.shape
center = np.array([h / 2, w / 2])

distances = []
for c in centroids:
    c = np.array(c)
    d = np.linalg.norm(c - center)
    distances.append(d)
distances = np.array(distances)

# ===================== Plot =====================
plt.figure(figsize=(6,6))
plt.imshow(place_map, cmap='Wistia', origin='upper')
plt.title('Place Map with Centroids and Distances to Center')
plt.scatter([c[1] for c in centroids], [c[0] for c in centroids],
            s=60, facecolors='blue', edgecolors='blue', label='Centroids', )
for i, (c, d) in enumerate(zip(centroids, distances)):
    plt.text(c[1]+2, c[0], f'{d:.1f}', color='blue', fontsize=8, ha='left', va='center')

plt.scatter(center[1], center[0], c='red', marker='x', s=100, label='Center')

plt.legend(loc='upper center')
plt.axis('off')
plt.show()

print("Centroid distances to center:")
for i, d in enumerate(distances, 1):
    print(f"Field {i}: {d:.2f}")
    
#%% Find the overlap number for each placefield (Meeting 6)
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import beta, norm, betabinom, binom, gamma, lognorm  # probability distributions
from scipy.optimize import minimize  # optimization for likelihood-based fits
from sklearn.mixture import GaussianMixture  # gmm fitting to raw data

import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import Counter

# =================== Setup ===================
dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [[(65, 116), (65, 117)],  # BigroomData
              [(43, 53), (43, 50), (43, 57)],  # CircleData
              [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
              [(12, 12), (13, 13)],  # HexagonData
              [(24, 36), (31, 40)],  # RectangleData
              [(20, 20)]]  # SquareData

location = 0  # Change if needed

# =================== Collect all data ===================
p_extended = [[] for _ in range(len(dnames))]  # One list per dataset
plist = []
nlist = []

for dnametype in range(len(dnames)):
    p = []
    n = []

    # Set data directory
    if location == 0:
        dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
    elif location == 1:
        dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'
    
    dir2 = os.listdir(dir1)

    for foldername in dir2:
        dir3 = os.path.join(dir1, foldername)
        if not os.path.isdir(dir3):
            continue
        dirs3 = os.listdir(dir3)
        dirs4 = [x for x in dirs3 if x[-7] == 'm']
        for matname in dirs4:
            mat = loadmat(os.path.join(dir3, matname))
            n.append(mat['numFields'][0][0])
            p.append(mat['pfields'])  # original, unmodified matrices

    # =================== Count original shapes with proportion ===================
    original_shapes = [pmap.shape for pmap in p]
    shape_counts = Counter(original_shapes)
    total_maps = len(original_shapes)
    
    print(f"{dnames[dnametype]} ({total_maps})")
    for shape, count in shape_counts.items():
        proportion = count / total_maps
        print(f"{shape}: {count} ({proportion:.2f})")
    print()

    # =================== Compute minimum shape and crop ===================
    min_x = min(x for x, y in shapeslist[dnametype])
    min_y = min(y for x, y in shapeslist[dnametype])
    target_shape = (min_x, min_y)

    cropped_list = []
    for pi in p:
        h, w = pi.shape
        H, W = target_shape
        cropped = pi[:H, :W]  # crop to minimum dimensions
        cropped_list.append(cropped)

    p_extended[dnametype] = cropped_list
    nlist.append(n)
    plist.append(p)
    
p1=[]
for room in plist:
    plistroom=[]
    for neuron in room:
        plistroom.append(1*(neuron>0))
    p1.append(plistroom)

#%% Setup: Overlap Histogram for Place Fields (Meeting 6)
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.ndimage import label
from collections import Counter

# =================== Setup ===================
dnames = ['BigroomData', 'CircleData', 'HalfroomData',
          'HexagonData', 'RectangleData', 'SquareData']

shapeslist = [
    [(65, 116), (65, 117)],                      # BigroomData
    [(43, 53), (43, 50), (43, 57)],              # CircleData
    [(62, 55), (63, 60), (63, 59), (62, 57), (62, 56)],  # HalfroomData
    [(12, 12), (13, 13)],                        # HexagonData
    [(24, 36), (31, 40)],                        # RectangleData
    [(20, 20)]                                   # SquareData
]

location = 0  # 0 = Ogamb path, 1 = Olgud path

# =================== Collect all data ===================
plist = []
nlist = []

for dnametype in range(len(dnames)):
    p = []
    n = []

    # Set directory
    if location == 0:
        dir1 = rf'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\DataforDrSingh\{dnames[dnametype]}'
    elif location == 1:
        dir1 = rf'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\{dnames[dnametype]}'

    dir2 = os.listdir(dir1)

    for foldername in dir2:
        dir3 = os.path.join(dir1, foldername)
        if not os.path.isdir(dir3):
            continue
        dirs3 = os.listdir(dir3)
        dirs4 = [x for x in dirs3 if x[-7] == 'm']
        for matname in dirs4:
            mat = loadmat(os.path.join(dir3, matname))
            n.append(mat['numFields'][0][0])
            p.append(mat['pfields'])

    # Show dataset info
    original_shapes = [pmap.shape for pmap in p]
    shape_counts = Counter(original_shapes)
    total_maps = len(original_shapes)
    print(f"{dnames[dnametype]} ({total_maps})")
    for shape, count in shape_counts.items():
        proportion = count / total_maps
        print(f"{shape}: {count} ({proportion:.2f})")
    print()

    plist.append(p)
    nlist.append(n)

# =================== Crop to smallest shape ===================
p_cropped = [[] for _ in range(len(dnames))]

for dnametype in range(len(dnames)):
    min_x = min([xy[0] for xy in shapeslist[dnametype]])
    min_y = min([xy[1] for xy in shapeslist[dnametype]])
    target_shape = (min_x, min_y)

    cropped_list = []
    for pi in plist[dnametype]:
        h, w = pi.shape
        H, W = target_shape
        cropped = pi[:H, :W]  # crop to smallest shape
        cropped_list.append(cropped)

    p_cropped[dnametype] = cropped_list

plist = p_cropped

# =================== Binarize ===================
p1 = []
for room in plist:
    plistroom = []
    for neuron in room:
        plistroom.append(1 * (neuron > 0))
    p1.append(plistroom)

#%%
# =================== Compute overlap counts ===================
all_overlap_counts = []

for d, room in enumerate(p1):
    print(f"Processing overlaps for {dnames[d]}...")
    all_fields = []
    for neuron in room:
        labeled, num_labels = label(neuron > 0)
        for i in range(1, num_labels + 1):
            field_mask = (labeled == i)
            all_fields.append(field_mask)

    n_fields = len(all_fields)
    overlap_counts = np.zeros(n_fields, dtype=int)

    for i in range(n_fields):
        for j in range(n_fields):
            if i == j:
                continue
            if np.any(all_fields[i] & all_fields[j]):
                overlap_counts[i] += 1

    all_overlap_counts.extend(overlap_counts)
    
#%% Thresholded histogram of overlap counts

a, b = 0, 300  # <-- set your desired threshold range here

# Threshold data
data = np.array(all_overlap_counts)
data = data[(data >= a) & (data <= b)]

# Plot histogram
plt.figure(figsize=(6, 4))
plt.hist(data,
         bins=np.arange(a, np.max(data) + 2) - 0.5,
         edgecolor='black')
plt.xlabel('Number of overlapping place fields')
plt.ylabel('Count')
plt.title(f'Histogram of Overlapping Place Fields (All Datasets)\nRange [{a}, {b}]')
plt.tight_layout()
plt.xlim([a, b])
plt.show()

#%% Plot best fit distribution
from scipy.stats import poisson, nbinom
import numpy as np
import matplotlib.pyplot as plt

# --- Threshold range ---
a, b = 0, 300  # <-- set your desired threshold range here

# --- Threshold data ---
data = np.array(all_overlap_counts)
data = data[(data >= a) & (data <= b)]

# --- Fit Poisson ---
mu_hat = np.mean(data)
x = np.arange(a, np.max(data) + 1)
poisson_pmf = poisson.pmf(x, mu_hat)

# --- Fit Negative Binomial ---
mean = np.mean(data)
var = np.var(data)
if var > mean:
    p_nb = mean / var
    r_nb = mean**2 / (var - mean)
    nbinom_pmf = nbinom.pmf(x, r_nb, p_nb)
else:
    nbinom_pmf = None

# --- Prepare histogram ---
plt.figure(figsize=(6, 4))
counts, bins, _ = plt.hist(data,
                           bins=np.arange(a, np.max(data) + 2) - 0.5,
                           edgecolor='black',
                           color='lightgray',
                           label='Data')

# Scale PMFs to histogram counts
scale_factor = len(data) * (bins[1] - bins[0])

if nbinom_pmf is not None:
    plt.plot(x, nbinom_pmf * scale_factor, 'r', lw=2,
             label=f'NegBin fit (r={r_nb:.1f}, p={p_nb:.2f})')

# --- Labels and formatting ---
plt.xlabel('Number of overlapping place fields')
plt.ylabel('Count')
plt.title(f'Histogram of Overlapping Place Fields (All Datasets)\nRange [{a}, {b}]')
plt.legend()
plt.tight_layout()
plt.xlim([a, b])
plt.show()

#%% Generate sample uniform centroids and plot PDF

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom, binom
from scipy.special import comb, beta as beta_func
from scipy.optimize import minimize
from scipy.ndimage import label, center_of_mass
import random

# =================== 1. Fit Negative Binomial ===================
def fit_negative_binomial(data):
    data = np.array(data)
    mean_pf = np.mean(data)
    var_pf = np.var(data)
    if var_pf <= mean_pf:
        var_pf = mean_pf + 1e-3
    p = mean_pf / var_pf
    r = mean_pf * p / (1 - p)
    return r, p

def sample_n_fields(r, p):
    return nbinom.rvs(r, p)

# =================== 2. Sample centroid locations ===================
def sample_centroids(matrix_shape, pf_shapes, n_fields, max_attempts=1000):
    H, W = matrix_shape
    centroids = []
    occupancy = np.zeros(matrix_shape)
    for _ in range(n_fields):
        placed = False
        attempts = 0
        while not placed and attempts < max_attempts:
            attempts += 1
            pf_shape = random.choice(pf_shapes)
            h_pf, w_pf = pf_shape
            if h_pf > H or w_pf > W:
                continue
            x = np.random.randint(0, H - h_pf + 1)
            y = np.random.randint(0, W - w_pf + 1)
            if np.sum(occupancy[x:x+h_pf, y:y+w_pf]) == 0:
                centroids.append((x, y, pf_shape))
                occupancy[x:x+h_pf, y:y+w_pf] = 1
                placed = True
    return centroids

# =================== 3. Place fields into matrix ===================
def generate_matrix(pfields, matrix_shape, n_fields):
    pf_shapes = [pf.shape for pf in pfields]
    centroids = sample_centroids(matrix_shape, pf_shapes, n_fields)
    new_matrix = np.zeros(matrix_shape)
    for x, y, shape in centroids:
        candidates = [pf for pf in pfields if pf.shape == shape]
        if not candidates:
            continue
        pf = random.choice(candidates)
        new_matrix[x:x+shape[0], y:y+shape[1]] += pf
    return new_matrix

# =================== 4. Generate multiple non-blank matrices and sum ===================
def generate_nonblank_matrices(pfields, nlist, matrix_shape, n_matrices=100):
    r, p = fit_negative_binomial(nlist)
    summed_matrix = np.zeros(matrix_shape)
    count = 0
    while count < n_matrices:
        n_fields_sampled = sample_n_fields(r, p)
        if n_fields_sampled == 0:
            continue
        mat = generate_matrix(pfields, matrix_shape, n_fields_sampled)
        summed_matrix += mat
        count += 1
    return summed_matrix

# =================== 5. Histogram helpers ===================
def hist_with_bins(data):
    max_val = int(np.max(data))
    bins = np.arange(-0.5, max_val+1.5, 1)
    hist, bins = np.histogram(data, bins=bins)
    bin_centers = (bins[:-1] + bins[1:])/2
    return hist, bin_centers

# =================== 6. Beta-Binomial PMF ===================
def beta_binomial_pmf(k, n, a, b):
    return comb(n, k) * beta_func(k+a, n-k+b) / beta_func(a, b)

# =================== 7. Fit Binomial Mixture ===================
def binomial_mixture_nll(params, data, n_max):
    f, p1, p2 = params
    pmf = f*binom.pmf(data, n_max, p1) + (1-f)*binom.pmf(data, n_max, p2)
    pmf = np.clip(pmf, 1e-10, None)
    return -np.sum(np.log(pmf))

def fit_binomial_mixture(data):
    n_max = max(int(max(data)), 1)
    x0 = [0.5, 0.2, 0.8]
    bounds = [(0,1), (0,1), (0,1)]
    res = minimize(binomial_mixture_nll, x0, args=(data, n_max), bounds=bounds)
    return res.x, res.fun, n_max

# =================== 8. Fit Beta-Binomial Mixture ===================
def beta_binomial_mixture_nll(params, data, n_max):
    f, a1, b1, a2, b2 = params
    pmf = f*beta_binomial_pmf(data, n_max, a1, b1) + (1-f)*beta_binomial_pmf(data, n_max, a2, b2)
    pmf = np.clip(pmf, 1e-10, None)
    return -np.sum(np.log(pmf))

def fit_beta_binomial_mixture(data):
    n_max = max(int(max(data)), 1)
    x0 = [0.5, 2, 5, 5, 2]
    bounds = [(0,1),(1e-2,None),(1e-2,None),(1e-2,None),(1e-2,None)]
    res = minimize(beta_binomial_mixture_nll, x0, args=(data,n_max), bounds=bounds)
    return res.x, res.fun, n_max

# =================== 9. Plot functions ===================
def plot_summed_matrix(matrix):
    plt.figure(figsize=(8,6))
    plt.imshow(matrix, cmap='viridis', origin='lower')
    plt.colorbar(label="Number of place fields")
    plt.title("Summed Place Fields Across Matrices")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.show()

def plot_histogram_neuron(nlist_data):
    hist, bin_centers = hist_with_bins(nlist_data)
    plt.figure(figsize=(8,5))
    plt.bar(bin_centers, hist, width=0.8, align='center', color='lightgreen', edgecolor='black', linewidth=1)
    plt.xlabel("Number of place fields per neuron")
    plt.ylabel("Number of neurons")
    plt.title("Histogram of Place Fields per Neuron")
    plt.xticks(bin_centers.astype(int))
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Fit negative binomial
    r, p = fit_negative_binomial(nlist_data)
    x_fit = np.arange(int(max(nlist_data))+1)
    nb_pmf = nbinom.pmf(x_fit, r, p)*len(nlist_data)
    nb_mean = r*(1-p)/p
    nb_std = np.sqrt(r*(1-p)/(p**2))
    nb_cv2 = (nb_std/nb_mean)**2
    nb_aic = 2*2 - 2*np.sum(nbinom.logpmf(nlist_data, r, p))
    plt.plot(x_fit, nb_pmf, 'r-o', label=f'NB: mean={nb_mean:.2f}, CV²={nb_cv2:.2f}, AIC={nb_aic:.1f}')
    plt.legend(fontsize=8)
    plt.show()

def plot_histogram_pixel_with_distributions(summed_matrix):
    data = summed_matrix.flatten()
    hist, bin_centers = hist_with_bins(data)
    x_fit = np.arange(int(max(data))+1)

    # NB
    r, p = fit_negative_binomial(data)
    nb_pmf = nbinom.pmf(x_fit, r, p)*len(data)
    nb_mean = r*(1-p)/p
    nb_std = np.sqrt(r*(1-p)/(p**2))
    nb_cv2 = (nb_std/nb_mean)**2
    nb_aic = 2*2 - 2*np.sum(nbinom.logpmf(data, r, p))

    # Binomial Mixture
    bm_params, bm_nll, n_max = fit_binomial_mixture(data)
    f_bm, p1_bm, p2_bm = bm_params
    bm_pmf = (f_bm*binom.pmf(x_fit, n_max, p1_bm) + (1-f_bm)*binom.pmf(x_fit, n_max, p2_bm))*len(data)
    bm_mean = f_bm*n_max*p1_bm + (1-f_bm)*n_max*p2_bm
    bm_var = f_bm*n_max*p1_bm*(1-p1_bm) + (1-f_bm)*n_max*p2_bm*(1-p2_bm) + f_bm*(1-f_bm)*(n_max*(p1_bm-p2_bm))**2
    bm_cv2 = bm_var/bm_mean**2
    bm_aic = 2*3 - 2*(-bm_nll)

    # Beta-Binomial Mixture
    bb_params, bb_nll, n_max = fit_beta_binomial_mixture(data)
    f_bb, a1_bb, b1_bb, a2_bb, b2_bb = bb_params
    pmf1 = beta_binomial_pmf(x_fit, n_max, a1_bb, b1_bb)
    pmf2 = beta_binomial_pmf(x_fit, n_max, a2_bb, b2_bb)
    bb_pmf = (f_bb*pmf1 + (1-f_bb)*pmf2)*len(data)
    bb_mean = f_bb*a1_bb/(a1_bb+b1_bb)*n_max + (1-f_bb)*a2_bb/(a2_bb+b2_bb)*n_max
    var1 = n_max*a1_bb*b1_bb/((a1_bb+b1_bb)**2*(a1_bb+b1_bb+1))
    var2 = n_max*a2_bb*b2_bb/((a2_bb+b2_bb)**2*(a2_bb+b2_bb+1))
    bb_var = f_bb*var1 + (1-f_bb)*var2
    bb_cv2 = bb_var/bb_mean**2
    bb_aic = 2*5 - 2*(-bb_nll)

    plt.figure(figsize=(9,6))
    plt.bar(bin_centers, hist, width=0.8, align='center', color='lightblue', edgecolor='black', linewidth=1, label='Data')
    plt.plot(x_fit, nb_pmf, 'r-o', label=f'NB: mean={nb_mean:.2f}, CV²={nb_cv2:.2f}, AIC={nb_aic:.1f}')
    plt.plot(x_fit, bm_pmf, 'g-s', label=f'Binom Mix: f={f_bm:.2f}, p1={p1_bm:.2f}, p2={p2_bm:.2f}, mean={bm_mean:.2f}, CV²={bm_cv2:.2f}, AIC={bm_aic:.1f}')
    plt.plot(x_fit, bb_pmf, 'm-^', label=f'Beta-Bin Mix: f={f_bb:.2f}, a1={a1_bb:.2f}, b1={b1_bb:.2f}, a2={a2_bb:.2f}, b2={b2_bb:.2f}, mean={bb_mean:.2f}, CV²={bb_cv2:.2f}, AIC={bb_aic:.1f}')
    plt.xlabel("Number of place fields per pixel")
    plt.ylabel("Number of pixels")
    plt.title("Histogram of Place Fields per Pixel with Distributions")
    plt.xticks(bin_centers.astype(int))
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.legend(fontsize=8)
    plt.show()

def plot_sample_generated_neuron(pfields, matrix_shape, nlist):
    r, p = fit_negative_binomial(nlist)
    n_fields_sampled = sample_n_fields(r, p)
    if n_fields_sampled==0: n_fields_sampled=1
    sample_matrix = generate_matrix(pfields, matrix_shape, n_fields_sampled)
    
    # Label contiguous fields
    structure = np.ones((3,3))
    labeled, n_features = label(sample_matrix>0, structure=structure)
    
    plt.figure(figsize=(6,5))
    plt.imshow(sample_matrix, cmap='hot', origin='lower')
    plt.colorbar(label="Firing rate (generated)")
    plt.title(f"Sample Generated Neuron Place Map ({n_features} fields)")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    
    # Add numbers at centroids
    for i in range(1, n_features+1):
        cx, cy = center_of_mass(sample_matrix, labeled, i)
        plt.text(cy, cx, str(i), color='red', fontsize=10, ha='center', va='center')
    plt.show()

# =================== 10. Example Usage ===================
dnum = 0
pfields = p1[dnum]       # list of place field arrays
nlist_data = nlist[dnum] # number of place fields per neuron
matrix_shape = min([pf.shape for pf in pfields], key=lambda x: x[0]*x[1])
n_matrices = 238

summed_matrix = generate_nonblank_matrices(pfields, nlist_data, matrix_shape, n_matrices=n_matrices)

# --- Plots ---
plot_summed_matrix(summed_matrix)
plot_histogram_neuron(nlist_data)
plot_histogram_pixel_with_distributions(summed_matrix)
plot_sample_generated_neuron(pfields, matrix_shape, nlist_data)

#%% test
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom
from scipy.ndimage import label
import random

# ============================================================
# 1. Fit Negative Binomial
# ============================================================
def fit_negative_binomial(data):
    mean_ = np.mean(data)
    var_ = np.var(data)
    if var_ <= mean_:
        var_ = mean_ + 1e-6
    r = mean_**2 / (var_ - mean_)
    p = r / (r + mean_)
    return r, p

# ============================================================
# 2. Sample Number of Fields per Neuron
# ============================================================
def sample_n_fields(r, p):
    return nbinom.rvs(r, p)

# ============================================================
# 3. Helper: Place Field Random Placement Without Overlap
# ============================================================
def place_field_random(matrix, pf, max_attempts=100):
    m_h, m_w = matrix.shape
    pf_h, pf_w = pf.shape

    if pf_h > m_h or pf_w > m_w:
        pf = pf[:m_h, :m_w]

    for _ in range(max_attempts):
        y0 = random.randint(0, m_h - pf_h)
        x0 = random.randint(0, m_w - pf_w)

        region = matrix[y0:y0 + pf_h, x0:x0 + pf_w]
        if np.all(region == 0):
            matrix[y0:y0 + pf_h, x0:x0 + pf_w] = pf
            return True
    return False

# ============================================================
# 4. Generate Matrix (Non-overlapping Placement)
# ============================================================
def generate_matrix_nonoverlapping(pfields, matrix_shape, n_fields):
    matrix = np.zeros(matrix_shape)
    for _ in range(n_fields):
        pf = random.choice(pfields)
        pf_norm = (pf - pf.min()) / (pf.max() - pf.min() + 1e-6)
        pf_bin = (pf_norm > 0.3).astype(float)
        placed = place_field_random(matrix, pf_bin)
        if not placed:
            continue
    return matrix

# ============================================================
# 5. Generate Multiple Matrices and Sum
# ============================================================
def generate_nonblank_matrices(pfields, nlist, matrix_shape, n_matrices=100):
    r, p = fit_negative_binomial(nlist)
    all_matrices = []
    for _ in range(n_matrices):
        n_fields_sampled = sample_n_fields(r, p)
        if n_fields_sampled <= 0:
            n_fields_sampled = 1
        m = generate_matrix_nonoverlapping(pfields, matrix_shape, n_fields_sampled)
        all_matrices.append(m)
    summed_matrix = np.sum(all_matrices, axis=0)
    return summed_matrix, all_matrices

# ============================================================
# 6. Plot Functions
# ============================================================
def plot_summed_matrix(summed_matrix):
    plt.figure(figsize=(6, 5))
    plt.imshow(summed_matrix, cmap='hot', origin='lower')
    plt.colorbar(label="Summed Firing Rate")
    plt.title("Summed Generated Place Fields (Non-overlapping)")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.show()

def plot_histogram_neuron_from_generated(all_matrices):
    """
    Histogram of number of place fields per generated neuron.
    """
    n_fields_per_matrix = []
    for mat in all_matrices:
        structure = np.ones((3,3))
        _, n_fields = label(mat > 0, structure=structure)
        n_fields_per_matrix.append(n_fields)
    n_fields_per_matrix = np.array(n_fields_per_matrix)
    
    plt.figure(figsize=(6, 4.5))
    bins = np.arange(n_fields_per_matrix.min() - 0.5, n_fields_per_matrix.max() + 1.5, 1)
    plt.hist(n_fields_per_matrix, bins=bins, density=True, 
             color='skyblue', edgecolor='black', linewidth=1.2, alpha=0.9)
    
    # Fit Negative Binomial
    r, p = fit_negative_binomial(n_fields_per_matrix)
    x = np.arange(n_fields_per_matrix.min(), n_fields_per_matrix.max() + 1)
    plt.plot(x, nbinom.pmf(x, r, p), 'r-o', lw=2, markersize=5, label='NegBinom Fit')
    
    mean = np.mean(n_fields_per_matrix)
    cv2 = np.var(n_fields_per_matrix) / mean**2
    plt.legend(title=f"mean={mean:.2f}\nCV²={cv2:.2f}")
    
    plt.xlabel("Number of Place Fields per Neuron", fontsize=12)
    plt.ylabel("Probability", fontsize=12)
    plt.title("Distribution of Place Fields per Generated Neuron", fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.xticks(x)
    plt.tight_layout()
    plt.show()


def plot_histogram_pixel_with_distributions(summed_matrix):
    """
    Histogram of place fields per pixel.
    """
    data = summed_matrix.flatten()
    data_nonzero = data[data > 0]
    
    plt.figure(figsize=(6, 4.5))
    bins = np.arange(data_nonzero.min() - 0.5, data_nonzero.max() + 1.5, 1)
    plt.hist(data_nonzero, bins=bins, color='lightgreen', edgecolor='black', linewidth=1.2, alpha=0.9)
    
    plt.xlabel("Number of Place Fields per Pixel", fontsize=12)
    plt.ylabel("Count", fontsize=12)
    plt.title("Histogram of Place Fields per Pixel", fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.xticks(np.arange(int(data_nonzero.min()), int(data_nonzero.max()) + 1))
    plt.tight_layout()
    plt.show()


def plot_sample_generated_neuron(pfields, matrix_shape, nlist):
    r, p = fit_negative_binomial(nlist)
    n_fields_sampled = sample_n_fields(r, p)
    if n_fields_sampled <= 0:
        n_fields_sampled = 1
    matrix = np.zeros(matrix_shape)
    for i in range(n_fields_sampled):
        pf = random.choice(pfields)
        pf_norm = (pf - pf.min()) / (pf.max() - pf.min() + 1e-6)
        pf_bin = (pf_norm > 0.3).astype(float)
        placed = place_field_random(matrix, pf_bin)
        if not placed:
            continue
        matrix[matrix > 0] = i + 1

    plt.figure(figsize=(6, 5))
    plt.imshow(matrix, origin='lower', cmap='tab20')
    plt.colorbar(label="Field Label")
    plt.title(f"Sample Generated Neuron Place Map ({n_fields_sampled} fields, non-overlapping)")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    
    structure = np.ones((3,3))
    labeled, n_fields = label(matrix > 0, structure=structure)
    for i in range(1, n_fields + 1):
        cy, cx = np.array(np.nonzero(labeled == i)).mean(axis=1)
        plt.text(cx, cy, str(i), color='red', fontsize=10, ha='center', va='center')
    
    plt.show()

# ============================================================
# 7. Example Usage
# ============================================================
dnum = 0
n_matrices = 238

# Flatten list if nested
if isinstance(p1[dnum][0], np.ndarray):
    all_pfields = p1[dnum]
    nlist_data = nlist[dnum]
else:
    all_pfields = [pf for neuron_fields in p1[dnum] for pf in neuron_fields]
    nlist_data = np.concatenate(nlist[dnum])

matrix_shape = min([pf.shape for pf in all_pfields], key=lambda x: x[0] * x[1])

summed_matrix, all_generated_matrices = generate_nonblank_matrices(
    all_pfields, nlist_data, matrix_shape, n_matrices=n_matrices
)

# --- Plots ---
plot_summed_matrix(summed_matrix)
plot_histogram_neuron_from_generated(all_generated_matrices)
plot_histogram_pixel_with_distributions(summed_matrix)
plot_sample_generated_neuron(all_pfields, matrix_shape, nlist_data)
