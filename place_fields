#%%
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import nbinom
from scipy.optimize import minimize

# ========================= Load data from .mat files =========================
n = [] # number of place fields
p = [] # neuron maps

location=1
if location==0:
    dir1 = r'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\space field\BigroomData'
if location==1:
    dir1 = r'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\BigroomData'
os.chdir(dir1)
dir2 = os.listdir(dir1)

for foldername in dir2:
    dir3 = os.path.join(dir1, foldername)
    os.chdir(dir3)
    dirs3 = os.listdir()
    dirs4 = [x for x in dirs3 if x[-7] == 'm']
    for matname in dirs4:
        dir5 = os.path.join(dir3, matname)
        mat = loadmat(dir5)
        n.append(mat['numFields'][0][0])
        p.append(mat['pfields'])
        
        
        
#%%  Threshold (5 and 10) and divide into rectangles

th=0
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
pth = [(pi>th)*pi for pi in p_trimmed]
pthc = np.array([(pi>th)*1 for pi in p_trimmed]) # convert to 1s and 0s

rates=np.unique(p_trimmed)

# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
#       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]

# Convert them to grids and count per grid

grid=(1,1)
pshape = pth[0].shape # (65, 116) 

x1 = pshape[0]//grid[0]
x2 = pshape[1]//grid[1]

plist=[]
clist=[]
for pi in pthc:
    patches=np.zeros((x1,x2), dtype=object)
    counts=np.zeros((x1,x2))
    for i in range(x1):
        for j in range(x2):
            patch=pi[i*grid[0]:(i+1)*grid[0],j*grid[1]:+(j+1)*grid[1]]
            count = 1*np.any(patch>0)
            patches[i,j]=patch
            counts[i,j]=count
    plist.append(patches)
    clist.append(counts)

csum=np.sum(clist, axis=0)
data=csum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)
fs=15
plt.hist(data, bins=binarr, density=True, 
         color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
plt.title('Searching a ({},{})-grid and counting all AP rates > {}'.format(grid[0], grid[1], th))
plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), 
           fontsize=fs)
plt.ylabel('Density of # neurons', fontsize=fs)
plt.tick_params(axis='both',labelsize=fs)
plt.show()
# =============================================================================
# plt.figure()
# plt.imshow(csum, cmap='hot')
# plt.title('Number of neurons firing in a ({},{})-grid'.format(grid[0], grid[1]))
# plt.colorbar()
# =============================================================================

#%%


# =================== Fit NB with integer r using grid search ===================

# =============================================================================
# possible_r = np.arange(1, 50)  # Try integer r from 1 to 49
# best_ll = -np.inf
# best_r = None
# best_p = None
#
# for r in possible_r:
#     mean = np.mean(n)
#     var = np.var(n)
#     if var > mean:  # NB only defined for overdispersed data
#         p_hat = r / (r + mean)
#         ll = np.sum(nbinom.logpmf(n, r, p_hat))
#         if ll > best_ll:
#             best_ll = ll
#             best_r = r
#             best_p = p_hat
# 
# rhat, phat = best_r, best_p
# 
# =============================================================================
# Original MLE estimation using continuous parameters
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(n,), bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

# ========================= Plot histogram + fitted distribution =========================
binarr = np.arange(np.min(n) - 0.5, np.max(n) + 1.5)
binplot = np.arange(np.min(n), np.max(n) + 1)

plt.figure(figsize=(8, 3))
plt.hist(n, bins=binarr, color='skyblue', edgecolor='black', alpha=0.7, density=True)
pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('Density of # of neurons', fontsize=12)
plt.xlabel('# of place fields for a neuron', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

# =============================================================================
# # Plot out all the place fields
# for i in range(len(p)):
#     plt.figure()
#     plt.imshow(p[i])
#     plt.colorbar()
# =============================================================================

pcount = [np.where(pi[:, :-1] != 0, 1, 0) if pi.shape[1] == 117 else np.where(pi != 0, 1, 0) for pi in p]

from collections import Counter

shape_counts = Counter([pi.shape for pi in pcount])
print(shape_counts) 

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom, norm

pcountsum = np.sum(pcount, axis=0)

plt.figure()
plt.imshow(pcountsum, cmap='hot')
plt.title('Number of neurons that fire at each pixel\nin the rectangular room')
plt.colorbar()

#%%

binarr = np.arange(np.min(pcountsum) - 0.5, np.max(pcountsum) + 1.5)
binplot = np.arange(np.min(pcountsum), np.max(pcountsum) + 1)
# Flatten data for fitting
data_flat = pcountsum.flatten()
# Fit Normal distribution
mu_norm, std_norm = norm.fit(data_flat)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skewnorm
from scipy.optimize import minimize

# Flatten and bin data
data = pcountsum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

# ======== Fit Skew-Normal ========
a_skew, loc_skew, scale_skew = skewnorm.fit(data)
x_vals = np.linspace(np.min(data), np.max(data), 1000)
pdf_skew = skewnorm.pdf(x_vals, a_skew, loc=loc_skew, scale=scale_skew)

# ======== Fit Gaussian Mixture (2 components) ========
def neg_log_likelihood(params):
    w, mu1, sigma1, mu2, sigma2 = params
    if not (0 < w < 1 and sigma1 > 0 and sigma2 > 0):
        return np.inf
    pdf_vals = w * norm.pdf(data, mu1, sigma1) + (1 - w) * norm.pdf(data, mu2, sigma2)
    return -np.sum(np.log(pdf_vals + 1e-12))

initial_guess = [0.5, np.percentile(data, 25), 1.0, np.percentile(data, 75), 1.0]
bounds = [(1e-3, 1 - 1e-3), (None, None), (1e-3, None), (None, None), (1e-3, None)]
result = minimize(neg_log_likelihood, initial_guess, bounds=bounds)
w, mu1, sigma1, mu2, sigma2 = result.x
pdf_mix = w * norm.pdf(x_vals, mu1, sigma1) + (1 - w) * norm.pdf(x_vals, mu2, sigma2)
pdf1 = w * norm.pdf(x_vals, mu1, sigma1)
pdf2 = (1 - w) * norm.pdf(x_vals, mu2, sigma2)

# ======== Plot All Together ========
plt.figure(figsize=(9, 4))
plt.hist(data, bins=binarr, density=True, color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# =============================================================================
# plt.plot(x_vals, pdf_skew, 'g-', linewidth=2, label=f'Skew Normal (a={a_skew:.2f})')
# =============================================================================
plt.plot(x_vals, pdf_mix, 'r-', linewidth=2, label='Gaussian Mixture')
# =============================================================================
# plt.plot(x_vals, pdf1, 'k--', label='Component 1')
# plt.plot(x_vals, pdf2, 'm--', label='Component 2')
# =============================================================================

plt.xlabel('# of neurons firing APs at a pixel')
plt.ylabel('Density of # neurons firing APs at a pixel')
plt.xticks(binplot)
plt.legend()
plt.tight_layout()
plt.show()

# Trim and collect matrices
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Convert to NumPy array stack only if all shapes are now equal
shapes = [pi.shape for pi in p_trimmed]
common_shape = shapes[0]
assert all(shape == common_shape for shape in shapes), "Not all matrices have the same shape after trimming."
# Sum them together
pratesum = np.sum(p_trimmed, axis=0)
plt.figure()
plt.title('Sum of all AP rates at each location')
plt.imshow(pratesum, cmap='hot')
plt.colorbar()

plt.figure()
plt.title('Mean rate at each location')
meanmap=pratesum/pcountsum
plt.imshow(meanmap, cmap='hot')
plt.colorbar()

#%%
# Count occurrences of most common non-zero rate in each pi
counts = [np.count_nonzero(pi == np.bincount(pi.flatten())[1:].argmax() + 1) for pi in p]

# Get the most common non-zero rate in each pi
rates = [np.bincount(pi.flatten())[1:].argmax() + 1 for pi in p]

print(counts)
print(rates)

# Histogram of counts
plt.hist(counts, bins=np.arange(min(counts), max(counts) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Counts')
plt.xlabel('Count')
plt.ylabel('Frequency')
plt.xticks(np.unique(counts))
plt.show()

# Histogram of rates
plt.hist(rates, bins=np.arange(min(rates), max(rates) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Rates')
plt.xlabel('Rate')
plt.ylabel('Frequency')
plt.xticks(np.unique(rates))
plt.show()

#%% Number of neurons with AP rate

# For each pi, find the unique nonzero rates
unique_rates_per_pi = [np.unique(pi[pi != 0]) for pi in p]

# Flatten the list and combine all unique rates
# A list of all the rates that appear in each neuron uniquely
all_unique_rates = np.concatenate(unique_rates_per_pi)

# Plot histogram
plt.figure(figsize=(6,4))
plt.hist(all_unique_rates, 
         bins=np.arange(all_unique_rates.min(), all_unique_rates.max() + 2) - 0.5, 
         edgecolor='black', alpha=0.7, density=True)
plt.xlabel('AP rate')
plt.ylabel('# of neurons with this AP rate in their place map')
plt.xticks(np.unique(all_unique_rates))

def neg_log_likelihood(params, data):
    r, pp = params
    if r <= 0 or not (0 < pp < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, pp))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(all_unique_rates,), 
                  bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('# of neurons', fontsize=12)
plt.xlabel('AP rates (Hz)', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

#%% Percentage of map covered

p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
pcovered = [np.sum(pi.flatten()>0)/(pi.shape[0]*pi.shape[1]) for pi in p_trimmed]
plt.hist(pcovered, bins=100)

#%% What percentage of all maps do their largest place field have the largest rates?
# For each pi, check if the largest place field has the largest rate
matches = []
for pi in p:
    if np.all(pi == 0):
        matches.append(False)
        continue
    max_rate = pi[pi != 0].max()
    rate_with_largest_field = np.bincount(pi.flatten())[1:].argmax() + 1
    matches.append(rate_with_largest_field == max_rate)

# Calculate percentage
percentage = np.mean(matches)
print(percentage, "largest place field, largest rate")


#%% Smallest size largest rate?
# Check if the highest rate appears the least number of times in each pi
matches = []
for pi in p:
    if np.all(pi == 0):
        continue
    flat = pi.flatten()
    flat = flat[flat != 0]
    counts = np.bincount(flat)
    rates = np.arange(len(counts))
    rates = rates[1:]
    counts = counts[1:]
    max_rate = rates[np.argmax(rates)]
    min_count_rate = rates[np.argmin(counts)]
    matches.append(max_rate == min_count_rate)

# Compute percentage
percent = np.mean(matches)
print(percent, 'smallest place field, largest rate')

#%%
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import nbinom, weibull_min

data = all_unique_rates
data = data[data > 0]

# Histogram (density=True for PDF scale)
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# --- Negative log likelihood for Negative Binomial ---
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

# Fit Negative Binomial params
initial_guess = [5, 0.5]
result = minimize(neg_log_likelihood, initial_guess, args=(data,), 
                  bounds=[(1e-3, None), (1e-5, 1-1e-5)])
r_hat, p_hat = result.x

# Negative Binomial PMF over data range
x_plot = np.arange(data.min(), data.max() + 1)
pmf_fit = nbinom.pmf(x_plot, r_hat, p_hat)

# --- Weibull Fit ---
# Weibull is continuous, so we use MLE from scipy (force loc=0)
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x_wb = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x_wb, c_wb, loc=loc_wb, scale=scale_wb)

# To compare Weibull PDF with histogram and NB PMF, scale PDF:
# Multiply by bin width ~ 1 for visual comparison on histogram scale
pdf_wb_scaled = pdf_wb

# --- Plot ---
plt.figure(figsize=(6, 3))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.6, label='Data (histogram)', color='skyblue')
plt.plot(x_plot, pmf_fit, 'ro-', label=f'Negative Binomial Fit (r={r_hat:.2f}, p={p_hat:.2f})')
plt.plot(x_wb, pdf_wb_scaled, 'g-', label=f'Weibull Fit (k={c_wb:.2f}, scale={scale_wb:.2f})')
plt.xlabel('AP rate of a place field (Hz)')
plt.ylabel('Density of # of neurons')
plt.legend()
plt.tight_layout()
plt.show()

#%%

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import weibull_min, invgamma, fisk  # fisk = log-logistic

# Your positive data
data = all_unique_rates
data = data[data > 0]

# Histogram for visualization
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Fit Weibull (shape c), force loc=0
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x, c_wb, loc=loc_wb, scale=scale_wb)

# Fit Inverse Gamma (shape a), force loc=0
a_ig, loc_ig, scale_ig = invgamma.fit(data, floc=0)
pdf_ig = invgamma.pdf(x, a_ig, loc=loc_ig, scale=scale_ig)

# Fit Log-Logistic (Fisk), force loc=0
c_ll, loc_ll, scale_ll = fisk.fit(data, floc=0)
pdf_ll = fisk.pdf(x, c_ll, loc=loc_ll, scale=scale_ll)

# Function to find inflection points (zero crossings of second derivative)
def find_inflection_points(pdf_vals, x_vals):
    dx = x_vals[1] - x_vals[0]
    second_deriv = np.gradient(np.gradient(pdf_vals, dx), dx)
    zero_crossings = np.where(np.diff(np.sign(second_deriv)))[0]
    return x_vals[zero_crossings]

# Find inflection points for each PDF
inflect_wb = find_inflection_points(pdf_wb, x)
inflect_ig = find_inflection_points(pdf_ig, x)
inflect_ll = find_inflection_points(pdf_ll, x)

# Plot
plt.figure(figsize=(12, 6))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.5, label='Data (histogram)', color='skyblue')

plt.plot(x, pdf_wb, 'r-', label=f'Weibull (k={c_wb:.3f})')
for ip in inflect_wb:
    plt.axvline(ip, color='r', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ig, 'g-', label=f'Inverse Gamma (a={a_ig:.3f})')
for ip in inflect_ig:
    plt.axvline(ip, color='g', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ll, 'm-', label=f'Log-Logistic (c={c_ll:.3f})')
for ip in inflect_ll:
    plt.axvline(ip, color='m', linestyle='--', alpha=0.5)

plt.xlabel('Rate')
plt.ylabel('Probability density')
plt.title('Fits and Inflection Points of Weibull, Inverse Gamma, Log-Logistic')
plt.legend()
plt.tight_layout()
plt.show()

# Print inflection points
print("Inflection points (x values) where PDF curvature changes sign:")
print(f"Weibull: {inflect_wb}")
print(f"Inverse Gamma: {inflect_ig}")
print(f"Log-Logistic: {inflect_ll}")
