import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import nbinom
from scipy.optimize import minimize

# ========================= Load data from .mat files =========================
n = []
p = []

location=0
if location==0:
    dir1 = r'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\space field\BigroomData'
if location==1:
    dir1 = r'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\BigroomData'
os.chdir(dir1)
dir2 = os.listdir(dir1)

for foldername in dir2:
    dir3 = os.path.join(dir1, foldername)
    os.chdir(dir3)
    dirs3 = os.listdir()
    dirs4 = [x for x in dirs3 if x[-7] == 'm']
    for matname in dirs4:
        dir5 = os.path.join(dir3, matname)
        mat = loadmat(dir5)
        n.append(mat['numFields'][0][0])
        p.append(mat['pfields'])

# =================== Fit NB with integer r using grid search ===================

# =============================================================================
# possible_r = np.arange(1, 50)  # Try integer r from 1 to 49
# best_ll = -np.inf
# best_r = None
# best_p = None
# 
# for r in possible_r:
#     mean = np.mean(n)
#     var = np.var(n)
#     if var > mean:  # NB only defined for overdispersed data
#         p_hat = r / (r + mean)
#         ll = np.sum(nbinom.logpmf(n, r, p_hat))
#         if ll > best_ll:
#             best_ll = ll
#             best_r = r
#             best_p = p_hat
# 
# rhat, phat = best_r, best_p
# 
# =============================================================================
# Original MLE estimation using continuous parameters
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(n,), bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

# ========================= Plot histogram + fitted distribution =========================
binarr = np.arange(np.min(n) - 0.5, np.max(n) + 1.5)
binplot = np.arange(np.min(n), np.max(n) + 1)

plt.figure(figsize=(8, 3))
plt.hist(n, bins=binarr, color='skyblue', edgecolor='black', alpha=0.7, density=True)
pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat}, p={phat:.2f})')
plt.ylabel('Number of neurons', fontsize=12)
plt.xlabel('Number of place fields', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

# =============================================================================
# # Plot out all the place fields
# for i in range(len(p)):
#     plt.figure()
#     plt.imshow(p[i])
#     plt.colorbar()
# =============================================================================

pcount = [np.where(pi[:, :-1] != 0, 1, 0) if pi.shape[1] == 117 else np.where(pi != 0, 1, 0) for pi in p]

from collections import Counter

shape_counts = Counter([pi.shape for pi in pcount])
print(shape_counts) 

pcountsum = np.sum(pcount, axis=0)
plt.figure()
plt.imshow(pcountsum, cmap='hot')
plt.title('Number of neurons that fire at each location')
plt.colorbar()

# Trim and collect matrices
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Convert to NumPy array stack only if all shapes are now equal
shapes = [pi.shape for pi in p_trimmed]
common_shape = shapes[0]
assert all(shape == common_shape for shape in shapes), "Not all matrices have the same shape after trimming."
# Sum them together
pratesum = np.sum(p_trimmed, axis=0)
plt.figure()
plt.title('Sum of all AP rates at each location')
plt.imshow(pratesum, cmap='hot')
plt.colorbar()

plt.figure()
plt.title('Mean rate at each location')
meanmap=pratesum/pcountsum
plt.imshow(meanmap, cmap='hot')
plt.colorbar()

#%%
# Count occurrences of most common non-zero rate in each pi
counts = [np.count_nonzero(pi == np.bincount(pi.flatten())[1:].argmax() + 1) for pi in p]

# Get the most common non-zero rate in each pi
rates = [np.bincount(pi.flatten())[1:].argmax() + 1 for pi in p]

print(counts)
print(rates)

# Histogram of counts
plt.hist(counts, bins=np.arange(min(counts), max(counts) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Counts')
plt.xlabel('Count')
plt.ylabel('Frequency')
plt.xticks(np.unique(counts))
plt.show()

# Histogram of rates
plt.hist(rates, bins=np.arange(min(rates), max(rates) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Rates')
plt.xlabel('Rate')
plt.ylabel('Frequency')
plt.xticks(np.unique(rates))
plt.show()

#%% Number of neurons with AP rate

# For each pi, find the unique nonzero rates
unique_rates_per_pi = [np.unique(pi[pi != 0]) for pi in p]

# Flatten the list and combine all unique rates
all_unique_rates = np.concatenate(unique_rates_per_pi)

# Plot histogram
plt.hist(all_unique_rates, 
         bins=np.arange(all_unique_rates.min(), all_unique_rates.max() + 2) - 0.5, 
         edgecolor='black', alpha=0.7, density=True)
plt.title('Distribution of Unique Rates Across all maps')
plt.xlabel('AP rate')
plt.ylabel('# of neurons with this AP rate in their place map')
plt.xticks(np.unique(all_unique_rates))

def neg_log_likelihood(params, data):
    r, pp = params
    if r <= 0 or not (0 < pp < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, pp))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(all_unique_rates,), 
                  bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat}, p={phat:.2f})')
plt.ylabel('Number of neurons', fontsize=12)
plt.xlabel('Number of place fields', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()


#%% Percentage of map covered

p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
pcovered = [np.sum(pi.flatten()>0)/(pi.shape[0]*pi.shape[1]) for pi in p_trimmed]
plt.hist(pcovered, bins=201)

#%% What percentage of all maps do their largest place field have the largest rates?
# For each pi, check if the largest place field has the largest rate
matches = []
for pi in p:
    if np.all(pi == 0):
        matches.append(False)
        continue
    max_rate = pi[pi != 0].max()
    rate_with_largest_field = np.bincount(pi.flatten())[1:].argmax() + 1
    matches.append(rate_with_largest_field == max_rate)

# Calculate percentage
percentage = np.mean(matches)
print(percentage, "largest place field, largest rate")


#%% Smallest size largest rate?
# Check if the highest rate appears the least number of times in each pi
matches = []
for pi in p:
    if np.all(pi == 0):
        continue
    flat = pi.flatten()
    flat = flat[flat != 0]
    counts = np.bincount(flat)
    rates = np.arange(len(counts))
    rates = rates[1:]
    counts = counts[1:]
    max_rate = rates[np.argmax(rates)]
    min_count_rate = rates[np.argmin(counts)]
    matches.append(max_rate == min_count_rate)

# Compute percentage
percent = np.mean(matches)
print(percent, 'smallest place field, largest rate')

#%%
# Your data
data = all_unique_rates
data = data[data > 0]

# Histogram
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Negative log likelihood for Negative Binomial
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

# Fit parameters
initial_guess = [5, 0.5]
result = minimize(neg_log_likelihood, initial_guess, args=(data,), 
                  bounds=[(1e-3, None), (1e-5, 1-1e-5)])
r_hat, p_hat = result.x

# Plot
x_plot = np.arange(data.min(), data.max() + 1)
pmf_fit = nbinom.pmf(x_plot, r_hat, p_hat)

plt.figure(figsize=(8, 4))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.6, label='Data (histogram)', color='skyblue')
plt.plot(x_plot, pmf_fit, 'ro-', label=f'Negative Binomial Fit (r={r_hat:.1f}, p={p_hat:.2f})')
plt.xlabel('Rate')
plt.ylabel('Probability')
plt.title('Negative Binomial Fit to Data')
plt.legend()
plt.tight_layout()
plt.show()
