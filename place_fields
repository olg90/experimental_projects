#%%
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from scipy.stats import nbinom
from scipy.optimize import minimize

# ========================= Load data from .mat files =========================
n = [] # number of place fields
p = [] # neuron maps

location=1
if location==0:
    dir1 = r'C:\Users\ogamb\OneDrive\Desktop\Research\place_fields\space field\BigroomData'
if location==1:
    dir1 = r'C:\Users\olgud\OneDrive\Desktop\PhD\3-Research\Neuron\place fields\DataforDrSingh\BigroomData'
os.chdir(dir1)
dir2 = os.listdir(dir1)

for foldername in dir2:
    dir3 = os.path.join(dir1, foldername)
    os.chdir(dir3)
    dirs3 = os.listdir()
    dirs4 = [x for x in dirs3 if x[-7] == 'm']
    for matname in dirs4:
        dir5 = os.path.join(dir3, matname)
        mat = loadmat(dir5)
        n.append(mat['numFields'][0][0])
        p.append(mat['pfields'])

p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

p0=p_trimmed[0]        
pshape=p0.shape
grid=(16,4)

plt.figure(figsize=(10,4))
plt.imshow(p0)
plt.colorbar()
plt.show()

plt.figure(figsize=(10,4))
plt.imshow(p0)
plt.yticks([])
plt.xticks([])
plt.show()

fs=20
plt.figure(figsize=(10,4))
plt.title('Firing rate map, grid size = ({},{})'.format(grid[0], grid[1]),fontsize=fs)
# =============================================================================
# plt.title('Firing rate map, Neuron # {}'.format(1),fontsize=fs)
# =============================================================================
im = plt.imshow(p0, cmap='viridis')
# Add colorbar
cbar = plt.colorbar(shrink=1)
cbar.ax.tick_params(labelsize=fs)
cbar.set_label('Firing rate', fontsize=fs, labelpad=20)
cbar.ax.yaxis.label.set_rotation(270)
plt.tick_params(axis='both', labelsize=fs)

# Draw red horizontal grid lines
for i in range(0, pshape[0], grid[0]):
    plt.hlines(i - 0.5, xmin=-0.5, xmax=pshape[1] - 0.5, colors='red', linewidth=2)

# Draw red vertical grid lines
for j in range(0, pshape[1], grid[1]):
    plt.vlines(j - 0.5, ymin=-0.5, ymax=pshape[0] - 0.5, colors='red', linewidth=2)

plt.tight_layout()
plt.show()


#%% Plot of grid over place field

import matplotlib.pyplot as plt
import numpy as np

# Your trimmed firing rate map and grid
p0 = p_trimmed[0]
pshape = p0.shape
grid = (8, 4)

# Plot the base firing rate map
fig, ax = plt.subplots()
im = ax.imshow(p0, cmap='viridis')

# Add colorbar
cbar = fig.colorbar(im, ax=ax, shrink=0.7)
cbar.set_label('Firing rate', fontsize=15, labelpad=20)
cbar.ax.yaxis.label.set_rotation(270)

# Draw horizontal and vertical red lines
for i in range(0, pshape[0], grid[0]):
    ax.axhline(i - 0.5, color='red', linewidth=1)

for j in range(0, pshape[1], grid[1]):
    ax.axvline(j - 0.5, color='red', linewidth=1)

# Optional: show tighter layout
plt.tight_layout()
plt.show()

# =============================================================================
# #%% Plots for collaborator meeting
# 
# th=10
# p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
# p_trimmed = [pi[:-1, :] for pi in p_trimmed]
# pth = [(pi>th)*pi for pi in p_trimmed]
# pthc = np.array([(pi>th)*1 for pi in p_trimmed]) # convert to 1s and 0s
# rates=np.unique(p_trimmed)
# 
# plt.imshow(pth[0])
# plt.xticks([])
# plt.yticks([])
# =============================================================================
#%%  beta binomali gaussian
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, betabinom, binom
from scipy.optimize import curve_fit, minimize



## Original
# --- Data preparation (based on list of arrays `p`) ---
# Step 1: Trim edges if necessary, shape is (64 x 116)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi > 0) * 1 for pi in p_trimmed]

# Step 2: Divide into grid and count active neurons
pshape = pth[0].shape
n = 1  # 1,2,4
grid = (n, n)
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

plist = []
clist = []
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Step 3: Flatten counts
csum = np.sum(clist, axis=0)
data = csum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20

# --- Gaussian Mixture Fit ---
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    w2 = 1 - w1
    return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)

hist_vals, _ = np.histogram(data, bins=binarr, density=True)
p0 = [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]
bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3],
          [1, np.inf, np.inf, np.inf, np.inf])

params_gmm, _ = curve_fit(
    gaussian_mixture, bin_centers, hist_vals, p0=p0, bounds=bounds, maxfev=10000
)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV² for each Gaussian component
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# --- Beta-binomial Fit ---
n_neurons = len(pth)

def neg_log_likelihood(params):
    a, b = params
    if a <= 0 or b <= 0:
        return np.inf
    return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))

res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B',
               bounds=[(1e-5, None), (1e-5, None)])
alpha_fit, beta_fit = res.x
x_discrete = np.arange(0, n_neurons + 1)
pmf_beta = betabinom.pmf(x_discrete, n=n_neurons, a=alpha_fit, b=beta_fit)

# Mean and CV² for beta-binomial
mean_bb = n_neurons * alpha_fit / (alpha_fit + beta_fit)
var_bb = (n_neurons * alpha_fit * beta_fit * (n_neurons + alpha_fit + beta_fit)) / \
         ((alpha_fit + beta_fit) ** 2 * (alpha_fit + beta_fit + 1))
cv2_bb = var_bb / mean_bb ** 2

# --- Binomial Fit (MLE) ---
p_hat = np.mean(data) / n_neurons
pmf_binom = binom.pmf(x_discrete, n=n_neurons, p=p_hat)
mean_binom = n_neurons * p_hat
var_binom = n_neurons * p_hat * (1 - p_hat)
cv2_binom = var_binom / mean_binom ** 2 if mean_binom != 0 else np.nan

# --- Plot All Fits ---
plt.figure()
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

plt.plot(bin_centers, y_gmm, 'r-', lw=5,
         label=f'Gaussian mixture\n(μ1={mu1:.2f}, CV²₁={cv2_1:.2f},\nμ2={mu2:.2f}, CV²₂={cv2_2:.2f})')

plt.plot(x_discrete, pmf_beta, 'g--', lw=5,
         label=f'Beta-binomial\n(mean={mean_bb:.2f}, CV²={cv2_bb:.2f})')

plt.plot(x_discrete, pmf_binom, 'm-.', lw=5,
         label=f'Binomial\n(n={n_neurons}, p={p_hat:.2f},\
             \nmean={mean_binom:.2f}, CV²={cv2_binom:.2f})')

plt.title(f'({grid[0]},{grid[1]})-grid', fontsize=fs+10)
plt.xlabel(f'# place fields in a ({grid[0]},{grid[1]}) grid', fontsize=fs+10)
plt.ylabel('Density of # neurons', fontsize=fs+10)
plt.tick_params(axis='both', labelsize=fs+10)
plt.legend(fontsize=fs+5)
plt.tight_layout()
plt.show()


#%% Binomial and negative binomial compare
# =============================================================================
# 
# p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
# p_trimmed = [pi[:-1, :] for pi in p_trimmed]
# pth = [(pi>0)*1 for pi in p_trimmed]
# rates=np.unique(p_trimmed)
# 
# ## The unique rates
# # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
# #       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
# 
# pshape = pth[0].shape
# 
# n=1 # 1,2,4
# grid=(n,n)
# x1 = pshape[0]//grid[0]
# x2 = pshape[1]//grid[1]
# 
# plist=[]
# clist=[]
# for pi in pth:
#     patches=np.zeros((x1,x2), dtype=object)
#     counts=np.zeros((x1,x2))
#     for i in range(x1):
#         for j in range(x2):
#             patch=pi[i*grid[0]:(i+1)*grid[0],j*grid[1]:+(j+1)*grid[1]]
#             count = 1*np.any(patch>0)
#             patches[i,j]=patch
#             counts[i,j]=count
#     plist.append(patches)
#     clist.append(counts)
# 
# csum=np.sum(clist, axis=0)
# data=csum.flatten()
# binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
# binplot = np.arange(np.min(data), np.max(data) + 1)
# fs=15
# 
# from scipy.stats import norm
# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.optimize import curve_fit
# 
# # mixture of two Gaussians
# def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
#     w2 = 1 - w1
#     return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)
# 
# # histogram
# hist_vals, bin_edges = np.histogram(data, bins=binarr, density=True)
# bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
# 
# # fit
# p0 = [0.5, np.mean(data)-1, 1, np.mean(data)+1, 1]
# params, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0, maxfev=10000)
# 
# # evaluate fit
# x_fit = np.linspace(np.min(data), np.max(data), 1000)
# y_fit = gaussian_mixture(x_fit, *params)
# 
# # plot
# plt.figure()
# plt.hist(data, bins=binarr, density=True,
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# plt.plot(x_fit, y_fit, 'r', label='Gaussian mixture fit')
# plt.title('({},{})-grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both', labelsize=fs)
# plt.legend()
# plt.show()
# 
# from scipy.stats import betabinom
# from scipy.optimize import minimize
# 
# # Step 1: Your `data` already contains counts per region (number of neurons active)
# data = csum.flatten()
# 
# # Step 2: Total number of neurons per grid patch = number of pthc arrays
# n_neurons = len(pth)
# 
# # Step 3: Fit alpha, beta using MLE
# def neg_log_likelihood(params):
#     a, b = params
#     if a <= 0 or b <= 0:
#         return np.inf
#     return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))
# 
# res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B', 
#                bounds=[(1e-5, None), (1e-5, None)])
# alpha_fit, beta_fit = res.x
# 
# print(f"Fitted alpha: {alpha_fit:.3f}, beta: {beta_fit:.3f}")
# 
# # Step 4: Plot histogram with fitted beta-binomial PMF
# binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
# binplot = np.arange(np.min(data), np.max(data) + 1)
# 
# fs = 15
# plt.figure()
# plt.hist(data, bins=binarr, density=True, 
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# 
# # Overlay beta-binomial fit
# x = np.arange(0, n_neurons + 1)
# pmf = betabinom.pmf(x, n=n_neurons, a=alpha_fit, b=beta_fit)
# plt.plot(x, pmf, 'r--', lw=2, label='Beta-binomial fit')
# 
# plt.title('({},{})-grid'.format(
#     grid[0], grid[1]), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both', labelsize=fs)
# plt.legend(fontsize=fs)
# plt.show()
# =============================================================================
        
#%%  Threshold (5 and 10) and divide into rectangles

th=0
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi>th)*pi for pi in p_trimmed]
pthc = np.array([(pi>th)*1 for pi in p_trimmed]) # convert to 1s and 0s
rates=np.unique(p_trimmed)

## The unique rates
# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
#       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]

pshape = pth[0].shape

# =============================================================================
# pshape = pth[0].shape # (65, 116)
# def find_factors(num):
#     factors = []
#     for i in range(1, int(num**0.5) + 1):
#         if num % i == 0:
#             factors.append(i)
#             if num // i != i:
#                 factors.append(num // i)
#     factors.sort()
#     return factors
# 
# f1=find_factors(pshape[0])
# f2=find_factors(pshape[1])
# =============================================================================
# =============================================================================
# grid=(1,1)
# x1 = pshape[0]//grid[0]
# x2 = pshape[1]//grid[1]
# =============================================================================
# grid x shape = [1, 2, 4, 8, 16, 32, 64]
# grid y shape = [1, 2, 4, 29, 58, 116]

n=1 # 1,2,4
grid=(n,n)
x1 = pshape[0]//grid[0]
x2 = pshape[1]//grid[1]

plist=[]
clist=[]
for pi in pthc:
    patches=np.zeros((x1,x2), dtype=object)
    counts=np.zeros((x1,x2))
    for i in range(x1):
        for j in range(x2):
            patch=pi[i*grid[0]:(i+1)*grid[0],j*grid[1]:+(j+1)*grid[1]]
            count = 1*np.any(patch>0)
            patches[i,j]=patch
            counts[i,j]=count
    plist.append(patches)
    clist.append(counts)

csum=np.sum(clist, axis=0)
data=csum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)
fs=15
# =============================================================================
# pth3=[]
# for pi in plist:
#     pth2 = np.zeros(plist[0].shape)
#     pishape=pi.shape    
#     for i in range(pishape[0]):
#         for j in range(pishape[1]):
#             pth2[i,j]=1 if np.sum(pi[i,j])>0 else 0
#     pth3.append(pth2)
# 
# =============================================================================
# =============================================================================
# 
##%%
# fig, ax = plt.subplots()
# 
# im = ax.imshow(pth3[0], extent=[0, 116, 0, 64], cmap='gray', origin='lower')
# 
# ax.set_xticks([])
# ax.set_yticks([])
# 
# # Horizontal lines: 16 rows → every 64/16 = 4 units
# for i in range(17):  # 16 divisions = 17 lines
#     y = i * (64 / 16)
#     ax.axhline(y, color='red', linewidth=1)
# 
# # Vertical lines: 116 columns → every 1 unit
# for j in range(117):  # 116 divisions = 117 lines
#     x = j * (116 / 116)
#     ax.axvline(x, color='red', linewidth=1)
# 
# plt.show()
# 
# 
#     #%%
# =============================================================================
# =============================================================================
# plt.figure()
# plt.hist(data, bins=binarr, density=True, 
#          color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# plt.title('({},{})-grid, counting all AP rates > {}'.format(
#     grid[0], grid[1], th), fontsize=fs)
# plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), 
#            fontsize=fs)
# plt.ylabel('Density of # neurons', fontsize=fs)
# plt.tick_params(axis='both',labelsize=fs)
# plt.show()
# =============================================================================
# =============================================================================
# plt.figure()
# plt.imshow(csum, cmap='hot')
# plt.title('Number of neurons firing in a ({},{})-grid'.format(grid[0], grid[1]))
# plt.colorbar()
# =============================================================================
from scipy.stats import norm
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

# mixture of two Gaussians
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    w2 = 1 - w1
    return w1 * norm.pdf(x, mu1, sigma1) + w2 * norm.pdf(x, mu2, sigma2)

# histogram
hist_vals, bin_edges = np.histogram(data, bins=binarr, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# fit
p0 = [0.5, np.mean(data)-1, 1, np.mean(data)+1, 1]
params, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0)

# evaluate fit
x_fit = np.linspace(np.min(data), np.max(data), 1000)
y_fit = gaussian_mixture(x_fit, *params)

# plot
plt.figure()
plt.hist(data, bins=binarr, density=True,
         color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
plt.plot(x_fit, y_fit, 'r', label='Gaussian mixture fit')
plt.title('({},{})-grid, counting all AP rates > {}'.format(grid[0], grid[1], th), fontsize=fs)
plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
plt.ylabel('Density of # neurons', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend()
plt.show()

from scipy.stats import betabinom
from scipy.optimize import minimize

# Step 1: Your `data` already contains counts per region (number of neurons active)
data = csum.flatten()

# Step 2: Total number of neurons per grid patch = number of pthc arrays
n_neurons = len(pthc)

# Step 3: Fit alpha, beta using MLE
def neg_log_likelihood(params):
    a, b = params
    if a <= 0 or b <= 0:
        return np.inf
    return -np.sum(betabinom.logpmf(data, n=n_neurons, a=a, b=b))

res = minimize(neg_log_likelihood, x0=[2, 2], method='L-BFGS-B', bounds=[(1e-5, None), (1e-5, None)])
alpha_fit, beta_fit = res.x

print(f"Fitted alpha: {alpha_fit:.3f}, beta: {beta_fit:.3f}")

# Step 4: Plot histogram with fitted beta-binomial PMF
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

fs = 15
plt.figure()
plt.hist(data, bins=binarr, density=True, 
         color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')

# Overlay beta-binomial fit
x = np.arange(0, n_neurons + 1)
pmf = betabinom.pmf(x, n=n_neurons, a=alpha_fit, b=beta_fit)
plt.plot(x, pmf, 'r--', lw=2, label='Beta-binomial fit')

plt.title('({},{})-grid, counting all AP rates > {}'.format(
    grid[0], grid[1], th), fontsize=fs)
plt.xlabel('# neurons firing APs in a ({},{}) grid'.format(grid[0], grid[1]), fontsize=fs)
plt.ylabel('Density of # neurons', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs)
plt.show()


#%% Percentage of filling of the space

th=10
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
p_trimmed = [pi[:-1, :] for pi in p_trimmed]
pth = [(pi>th)*pi for pi in p_trimmed]

per = [np.sum((pi>0)*1)/np.prod(pi.shape) for pi in p_trimmed]



from scipy.stats import expon
import numpy as np
import matplotlib.pyplot as plt

# Compute histogram
plt.hist(per, bins=40, density=True, label='data', edgecolor='k')

# Fit exponential distribution
loc, scale = expon.fit(per, floc=0)  # force location = 0
x = np.linspace(min(per), max(per), 1000)
pdf = expon.pdf(x, loc=loc, scale=scale)

# Plot exponential fit
plt.plot(x, pdf, 'r', label='exponential fit', linewidth=4, alpha=1)
plt.legend()
plt.xlabel('Percentage of the rectangular room the neuron fires in')
plt.ylabel('Density')

from scipy.stats import gamma
import numpy as np
import matplotlib.pyplot as plt

plt.figure()

# Histogram
plt.hist(per, bins=45, density=True, label='data', edgecolor='k')

# Fit gamma distribution
shape, loc, scale = gamma.fit(per, floc=0)  # force location = 0
x = np.linspace(min(per), max(per), 1000)
pdf = gamma.pdf(x, a=shape, loc=loc, scale=scale)

# Plot gamma fit
plt.plot(x, pdf, 'r', label='gamma fit', linewidth=3)
plt.legend()
plt.xlabel('Percentage of the rectangular room the neuron fires in')
plt.ylabel('Density')
#%%

# =================== Fit NB with integer r using grid search ===================

# =============================================================================
# possible_r = np.arange(1, 50)  # Try integer r from 1 to 49
# best_ll = -np.inf
# best_r = None
# best_p = None
#
# for r in possible_r:
#     mean = np.mean(n)
#     var = np.var(n)
#     if var > mean:  # NB only defined for overdispersed data
#         p_hat = r / (r + mean)
#         ll = np.sum(nbinom.logpmf(n, r, p_hat))
#         if ll > best_ll:
#             best_ll = ll
#             best_r = r
#             best_p = p_hat
# 
# rhat, phat = best_r, best_p
# 
# =============================================================================
# Original MLE estimation using continuous parameters
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(n,), bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

# ========================= Plot histogram + fitted distribution =========================
binarr = np.arange(np.min(n) - 0.5, np.max(n) + 1.5)
binplot = np.arange(np.min(n), np.max(n) + 1)

plt.figure(figsize=(8, 3))
plt.hist(n, bins=binarr, color='skyblue', edgecolor='black', alpha=0.7, density=True)
pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('Density of # of neurons', fontsize=12)
plt.xlabel('# of place fields for a neuron', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

# =============================================================================
# # Plot out all the place fields
# for i in range(len(p)):
#     plt.figure()
#     plt.imshow(p[i])
#     plt.colorbar()
# =============================================================================

pcount = [np.where(pi[:, :-1] != 0, 1, 0) if pi.shape[1] == 117 else np.where(pi != 0, 1, 0) for pi in p]

from collections import Counter

shape_counts = Counter([pi.shape for pi in pcount])
print(shape_counts) 

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom, norm

pcountsum = np.sum(pcount, axis=0)

plt.figure()
plt.imshow(pcountsum, cmap='hot')
plt.title('Number of neurons that fire at each pixel\nin the rectangular room')
plt.colorbar()

#%%

binarr = np.arange(np.min(pcountsum) - 0.5, np.max(pcountsum) + 1.5)
binplot = np.arange(np.min(pcountsum), np.max(pcountsum) + 1)
# Flatten data for fitting
data_flat = pcountsum.flatten()
# Fit Normal distribution
mu_norm, std_norm = norm.fit(data_flat)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skewnorm
from scipy.optimize import minimize

# Flatten and bin data
data = pcountsum.flatten()
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
binplot = np.arange(np.min(data), np.max(data) + 1)

# ======== Fit Skew-Normal ========
a_skew, loc_skew, scale_skew = skewnorm.fit(data)
x_vals = np.linspace(np.min(data), np.max(data), 1000)
pdf_skew = skewnorm.pdf(x_vals, a_skew, loc=loc_skew, scale=scale_skew)

# ======== Fit Gaussian Mixture (2 components) ========
def neg_log_likelihood(params):
    w, mu1, sigma1, mu2, sigma2 = params
    if not (0 < w < 1 and sigma1 > 0 and sigma2 > 0):
        return np.inf
    pdf_vals = w * norm.pdf(data, mu1, sigma1) + (1 - w) * norm.pdf(data, mu2, sigma2)
    return -np.sum(np.log(pdf_vals + 1e-12))

initial_guess = [0.5, np.percentile(data, 25), 1.0, np.percentile(data, 75), 1.0]
bounds = [(1e-3, 1 - 1e-3), (None, None), (1e-3, None), (None, None), (1e-3, None)]
result = minimize(neg_log_likelihood, initial_guess, bounds=bounds)
w, mu1, sigma1, mu2, sigma2 = result.x
pdf_mix = w * norm.pdf(x_vals, mu1, sigma1) + (1 - w) * norm.pdf(x_vals, mu2, sigma2)
pdf1 = w * norm.pdf(x_vals, mu1, sigma1)
pdf2 = (1 - w) * norm.pdf(x_vals, mu2, sigma2)

# ======== Plot All Together ========
plt.figure(figsize=(9, 4))
plt.hist(data, bins=binarr, density=True, color='skyblue', edgecolor='black', alpha=0.7, label='Data histogram')
# =============================================================================
# plt.plot(x_vals, pdf_skew, 'g-', linewidth=2, label=f'Skew Normal (a={a_skew:.2f})')
# =============================================================================
plt.plot(x_vals, pdf_mix, 'r-', linewidth=2, label='Gaussian Mixture')
# =============================================================================
# plt.plot(x_vals, pdf1, 'k--', label='Component 1')
# plt.plot(x_vals, pdf2, 'm--', label='Component 2')
# =============================================================================

plt.xlabel('# of neurons firing APs at a pixel')
plt.ylabel('Density of # neurons firing APs at a pixel')
plt.xticks(binplot)
plt.legend()
plt.tight_layout()
plt.show()

# Trim and collect matrices
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Convert to NumPy array stack only if all shapes are now equal
shapes = [pi.shape for pi in p_trimmed]
common_shape = shapes[0]
assert all(shape == common_shape for shape in shapes), "Not all matrices have the same shape after trimming."
# Sum them together
pratesum = np.sum(p_trimmed, axis=0)
plt.figure()
plt.title('Sum of all AP rates at each location')
plt.imshow(pratesum, cmap='hot')
plt.colorbar()

plt.figure()
plt.title('Mean rate at each location')
meanmap=pratesum/pcountsum
plt.imshow(meanmap, cmap='hot')
plt.colorbar()

#%%
# Count occurrences of most common non-zero rate in each pi
counts = [np.count_nonzero(pi == np.bincount(pi.flatten())[1:].argmax() + 1) for pi in p]

# Get the most common non-zero rate in each pi
rates = [np.bincount(pi.flatten())[1:].argmax() + 1 for pi in p]

print(counts)
print(rates)

# Histogram of counts
plt.hist(counts, bins=np.arange(min(counts), max(counts) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Counts')
plt.xlabel('Count')
plt.ylabel('Frequency')
plt.xticks(np.unique(counts))
plt.show()

# Histogram of rates
plt.hist(rates, bins=np.arange(min(rates), max(rates) + 2) - 0.5, edgecolor='black', alpha=0.7)
plt.title('Histogram of Rates')
plt.xlabel('Rate')
plt.ylabel('Frequency')
plt.xticks(np.unique(rates))
plt.show()

#%% Number of neurons with AP rate

# For each pi, find the unique nonzero rates
unique_rates_per_pi = [np.unique(pi[pi != 0]) for pi in p]

# Flatten the list and combine all unique rates
# A list of all the rates that appear in each neuron uniquely
all_unique_rates = np.concatenate(unique_rates_per_pi)

# Plot histogram
plt.figure(figsize=(6,4))
plt.hist(all_unique_rates, 
         bins=np.arange(all_unique_rates.min(), all_unique_rates.max() + 2) - 0.5, 
         edgecolor='black', alpha=0.7, density=True)
plt.xlabel('AP rate')
plt.ylabel('# of neurons with this AP rate in their place map')
plt.xticks(np.unique(all_unique_rates))

def neg_log_likelihood(params, data):
    r, pp = params
    if r <= 0 or not (0 < pp < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, pp))

initial_guess = [2.0, 0.1]
result = minimize(neg_log_likelihood, initial_guess, args=(all_unique_rates,), 
                  bounds=[(1e-3, None), (1e-5, 1 - 1e-5)])
rhat, phat = result.x

pmf = nbinom.pmf(binplot, rhat, phat)
plt.plot(binplot, pmf, 'ro-', label=f'Negative Binomial(r={rhat:.2f}, p={phat:.2f})')
plt.ylabel('# of neurons', fontsize=12)
plt.xlabel('AP rates (Hz)', fontsize=10)
plt.xticks(binplot)
plt.tick_params(axis='x', labelsize=8)
plt.legend()
plt.tight_layout()
plt.show()

#%% Percentage of map covered

p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]
pcovered = [np.sum(pi.flatten()>0)/(pi.shape[0]*pi.shape[1]) for pi in p_trimmed]
plt.hist(pcovered, bins=100)

#%% What percentage of all maps do their largest place field have the largest rates?
# For each pi, check if the largest place field has the largest rate
matches = []
for pi in p:
    if np.all(pi == 0):
        matches.append(False)
        continue
    max_rate = pi[pi != 0].max()
    rate_with_largest_field = np.bincount(pi.flatten())[1:].argmax() + 1
    matches.append(rate_with_largest_field == max_rate)

# Calculate percentage
percentage = np.mean(matches)
print(percentage, "largest place field, largest rate")


#%% Smallest size largest rate?
# Check if the highest rate appears the least number of times in each pi
matches = []
for pi in p:
    if np.all(pi == 0):
        continue
    flat = pi.flatten()
    flat = flat[flat != 0]
    counts = np.bincount(flat)
    rates = np.arange(len(counts))
    rates = rates[1:]
    counts = counts[1:]
    max_rate = rates[np.argmax(rates)]
    min_count_rate = rates[np.argmin(counts)]
    matches.append(max_rate == min_count_rate)

# Compute percentage
percent = np.mean(matches)
print(percent, 'smallest place field, largest rate')

#%%
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import nbinom, weibull_min

data = all_unique_rates
data = data[data > 0]

# Histogram (density=True for PDF scale)
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# --- Negative log likelihood for Negative Binomial ---
def neg_log_likelihood(params, data):
    r, p = params
    if r <= 0 or not (0 < p < 1):
        return np.inf
    return -np.sum(nbinom.logpmf(data, r, p))

# Fit Negative Binomial params
initial_guess = [5, 0.5]
result = minimize(neg_log_likelihood, initial_guess, args=(data,), 
                  bounds=[(1e-3, None), (1e-5, 1-1e-5)])
r_hat, p_hat = result.x

# Negative Binomial PMF over data range
x_plot = np.arange(data.min(), data.max() + 1)
pmf_fit = nbinom.pmf(x_plot, r_hat, p_hat)

# --- Weibull Fit ---
# Weibull is continuous, so we use MLE from scipy (force loc=0)
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x_wb = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x_wb, c_wb, loc=loc_wb, scale=scale_wb)

# To compare Weibull PDF with histogram and NB PMF, scale PDF:
# Multiply by bin width ~ 1 for visual comparison on histogram scale
pdf_wb_scaled = pdf_wb

# --- Plot ---
plt.figure(figsize=(6, 3))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.6, label='Data (histogram)', color='skyblue')
plt.plot(x_plot, pmf_fit, 'ro-', label=f'Negative Binomial Fit (r={r_hat:.2f}, p={p_hat:.2f})')
plt.plot(x_wb, pdf_wb_scaled, 'g-', label=f'Weibull Fit (k={c_wb:.2f}, scale={scale_wb:.2f})')
plt.xlabel('AP rate of a place field (Hz)')
plt.ylabel('Density of # of neurons')
plt.legend()
plt.tight_layout()
plt.show()

#%%

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import weibull_min, invgamma, fisk  # fisk = log-logistic

# Your positive data
data = all_unique_rates
data = data[data > 0]

# Histogram for visualization
bin_edges = np.arange(data.min() - 0.5, data.max() + 1.5, 1)
hist_y, _ = np.histogram(data, bins=bin_edges, density=True)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Fit Weibull (shape c), force loc=0
c_wb, loc_wb, scale_wb = weibull_min.fit(data, floc=0)
x = np.linspace(data.min(), data.max(), 1000)
pdf_wb = weibull_min.pdf(x, c_wb, loc=loc_wb, scale=scale_wb)

# Fit Inverse Gamma (shape a), force loc=0
a_ig, loc_ig, scale_ig = invgamma.fit(data, floc=0)
pdf_ig = invgamma.pdf(x, a_ig, loc=loc_ig, scale=scale_ig)

# Fit Log-Logistic (Fisk), force loc=0
c_ll, loc_ll, scale_ll = fisk.fit(data, floc=0)
pdf_ll = fisk.pdf(x, c_ll, loc=loc_ll, scale=scale_ll)

# Function to find inflection points (zero crossings of second derivative)
def find_inflection_points(pdf_vals, x_vals):
    dx = x_vals[1] - x_vals[0]
    second_deriv = np.gradient(np.gradient(pdf_vals, dx), dx)
    zero_crossings = np.where(np.diff(np.sign(second_deriv)))[0]
    return x_vals[zero_crossings]

# Find inflection points for each PDF
inflect_wb = find_inflection_points(pdf_wb, x)
inflect_ig = find_inflection_points(pdf_ig, x)
inflect_ll = find_inflection_points(pdf_ll, x)

# Plot
plt.figure(figsize=(12, 6))
plt.bar(bin_centers, hist_y, width=1, edgecolor='black', alpha=0.5, label='Data (histogram)', color='skyblue')

plt.plot(x, pdf_wb, 'r-', label=f'Weibull (k={c_wb:.3f})')
for ip in inflect_wb:
    plt.axvline(ip, color='r', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ig, 'g-', label=f'Inverse Gamma (a={a_ig:.3f})')
for ip in inflect_ig:
    plt.axvline(ip, color='g', linestyle='--', alpha=0.5)

plt.plot(x, pdf_ll, 'm-', label=f'Log-Logistic (c={c_ll:.3f})')
for ip in inflect_ll:
    plt.axvline(ip, color='m', linestyle='--', alpha=0.5)

plt.xlabel('Rate')
plt.ylabel('Probability density')
plt.title('Fits and Inflection Points of Weibull, Inverse Gamma, Log-Logistic')
plt.legend()
plt.tight_layout()
plt.show()

# Print inflection points
print("Inflection points (x values) where PDF curvature changes sign:")
print(f"Weibull: {inflect_wb}")
print(f"Inverse Gamma: {inflect_ig}")
print(f"Log-Logistic: {inflect_ll}")

#%% Mixture Model Fits: Gaussian, Beta-binomial, Binomial Mixtures + AIC

import numpy as np  # Numerical computing library
import matplotlib.pyplot as plt  # Plotting library
from scipy.stats import norm, betabinom, binom  # Probability distributions
from scipy.optimize import curve_fit, minimize  # Optimization and fitting tools

# --- Data Preparation ---

# Trim each array in 'p' if it has 117 columns (remove last column)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Remove the last row from all matrices to match expected size
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

# Binarize: convert all values > 0 to 1 (active), 0 otherwise
pth = [(pi > 0).astype(int) for pi in p_trimmed]

# Extract the shape of the arrays (should all be same shape)
pshape = pth[0].shape

# Grid size for patching
n = 4
grid = (n, n)

# Compute number of patches that fit in each dimension
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

# Initialize storage for patches and counts
plist = []
clist = []

# Divide each array into patches and count active neurons in each patch
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            # Extract n x n patch
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            # Count if any neuron is active in this patch
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Sum counts across all neurons, flatten to 1D array
csum = np.sum(clist, axis=0)
data = csum.flatten()

# Binning setup for histogram
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20  # Font size for plots

# --- Gaussian Mixture Model ---

# Define PDF of 2-component Gaussian mixture
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    return w1 * norm.pdf(x, mu1, sigma1) + (1 - w1) * norm.pdf(x, mu2, sigma2)

# Histogram of data (used for fitting)
hist_vals, _ = np.histogram(data, bins=binarr, density=True)

# Initial guess and bounds for fitting
p0 = [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]
bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3], [1, np.inf, np.inf, np.inf, np.inf])

# Fit Gaussian mixture model to histogram
params_gmm, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=p0, bounds=bounds)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV² for both Gaussian components
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# --- Beta-binomial Mixture Model ---

n_neurons = len(pth)  # Number of neurons

# Define 2-component beta-binomial mixture PMF
def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

# Negative log-likelihood for beta-binomial mixture
def nll_bb_mix(params):
    w, a1, b1, a2, b2 = params
    pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
    if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to fit parameters    w  a1 b1 a2 b2
res_bb_mix = minimize(nll_bb_mix, [0.5, 1, 2, 5, 5], bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x

# Generate PMF from fit
x_discrete = np.arange(0, n_neurons + 1)
pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)

# Compute mean, variance, CV² for each component
mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# --- Binomial Mixture Model ---

# Define 2-component binomial mixture PMF
def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# Negative log-likelihood for binomial mixture
def nll_binom_mix(params):
    w, p1, p2 = params
    pmf = pmf_binom_mix(data, w, p1, p2)
    if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to find best parameters
res_binom_mix = minimize(nll_binom_mix, [0.05, 0.1, 0.2], bounds=[(1e-3, 1 - 1e-3), (1e-5, 1 - 1e-5), (1e-5, 1 - 1e-5)])
w_bm, p1_bm, p2_bm = res_binom_mix.x
pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)

# Compute means, variances, CV² for both binomial components
mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# --- AIC Calculation ---

# Compute log-likelihoods
logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
logL_bbmix = np.sum(np.log(pmf_bb_mix(data, w_bb, a1_bb, b1_bb, a2_bb, b2_bb) + 1e-12))
logL_binmix = np.sum(np.log(pmf_binom_mix(data, w_bm, p1_bm, p2_bm) + 1e-12))

# AIC = 2k - 2*logL where k = number of fitted parameters
aic_gmm = 2 * 5 - 2 * logL_gmm
aic_bbmix = 2 * 5 - 2 * logL_bbmix
aic_binmix = 2 * 3 - 2 * logL_binmix

# Print AIC table
print("\nMixture Model AIC Comparison:")
print(f"{'Model':30} {'AIC':>10}")
print("-" * 42)
print(f"{'Gaussian Mixture':30} {aic_gmm:10.2f}")
print(f"{'Beta-binomial Mixture':30} {aic_bbmix:10.2f}")
print(f"{'Binomial Mixture':30} {aic_binmix:10.2f}")

# --- Plotting ---

plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

# Plot Gaussian Mixture
plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
         label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

# Plot Beta-binomial Mixture
plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

# Plot Binomial Mixture
plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'Binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({grid[0]},{grid[1]})-grid: Mixture Model Fits', fontsize=fs+6)
plt.xlabel(f'# place fields per ({grid[0]},{grid[1]}) patch', fontsize=fs)
plt.ylabel('Count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0]-1, binarr[-1]+1])
plt.show()

#%% Mixture Model Fits: Gaussian, Beta-binomial, Binomial Mixtures + AIC

import numpy as np  # Numerical computing library
import matplotlib.pyplot as plt  # Plotting library
from scipy.stats import norm, betabinom, binom  # Probability distributions
from scipy.optimize import curve_fit, minimize  # Optimization and fitting tools

# --- Data Preparation ---

# Trim each array in 'p' if it has 117 columns (remove last column)
p_trimmed = [pi[:, :-1] if pi.shape[1] == 117 else pi for pi in p]

# Remove the last row from all matrices to match expected size
p_trimmed = [pi[:-1, :] for pi in p_trimmed]

# Binarize: convert all values > 0 to 1 (active), 0 otherwise
pth = [(pi > 0).astype(int) for pi in p_trimmed]

# Extract the shape of the arrays (should all be same shape)
pshape = pth[0].shape

# Grid size for patching
n = 1

ic_gmm=[[0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1],
        [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1],
        [0.5, np.mean(data) - 1, 1, np.mean(data) + 1, 1]]

ic_bb=[[0.1, 1, 2, 5, 5],
       [0.1, 1, 2, 5, 5],
       [0.1, 1, 2, 5, 5]]

ics_bin=[[0.05, 0.1, 0.2],
         [0.05, 0.1, 0.2],
         [0.05, 0.1, 0.2]]

grid = (n, n)

# Compute number of patches that fit in each dimension
x1 = pshape[0] // grid[0]
x2 = pshape[1] // grid[1]

# Initialize storage for patches and counts
plist = []
clist = []

# Divide each array into patches and count active neurons in each patch
for pi in pth:
    patches = np.zeros((x1, x2), dtype=object)
    counts = np.zeros((x1, x2))
    for i in range(x1):
        for j in range(x2):
            # Extract n x n patch
            patch = pi[i * grid[0]:(i + 1) * grid[0], j * grid[1]:(j + 1) * grid[1]]
            # Count if any neuron is active in this patch
            count = 1 * np.any(patch > 0)
            patches[i, j] = patch
            counts[i, j] = count
    plist.append(patches)
    clist.append(counts)

# Sum counts across all neurons, flatten to 1D array
csum = np.sum(clist, axis=0)
data = csum.flatten()

# Binning setup for histogram
binarr = np.arange(np.min(data) - 0.5, np.max(data) + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
fs = 20  # Font size for plots


def getics(ics, n):
    if n == 1:
        return ics[0]
    if n==2:
        return ics[1]
    if n==4:
        return ics[2]

# --- Gaussian Mixture Model ---

# Define PDF of 2-component Gaussian mixture
def gaussian_mixture(x, w1, mu1, sigma1, mu2, sigma2):
    return w1 * norm.pdf(x, mu1, sigma1) + (1 - w1) * norm.pdf(x, mu2, sigma2)

# Histogram of data (used for fitting)
hist_vals, _ = np.histogram(data, bins=binarr, density=True)

# Initial guess and bounds for fitting

bounds = ([0, -np.inf, 1e-3, -np.inf, 1e-3], [1, np.inf, np.inf, np.inf, np.inf])

# Fit Gaussian mixture model to histogram
params_gmm, _ = curve_fit(gaussian_mixture, bin_centers, hist_vals, p0=getics(ic_gmm,n), bounds=bounds)
w1, mu1, sigma1, mu2, sigma2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)

# Compute CV² for both Gaussian components
cv2_1 = (sigma1 ** 2) / mu1 ** 2 if mu1 != 0 else np.nan
cv2_2 = (sigma2 ** 2) / mu2 ** 2 if mu2 != 0 else np.nan

# --- Beta-binomial Mixture Model ---

n_neurons = len(pth)  # Number of neurons

# Define 2-component beta-binomial mixture PMF
def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

# Negative log-likelihood for beta-binomial mixture
def nll_bb_mix(params):
    w, a1, b1, a2, b2 = params
    pmf = pmf_bb_mix(data, w, a1, b1, a2, b2)
    if 0 < w < 1 and all(x > 0 for x in [a1, b1, a2, b2]):
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to fit parameters    w  a1 b1 a2 b2
res_bb_mix = minimize(nll_bb_mix, getics(ic_bb, n), bounds=[(1e-3, 1 - 1e-3)] + [(1e-5, None)] * 4)
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = res_bb_mix.x

# Generate PMF from fit
x_discrete = np.arange(0, n_neurons + 1)
pmf_bbmix = pmf_bb_mix(x_discrete, w_bb, a1_bb, b1_bb, a2_bb, b2_bb)

# Compute mean, variance, CV² for each component
mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# --- Binomial Mixture Model ---

# Define 2-component binomial mixture PMF
def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# Negative log-likelihood for binomial mixture
def nll_binom_mix(params):
    w, p1, p2 = params
    pmf = pmf_binom_mix(data, w, p1, p2)
    if 0 < w < 1 and 0 < p1 < 1 and 0 < p2 < 1:
        return -np.sum(np.log(pmf + 1e-12))
    return np.inf

# Minimize NLL to find best parameters
res_binom_mix = minimize(nll_binom_mix, getics(ics_bin,n), bounds=[(1e-3, 1 - 1e-3), (1e-5, 1 - 1e-5), (1e-5, 1 - 1e-5)])
w_bm, p1_bm, p2_bm = res_binom_mix.x
pmf_binmix = pmf_binom_mix(x_discrete, w_bm, p1_bm, p2_bm)

# Compute means, variances, CV² for both binomial components
mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# --- AIC Calculation ---

# Compute log-likelihoods
logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
logL_bbmix = np.sum(np.log(pmf_bb_mix(data, w_bb, a1_bb, b1_bb, a2_bb, b2_bb) + 1e-12))
logL_binmix = np.sum(np.log(pmf_binom_mix(data, w_bm, p1_bm, p2_bm) + 1e-12))

# AIC = 2k - 2*logL where k = number of fitted parameters
aic_gmm = 2 * 5 - 2 * logL_gmm
aic_bbmix = 2 * 5 - 2 * logL_bbmix
aic_binmix = 2 * 3 - 2 * logL_binmix

# Print AIC table
print("\nMixture Model AIC Comparison:")
print(f"{'Model':30} {'AIC':>10}")
print("-" * 42)
print(f"{'Gaussian Mixture':30} {aic_gmm:10.2f}")
print(f"{'Beta-binomial Mixture':30} {aic_bbmix:10.2f}")
print(f"{'Binomial Mixture':30} {aic_binmix:10.2f}")

# --- Plotting ---

plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

# Plot Gaussian Mixture
plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
         label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

# Plot Beta-binomial Mixture
plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

# Plot Binomial Mixture
plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'Binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({grid[0]},{grid[1]})-grid: Mixture Model Fits', fontsize=fs+6)
plt.xlabel(f'# place fields per ({grid[0]},{grid[1]}) patch', fontsize=fs)
plt.ylabel('Count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0]-1, binarr[-1]+1])
plt.show()

#%% Manual parameters
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, betabinom, binom

# === User-defined parameters ===
params_gmm = [0.5, 2.0, 1.0, 5.0, 1.5] # w, mu1, sigma1, mu2, sigma2
params_bb  = [0.5, 3,10, 1, 5]     # w, a1, b1, a2, b2
params_bin = [0.83, 0.16, 0.11]          # w, p1, p2

# === Prepare data ===
n = 4
pth = [((pi[:, :-1] if pi.shape[1] == 117 else pi)[:-1, :] > 0).astype(int) for pi in p]
x1, x2 = pth[0].shape[0] // n, pth[0].shape[1] // n
csum = np.sum([[[np.any(pi[i*n:(i+1)*n, j*n:(j+1)*n])
                 for j in range(x2)] for i in range(x1)] for pi in pth], axis=0)
data = csum.flatten()

# === Mixture model definitions ===
n_neurons = len(pth)
binarr = np.arange(data.min() - 0.5, data.max() + 1.5)
bin_centers = (binarr[:-1] + binarr[1:]) / 2
x_discrete = np.arange(0, n_neurons + 1)

def gaussian_mixture(x, w, mu1, s1, mu2, s2):
    return w * norm.pdf(x, mu1, s1) + (1 - w) * norm.pdf(x, mu2, s2)

def pmf_bb_mix(x, w, a1, b1, a2, b2):
    return w * betabinom.pmf(x, n_neurons, a1, b1) + (1 - w) * betabinom.pmf(x, n_neurons, a2, b2)

def pmf_binom_mix(x, w, p1, p2):
    return w * binom.pmf(x, n_neurons, p1) + (1 - w) * binom.pmf(x, n_neurons, p2)

# === Evaluate Gaussian mixture ===
w1, mu1, s1, mu2, s2 = params_gmm
y_gmm = gaussian_mixture(bin_centers, *params_gmm)
cv2_1 = (s1 / mu1)**2 if mu1 else np.nan
cv2_2 = (s2 / mu2)**2 if mu2 else np.nan
logL_gmm = np.sum(np.log(gaussian_mixture(data, *params_gmm) + 1e-12))
aic_gmm = 2 * 5 - 2 * logL_gmm

# === Evaluate beta-binomial mixture ===
w_bb, a1_bb, b1_bb, a2_bb, b2_bb = params_bb
pmf_bbmix = pmf_bb_mix(x_discrete, *params_bb)
logL_bbmix = np.sum(np.log(pmf_bb_mix(data, *params_bb) + 1e-12))
aic_bbmix = 2 * 5 - 2 * logL_bbmix

mean1_bb = n_neurons * a1_bb / (a1_bb + b1_bb)
var1_bb = (n_neurons * a1_bb * b1_bb * (n_neurons + a1_bb + b1_bb)) / ((a1_bb + b1_bb) ** 2 * (a1_bb + b1_bb + 1))
cv2_1_bb = var1_bb / mean1_bb ** 2

mean2_bb = n_neurons * a2_bb / (a2_bb + b2_bb)
var2_bb = (n_neurons * a2_bb * b2_bb * (n_neurons + a2_bb + b2_bb)) / ((a2_bb + b2_bb) ** 2 * (a2_bb + b2_bb + 1))
cv2_2_bb = var2_bb / mean2_bb ** 2

# === Evaluate binomial mixture ===
w_bm, p1_bm, p2_bm = params_bin
pmf_binmix = pmf_binom_mix(x_discrete, *params_bin)
logL_binmix = np.sum(np.log(pmf_binom_mix(data, *params_bin) + 1e-12))
aic_binmix = 2 * 3 - 2 * logL_binmix

mean1_bm = n_neurons * p1_bm
var1_bm = n_neurons * p1_bm * (1 - p1_bm)
cv2_1_bm = var1_bm / mean1_bm ** 2

mean2_bm = n_neurons * p2_bm
var2_bm = n_neurons * p2_bm * (1 - p2_bm)
cv2_2_bm = var2_bm / mean2_bm ** 2

# === AIC Table ===
print("\nMixture Model AIC Comparison:")
print(f"{'Model':30} {'AIC':>10}")
print("-" * 42)
print(f"{'Gaussian Mixture':30} {aic_gmm:10.2f}")
print(f"{'Beta-binomial Mixture':30} {aic_bbmix:10.2f}")
print(f"{'Binomial Mixture':30} {aic_binmix:10.2f}")

# === Plotting ===
fs = 20
plt.figure(figsize=(10, 8))
plt.hist(data, bins=binarr, density=False,
         color='skyblue', edgecolor='black', alpha=0.6, label='Histogram')

plt.plot(bin_centers, y_gmm * len(data), 'r-', lw=5,
         label=f'Gaussian mixture\nμ₁={mu1:.2f}, CV²₁={cv2_1:.2f}, w={w1:.2f},\n'
               f'μ₂={mu2:.2f}, CV²₂={cv2_2:.2f},\nAIC={aic_gmm:.2f}')

plt.plot(x_discrete, pmf_bbmix * len(data), 'g--', lw=5,
         label=f'Beta-binomial mixture\nμ₁={mean1_bb:.2f}, CV²₁={cv2_1_bb:.2f}, w={w_bb:.2f},\n'
               f'μ₂={mean2_bb:.2f}, CV²₂={cv2_2_bb:.2f},\nAIC={aic_bbmix:.2f}')

plt.plot(x_discrete, pmf_binmix * len(data), 'm-.', lw=5,
         label=f'Binomial mixture\nn={n_neurons},\n'
               f'p₁={p1_bm:.2f}, μ₁={mean1_bm:.2f}, CV²₁={cv2_1_bm:.2f}, w={w_bm:.2f},\n'
               f'p₂={p2_bm:.2f}, μ₂={mean2_bm:.2f}, CV²₂={cv2_2_bm:.2f},\nAIC={aic_binmix:.2f}')

plt.title(f'({n},{n})-grid: Mixture Model Fits', fontsize=fs + 6)
plt.xlabel(f'# place fields per ({n},{n}) patch', fontsize=fs)
plt.ylabel('Count', fontsize=fs)
plt.tick_params(axis='both', labelsize=fs)
plt.legend(fontsize=fs - 2)
plt.tight_layout()
plt.xlim([binarr[0] - 1, binarr[-1] + 1])
plt.show()
